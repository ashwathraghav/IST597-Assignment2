{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST_597_Assignment2_FMNIST_New.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bnfwshwE4hvt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(6447) #replaced 1234 with my emailID number\n",
        "tf.random.set_seed(6447) #replaced 1234 with my emailID number"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tzhXWC656pi",
        "outputId": "8a91cacc-5ad0-4df0-c8de-ee1a731f167d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqlq0mqE597P",
        "outputId": "501af4b8-dfaf-4004-fb62-e07b79f41eea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQF18YpN5-C4",
        "outputId": "0bd45ff2-2244-443f-8e58-c26c60711bce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255"
      ],
      "metadata": {
        "id": "jZtAM8qH6FCY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "id": "hKGQ0pvX6Kae"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 1:**\n",
        "\n",
        "Using default batch size without softmax activation in output layer, without any regularization to determine the Categorical Cross-Entropy of test dataset and determine the accuracy of default model in GPU,TPU,CPU and on default mode."
      ],
      "metadata": {
        "id": "6y3r3Mr_6LdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "VqnT8Hot68Gc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LBTr2myI7JIn",
        "outputId": "775d98b1-6142-4ac4-fb3e-53d1d34e5e4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8236\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00528132568359375 \n",
            "\n",
            "Validation Accuracy: 0.8152\n",
            "\n",
            "Train Accuracy: 0.8551\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0035405474853515626 \n",
            "\n",
            "Validation Accuracy: 0.8429\n",
            "\n",
            "Train Accuracy: 0.8552\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0031083917236328124 \n",
            "\n",
            "Validation Accuracy: 0.8385\n",
            "\n",
            "Train Accuracy: 0.8595\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0028912567138671876 \n",
            "\n",
            "Validation Accuracy: 0.8439\n",
            "\n",
            "Train Accuracy: 0.8706\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0027229766845703125 \n",
            "\n",
            "Validation Accuracy: 0.8544\n",
            "\n",
            "Train Accuracy: 0.8760\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0025909075927734375 \n",
            "\n",
            "Validation Accuracy: 0.8576\n",
            "\n",
            "Train Accuracy: 0.8769\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002462238464355469 \n",
            "\n",
            "Validation Accuracy: 0.8568\n",
            "\n",
            "Train Accuracy: 0.8823\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0023721397399902346 \n",
            "\n",
            "Validation Accuracy: 0.8635\n",
            "\n",
            "Train Accuracy: 0.8874\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0022798126220703123 \n",
            "\n",
            "Validation Accuracy: 0.8677\n",
            "\n",
            "Train Accuracy: 0.8878\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.002216253356933594 \n",
            "\n",
            "Validation Accuracy: 0.8673\n",
            "\n",
            "Train Accuracy: 0.8987\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0021384275817871094 \n",
            "\n",
            "Validation Accuracy: 0.8755\n",
            "\n",
            "Train Accuracy: 0.8996\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0020691226196289064 \n",
            "\n",
            "Validation Accuracy: 0.8745\n",
            "\n",
            "Train Accuracy: 0.8969\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0020011210632324217 \n",
            "\n",
            "Validation Accuracy: 0.8704\n",
            "\n",
            "Train Accuracy: 0.9025\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.001928852996826172 \n",
            "\n",
            "Validation Accuracy: 0.8708\n",
            "\n",
            "Train Accuracy: 0.9077\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0018827130126953125 \n",
            "\n",
            "Validation Accuracy: 0.8755\n",
            "\n",
            "Train Accuracy: 0.9054\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018401507568359375 \n",
            "\n",
            "Validation Accuracy: 0.8740\n",
            "\n",
            "Train Accuracy: 0.9012\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0017834097290039062 \n",
            "\n",
            "Validation Accuracy: 0.8686\n",
            "\n",
            "Train Accuracy: 0.9121\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0017414265441894532 \n",
            "\n",
            "Validation Accuracy: 0.8745\n",
            "\n",
            "Train Accuracy: 0.9073\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0016855844116210937 \n",
            "\n",
            "Validation Accuracy: 0.8706\n",
            "\n",
            "Train Accuracy: 0.9187\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016450762939453124 \n",
            "\n",
            "Validation Accuracy: 0.8792\n",
            "\n",
            "Total time taken (in seconds): 244.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_gpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W2D4ZrOO7Vls",
        "outputId": "44286f26-f847-42ee-a1ee-d3f0e2a6cff9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8339\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005290037841796875 \n",
            "\n",
            "Validation Accuracy: 0.8250\n",
            "\n",
            "Train Accuracy: 0.8498\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.003575921325683594 \n",
            "\n",
            "Validation Accuracy: 0.8410\n",
            "\n",
            "Train Accuracy: 0.8598\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.003140069580078125 \n",
            "\n",
            "Validation Accuracy: 0.8500\n",
            "\n",
            "Train Accuracy: 0.8604\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0029202603149414063 \n",
            "\n",
            "Validation Accuracy: 0.8491\n",
            "\n",
            "Train Accuracy: 0.8650\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0027462362670898437 \n",
            "\n",
            "Validation Accuracy: 0.8514\n",
            "\n",
            "Train Accuracy: 0.8714\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0026214337158203126 \n",
            "\n",
            "Validation Accuracy: 0.8567\n",
            "\n",
            "Train Accuracy: 0.8670\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0025012506103515627 \n",
            "\n",
            "Validation Accuracy: 0.8510\n",
            "\n",
            "Train Accuracy: 0.8804\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0024042800903320313 \n",
            "\n",
            "Validation Accuracy: 0.8646\n",
            "\n",
            "Train Accuracy: 0.8886\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0023114175415039065 \n",
            "\n",
            "Validation Accuracy: 0.8704\n",
            "\n",
            "Train Accuracy: 0.8897\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0022525250244140624 \n",
            "\n",
            "Validation Accuracy: 0.8698\n",
            "\n",
            "Train Accuracy: 0.8985\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0021742176818847657 \n",
            "\n",
            "Validation Accuracy: 0.8751\n",
            "\n",
            "Train Accuracy: 0.8987\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0021047276306152343 \n",
            "\n",
            "Validation Accuracy: 0.8755\n",
            "\n",
            "Train Accuracy: 0.9040\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0020376527404785158 \n",
            "\n",
            "Validation Accuracy: 0.8786\n",
            "\n",
            "Train Accuracy: 0.9002\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0019789468383789063 \n",
            "\n",
            "Validation Accuracy: 0.8748\n",
            "\n",
            "Train Accuracy: 0.9068\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0019387249755859375 \n",
            "\n",
            "Validation Accuracy: 0.8767\n",
            "\n",
            "Train Accuracy: 0.9074\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018784194946289063 \n",
            "\n",
            "Validation Accuracy: 0.8770\n",
            "\n",
            "Train Accuracy: 0.9045\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0018279122924804688 \n",
            "\n",
            "Validation Accuracy: 0.8731\n",
            "\n",
            "Train Accuracy: 0.9093\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0017743424987792969 \n",
            "\n",
            "Validation Accuracy: 0.8745\n",
            "\n",
            "Train Accuracy: 0.8940\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.001726641845703125 \n",
            "\n",
            "Validation Accuracy: 0.8601\n",
            "\n",
            "Train Accuracy: 0.9066\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016773966979980468 \n",
            "\n",
            "Validation Accuracy: 0.8708\n",
            "\n",
            "Total time taken (in seconds): 206.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_tpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='tpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_tpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_tpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_tpu.loss(preds, outputs)\n",
        "    mlp_on_tpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_tpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_tpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NUTgor7a8kZY",
        "outputId": "e93b3ac1-06d5-4fcf-e051-ba5c454ade33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8113\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005380872802734375 \n",
            "\n",
            "Validation Accuracy: 0.8053\n",
            "\n",
            "Train Accuracy: 0.8407\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0035670187377929686 \n",
            "\n",
            "Validation Accuracy: 0.8341\n",
            "\n",
            "Train Accuracy: 0.8569\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.00314642822265625 \n",
            "\n",
            "Validation Accuracy: 0.8436\n",
            "\n",
            "Train Accuracy: 0.8518\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0029263494873046875 \n",
            "\n",
            "Validation Accuracy: 0.8400\n",
            "\n",
            "Train Accuracy: 0.8617\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0027509283447265625 \n",
            "\n",
            "Validation Accuracy: 0.8460\n",
            "\n",
            "Train Accuracy: 0.8800\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0026241912841796877 \n",
            "\n",
            "Validation Accuracy: 0.8627\n",
            "\n",
            "Train Accuracy: 0.8836\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002495645751953125 \n",
            "\n",
            "Validation Accuracy: 0.8647\n",
            "\n",
            "Train Accuracy: 0.8867\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0024072332763671873 \n",
            "\n",
            "Validation Accuracy: 0.8680\n",
            "\n",
            "Train Accuracy: 0.8969\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0023096492004394532 \n",
            "\n",
            "Validation Accuracy: 0.8756\n",
            "\n",
            "Train Accuracy: 0.8973\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0022480101013183595 \n",
            "\n",
            "Validation Accuracy: 0.8748\n",
            "\n",
            "Train Accuracy: 0.9015\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0021633085632324217 \n",
            "\n",
            "Validation Accuracy: 0.8763\n",
            "\n",
            "Train Accuracy: 0.9017\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0020963227844238283 \n",
            "\n",
            "Validation Accuracy: 0.8737\n",
            "\n",
            "Train Accuracy: 0.9035\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0020236289978027345 \n",
            "\n",
            "Validation Accuracy: 0.8775\n",
            "\n",
            "Train Accuracy: 0.9048\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0019644891357421877 \n",
            "\n",
            "Validation Accuracy: 0.8760\n",
            "\n",
            "Train Accuracy: 0.9078\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.001909723358154297 \n",
            "\n",
            "Validation Accuracy: 0.8770\n",
            "\n",
            "Train Accuracy: 0.9148\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018529653930664063 \n",
            "\n",
            "Validation Accuracy: 0.8835\n",
            "\n",
            "Train Accuracy: 0.9130\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.001800928955078125 \n",
            "\n",
            "Validation Accuracy: 0.8778\n",
            "\n",
            "Train Accuracy: 0.9137\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0017539419555664063 \n",
            "\n",
            "Validation Accuracy: 0.8788\n",
            "\n",
            "Train Accuracy: 0.9000\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0017092977905273437 \n",
            "\n",
            "Validation Accuracy: 0.8677\n",
            "\n",
            "Train Accuracy: 0.9178\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016709097290039062 \n",
            "\n",
            "Validation Accuracy: 0.8792\n",
            "\n",
            "Total time taken (in seconds): 205.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V-mdw2mH9BIQ",
        "outputId": "dc239a23-46e8-4d74-c326-52858139a0db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8118\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005457958374023438 \n",
            "\n",
            "Validation Accuracy: 0.8067\n",
            "\n",
            "Train Accuracy: 0.8392\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0036173028564453124 \n",
            "\n",
            "Validation Accuracy: 0.8310\n",
            "\n",
            "Train Accuracy: 0.8583\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.003162056884765625 \n",
            "\n",
            "Validation Accuracy: 0.8462\n",
            "\n",
            "Train Accuracy: 0.8685\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0029312518310546875 \n",
            "\n",
            "Validation Accuracy: 0.8579\n",
            "\n",
            "Train Accuracy: 0.8745\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002747274475097656 \n",
            "\n",
            "Validation Accuracy: 0.8578\n",
            "\n",
            "Train Accuracy: 0.8827\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002620014343261719 \n",
            "\n",
            "Validation Accuracy: 0.8673\n",
            "\n",
            "Train Accuracy: 0.8737\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0024806398010253907 \n",
            "\n",
            "Validation Accuracy: 0.8547\n",
            "\n",
            "Train Accuracy: 0.8805\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002389943542480469 \n",
            "\n",
            "Validation Accuracy: 0.8593\n",
            "\n",
            "Train Accuracy: 0.8948\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002292981872558594 \n",
            "\n",
            "Validation Accuracy: 0.8721\n",
            "\n",
            "Train Accuracy: 0.8883\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.002222164306640625 \n",
            "\n",
            "Validation Accuracy: 0.8644\n",
            "\n",
            "Train Accuracy: 0.8953\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0021410647583007814 \n",
            "\n",
            "Validation Accuracy: 0.8686\n",
            "\n",
            "Train Accuracy: 0.8994\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0020811880493164063 \n",
            "\n",
            "Validation Accuracy: 0.8688\n",
            "\n",
            "Train Accuracy: 0.9006\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.002011249542236328 \n",
            "\n",
            "Validation Accuracy: 0.8694\n",
            "\n",
            "Train Accuracy: 0.9031\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0019490672302246094 \n",
            "\n",
            "Validation Accuracy: 0.8708\n",
            "\n",
            "Train Accuracy: 0.9095\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.001896121826171875 \n",
            "\n",
            "Validation Accuracy: 0.8759\n",
            "\n",
            "Train Accuracy: 0.9009\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018386395263671875 \n",
            "\n",
            "Validation Accuracy: 0.8677\n",
            "\n",
            "Train Accuracy: 0.9099\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0017894630432128905 \n",
            "\n",
            "Validation Accuracy: 0.8739\n",
            "\n",
            "Train Accuracy: 0.9063\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.001732930908203125 \n",
            "\n",
            "Validation Accuracy: 0.8693\n",
            "\n",
            "Train Accuracy: 0.8949\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0016972735595703124 \n",
            "\n",
            "Validation Accuracy: 0.8589\n",
            "\n",
            "Train Accuracy: 0.9137\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016382525634765625 \n",
            "\n",
            "Validation Accuracy: 0.8740\n",
            "\n",
            "Total time taken (in seconds): 206.55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_default.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_default.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7K0Q49Kf9I3c",
        "outputId": "fd1ff59f-a5cc-4f37-cc67-8e96fbc34e18"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8240\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005279668579101562 \n",
            "\n",
            "Validation Accuracy: 0.8162\n",
            "\n",
            "Train Accuracy: 0.8480\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.003560540466308594 \n",
            "\n",
            "Validation Accuracy: 0.8384\n",
            "\n",
            "Train Accuracy: 0.8599\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0031429345703125 \n",
            "\n",
            "Validation Accuracy: 0.8467\n",
            "\n",
            "Train Accuracy: 0.8647\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.00293307861328125 \n",
            "\n",
            "Validation Accuracy: 0.8525\n",
            "\n",
            "Train Accuracy: 0.8673\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002754448547363281 \n",
            "\n",
            "Validation Accuracy: 0.8521\n",
            "\n",
            "Train Accuracy: 0.8759\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.00262025146484375 \n",
            "\n",
            "Validation Accuracy: 0.8575\n",
            "\n",
            "Train Accuracy: 0.8748\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002500082550048828 \n",
            "\n",
            "Validation Accuracy: 0.8562\n",
            "\n",
            "Train Accuracy: 0.8805\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0024016348266601563 \n",
            "\n",
            "Validation Accuracy: 0.8618\n",
            "\n",
            "Train Accuracy: 0.8888\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002301828918457031 \n",
            "\n",
            "Validation Accuracy: 0.8673\n",
            "\n",
            "Train Accuracy: 0.8895\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0022380023193359377 \n",
            "\n",
            "Validation Accuracy: 0.8668\n",
            "\n",
            "Train Accuracy: 0.8964\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0021568226623535154 \n",
            "\n",
            "Validation Accuracy: 0.8722\n",
            "\n",
            "Train Accuracy: 0.9003\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.002089212341308594 \n",
            "\n",
            "Validation Accuracy: 0.8738\n",
            "\n",
            "Train Accuracy: 0.8983\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.002029943389892578 \n",
            "\n",
            "Validation Accuracy: 0.8691\n",
            "\n",
            "Train Accuracy: 0.8717\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00196244384765625 \n",
            "\n",
            "Validation Accuracy: 0.8466\n",
            "\n",
            "Train Accuracy: 0.9049\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0019182487487792968 \n",
            "\n",
            "Validation Accuracy: 0.8711\n",
            "\n",
            "Train Accuracy: 0.8986\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00185270263671875 \n",
            "\n",
            "Validation Accuracy: 0.8684\n",
            "\n",
            "Train Accuracy: 0.8958\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0018016392517089844 \n",
            "\n",
            "Validation Accuracy: 0.8612\n",
            "\n",
            "Train Accuracy: 0.8892\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0017470997619628906 \n",
            "\n",
            "Validation Accuracy: 0.8580\n",
            "\n",
            "Train Accuracy: 0.8782\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0017034649658203124 \n",
            "\n",
            "Validation Accuracy: 0.8476\n",
            "\n",
            "Train Accuracy: 0.9047\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016615151977539063 \n",
            "\n",
            "Validation Accuracy: 0.8680\n",
            "\n",
            "Total time taken (in seconds): 203.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9m0tsn09dCd",
        "outputId": "ca103e59-23fb-4a22-9ef1-c0f69668a262"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0952\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_gpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWvaIS1r9i9P",
        "outputId": "c53d524c-c5de-4323-a0d5-c2e1441e8b0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1000\n",
            "\n",
            "Test Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_tpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_tpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_tpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRsnUDoK9jqD",
        "outputId": "02681ffd-86c8-4f1b-bd5c-cfd0c4e9fb34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0929\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_default.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_default.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8XaX3pA9kK2",
        "outputId": "59050042-1ded-4e73-8322-ecec4f16f1a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1012\n",
            "\n",
            "Test Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying Dropout penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on cpu function and not mlp on other modes."
      ],
      "metadata": {
        "id": "5AHqM4aU-HoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.dlayer1, self.dlayer2, self.dlayer3, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    h1 = self.dlayer1(h1)\n",
        "\n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    h2 = self.dlayer2(h2)\n",
        "\n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "    h3 = self.dlayer3(h3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "Saiuqvvo-U_g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0_29Lu2F-vnk",
        "outputId": "c55b4e55-e430-4bf2-9f51-f5a5c83f0cf3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8257\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0053565625 \n",
            "\n",
            "Validation Accuracy: 0.8192\n",
            "\n",
            "Train Accuracy: 0.8439\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0035661129760742186 \n",
            "\n",
            "Validation Accuracy: 0.8331\n",
            "\n",
            "Train Accuracy: 0.8565\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0031295550537109376 \n",
            "\n",
            "Validation Accuracy: 0.8460\n",
            "\n",
            "Train Accuracy: 0.8651\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.002919921569824219 \n",
            "\n",
            "Validation Accuracy: 0.8558\n",
            "\n",
            "Train Accuracy: 0.8742\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0027500442504882813 \n",
            "\n",
            "Validation Accuracy: 0.8624\n",
            "\n",
            "Train Accuracy: 0.8773\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002623426513671875 \n",
            "\n",
            "Validation Accuracy: 0.8603\n",
            "\n",
            "Train Accuracy: 0.8815\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0024925343322753906 \n",
            "\n",
            "Validation Accuracy: 0.8618\n",
            "\n",
            "Train Accuracy: 0.8865\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0024005955505371095 \n",
            "\n",
            "Validation Accuracy: 0.8675\n",
            "\n",
            "Train Accuracy: 0.8913\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0022990264892578127 \n",
            "\n",
            "Validation Accuracy: 0.8687\n",
            "\n",
            "Train Accuracy: 0.8944\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0022354054260253905 \n",
            "\n",
            "Validation Accuracy: 0.8706\n",
            "\n",
            "Train Accuracy: 0.9010\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0021546693420410154 \n",
            "\n",
            "Validation Accuracy: 0.8768\n",
            "\n",
            "Train Accuracy: 0.9034\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0020948277282714844 \n",
            "\n",
            "Validation Accuracy: 0.8756\n",
            "\n",
            "Train Accuracy: 0.9030\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.002026835632324219 \n",
            "\n",
            "Validation Accuracy: 0.8769\n",
            "\n",
            "Train Accuracy: 0.9074\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.001964757995605469 \n",
            "\n",
            "Validation Accuracy: 0.8797\n",
            "\n",
            "Train Accuracy: 0.9107\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.001922957763671875 \n",
            "\n",
            "Validation Accuracy: 0.8803\n",
            "\n",
            "Train Accuracy: 0.9106\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018557363891601563 \n",
            "\n",
            "Validation Accuracy: 0.8785\n",
            "\n",
            "Train Accuracy: 0.9117\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0018095114135742187 \n",
            "\n",
            "Validation Accuracy: 0.8782\n",
            "\n",
            "Train Accuracy: 0.9142\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.001756074981689453 \n",
            "\n",
            "Validation Accuracy: 0.8797\n",
            "\n",
            "Train Accuracy: 0.9102\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0017128956604003907 \n",
            "\n",
            "Validation Accuracy: 0.8750\n",
            "\n",
            "Train Accuracy: 0.9160\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016641122436523437 \n",
            "\n",
            "Validation Accuracy: 0.8790\n",
            "\n",
            "Total time taken (in seconds): 233.35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUvzKYRn_IQO",
        "outputId": "3667e18a-3f76-4352-b473-cdd9d14921a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0936\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 3:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l1 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on cpu function and not mlp on other modes."
      ],
      "metadata": {
        "id": "KVdoHy_Q-7yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L1 = (tf.reduce_sum(tf.math.abs(self.W1))+ tf.reduce_sum(tf.math.abs(self.W2))+tf.reduce_sum(tf.math.abs(self.W3)) + tf.reduce_sum(tf.math.abs(self.W4)))\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L1\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "92eCFcBK_Ftg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dFupnyIB_SK2",
        "outputId": "b8d7b5b9-b201-497a-8935-11414162f616"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8215\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0057369287109375 \n",
            "\n",
            "Validation Accuracy: 0.8170\n",
            "\n",
            "Train Accuracy: 0.8220\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.004371289367675781 \n",
            "\n",
            "Validation Accuracy: 0.8163\n",
            "\n",
            "Train Accuracy: 0.8122\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.004342825012207031 \n",
            "\n",
            "Validation Accuracy: 0.8034\n",
            "\n",
            "Train Accuracy: 0.8165\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.004496407470703125 \n",
            "\n",
            "Validation Accuracy: 0.8125\n",
            "\n",
            "Train Accuracy: 0.8184\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0045858816528320315 \n",
            "\n",
            "Validation Accuracy: 0.8152\n",
            "\n",
            "Train Accuracy: 0.8111\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.004631974792480469 \n",
            "\n",
            "Validation Accuracy: 0.8049\n",
            "\n",
            "Train Accuracy: 0.8111\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0043668795776367186 \n",
            "\n",
            "Validation Accuracy: 0.8063\n",
            "\n",
            "Train Accuracy: 0.8127\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.004224162292480469 \n",
            "\n",
            "Validation Accuracy: 0.8124\n",
            "\n",
            "Train Accuracy: 0.8341\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.004187511596679687 \n",
            "\n",
            "Validation Accuracy: 0.8321\n",
            "\n",
            "Train Accuracy: 0.8243\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.004318493041992187 \n",
            "\n",
            "Validation Accuracy: 0.8201\n",
            "\n",
            "Train Accuracy: 0.7901\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0039847500610351565 \n",
            "\n",
            "Validation Accuracy: 0.7829\n",
            "\n",
            "Train Accuracy: 0.7973\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0039664654541015625 \n",
            "\n",
            "Validation Accuracy: 0.7924\n",
            "\n",
            "Train Accuracy: 0.8323\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.003925751037597657 \n",
            "\n",
            "Validation Accuracy: 0.8284\n",
            "\n",
            "Train Accuracy: 0.8143\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0043351773071289065 \n",
            "\n",
            "Validation Accuracy: 0.8058\n",
            "\n",
            "Train Accuracy: 0.6827\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.003961909484863281 \n",
            "\n",
            "Validation Accuracy: 0.6782\n",
            "\n",
            "Train Accuracy: 0.8213\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.004037693786621094 \n",
            "\n",
            "Validation Accuracy: 0.8176\n",
            "\n",
            "Train Accuracy: 0.8024\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.003808457946777344 \n",
            "\n",
            "Validation Accuracy: 0.7981\n",
            "\n",
            "Train Accuracy: 0.8242\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.004178740539550781 \n",
            "\n",
            "Validation Accuracy: 0.8155\n",
            "\n",
            "Train Accuracy: 0.7947\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0037649444580078123 \n",
            "\n",
            "Validation Accuracy: 0.7875\n",
            "\n",
            "Train Accuracy: 0.7922\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.004736324462890625 \n",
            "\n",
            "Validation Accuracy: 0.7865\n",
            "\n",
            "Total time taken (in seconds): 306.83\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybvbLjbJ_SSe",
        "outputId": "ddb88d30-dbb7-4149-c305-bbc361eeabc0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1530\n",
            "\n",
            "Test Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 4:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l2 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on cpu function and not mlp on other modes."
      ],
      "metadata": {
        "id": "iPZIZXbs_b96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))/4\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "wfOxgGgP_cLA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oMbFi0rM_fSE",
        "outputId": "2b2dcfb3-3089-4a2f-b48f-2dad02561e0b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8209\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005381085815429687 \n",
            "\n",
            "Validation Accuracy: 0.8159\n",
            "\n",
            "Train Accuracy: 0.8503\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.00362435302734375 \n",
            "\n",
            "Validation Accuracy: 0.8409\n",
            "\n",
            "Train Accuracy: 0.8522\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0031934884643554685 \n",
            "\n",
            "Validation Accuracy: 0.8416\n",
            "\n",
            "Train Accuracy: 0.8561\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.00298568603515625 \n",
            "\n",
            "Validation Accuracy: 0.8438\n",
            "\n",
            "Train Accuracy: 0.8657\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002806798400878906 \n",
            "\n",
            "Validation Accuracy: 0.8506\n",
            "\n",
            "Train Accuracy: 0.8789\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0026924807739257812 \n",
            "\n",
            "Validation Accuracy: 0.8621\n",
            "\n",
            "Train Accuracy: 0.8754\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0025654156494140626 \n",
            "\n",
            "Validation Accuracy: 0.8610\n",
            "\n",
            "Train Accuracy: 0.8808\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002480463104248047 \n",
            "\n",
            "Validation Accuracy: 0.8647\n",
            "\n",
            "Train Accuracy: 0.8877\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0023946087646484373 \n",
            "\n",
            "Validation Accuracy: 0.8695\n",
            "\n",
            "Train Accuracy: 0.8821\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0023414443969726564 \n",
            "\n",
            "Validation Accuracy: 0.8638\n",
            "\n",
            "Train Accuracy: 0.8951\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0022728546142578125 \n",
            "\n",
            "Validation Accuracy: 0.8749\n",
            "\n",
            "Train Accuracy: 0.8966\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0022199681091308594 \n",
            "\n",
            "Validation Accuracy: 0.8752\n",
            "\n",
            "Train Accuracy: 0.8997\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0021631463623046875 \n",
            "\n",
            "Validation Accuracy: 0.8753\n",
            "\n",
            "Train Accuracy: 0.8900\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0021091993713378905 \n",
            "\n",
            "Validation Accuracy: 0.8644\n",
            "\n",
            "Train Accuracy: 0.9023\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.002067091064453125 \n",
            "\n",
            "Validation Accuracy: 0.8768\n",
            "\n",
            "Train Accuracy: 0.8970\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00202967041015625 \n",
            "\n",
            "Validation Accuracy: 0.8718\n",
            "\n",
            "Train Accuracy: 0.8845\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0019854144287109374 \n",
            "\n",
            "Validation Accuracy: 0.8577\n",
            "\n",
            "Train Accuracy: 0.8882\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.001967080993652344 \n",
            "\n",
            "Validation Accuracy: 0.8613\n",
            "\n",
            "Train Accuracy: 0.8625\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0019177745056152343 \n",
            "\n",
            "Validation Accuracy: 0.8365\n",
            "\n",
            "Train Accuracy: 0.9002\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0018882864379882811 \n",
            "\n",
            "Validation Accuracy: 0.8685\n",
            "\n",
            "Total time taken (in seconds): 372.61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUatt060_fYp",
        "outputId": "f475f5e0-3335-4675-dc1a-e7b77a9bdd6c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0953\n",
            "\n",
            "Test Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 5:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying a combination of l1 and l2 penalty/regularization[Elastic Net Regularization].Dropout regularization from Keras. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ],
      "metadata": {
        "id": "qK51vzkg_f_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L1 = (tf.reduce_sum(tf.math.abs(self.W1))+ tf.reduce_sum(tf.math.abs(self.W2))+tf.reduce_sum(tf.math.abs(self.W3)) + tf.reduce_sum(tf.math.abs(self.W4)))\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))/4\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L1 + 0.005 * L2\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "9ZUKvVNX_gwu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xWg85jXEAHa1",
        "outputId": "4fef1f6d-388c-4ae1-d0a3-a26baf0a746e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.7877\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005778128662109375 \n",
            "\n",
            "Validation Accuracy: 0.7879\n",
            "\n",
            "Train Accuracy: 0.8029\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.00466728515625 \n",
            "\n",
            "Validation Accuracy: 0.7916\n",
            "\n",
            "Train Accuracy: 0.7709\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.004755519104003906 \n",
            "\n",
            "Validation Accuracy: 0.7637\n",
            "\n",
            "Train Accuracy: 0.8045\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.00483935791015625 \n",
            "\n",
            "Validation Accuracy: 0.7991\n",
            "\n",
            "Train Accuracy: 0.8060\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0047833709716796876 \n",
            "\n",
            "Validation Accuracy: 0.7989\n",
            "\n",
            "Train Accuracy: 0.7839\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0046928054809570316 \n",
            "\n",
            "Validation Accuracy: 0.7813\n",
            "\n",
            "Train Accuracy: 0.8100\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.004504398803710938 \n",
            "\n",
            "Validation Accuracy: 0.8042\n",
            "\n",
            "Train Accuracy: 0.8069\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.004426817626953125 \n",
            "\n",
            "Validation Accuracy: 0.8009\n",
            "\n",
            "Train Accuracy: 0.8260\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.004397035522460938 \n",
            "\n",
            "Validation Accuracy: 0.8193\n",
            "\n",
            "Train Accuracy: 0.8123\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.00432670654296875 \n",
            "\n",
            "Validation Accuracy: 0.8050\n",
            "\n",
            "Train Accuracy: 0.7790\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.00427448486328125 \n",
            "\n",
            "Validation Accuracy: 0.7689\n",
            "\n",
            "Train Accuracy: 0.7673\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.004262619018554688 \n",
            "\n",
            "Validation Accuracy: 0.7583\n",
            "\n",
            "Train Accuracy: 0.8226\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.004236524658203125 \n",
            "\n",
            "Validation Accuracy: 0.8151\n",
            "\n",
            "Train Accuracy: 0.7775\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.004206279602050781 \n",
            "\n",
            "Validation Accuracy: 0.7702\n",
            "\n",
            "Train Accuracy: 0.8142\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.004195877380371094 \n",
            "\n",
            "Validation Accuracy: 0.8050\n",
            "\n",
            "Train Accuracy: 0.8254\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0041999044799804685 \n",
            "\n",
            "Validation Accuracy: 0.8199\n",
            "\n",
            "Train Accuracy: 0.8128\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.004176545715332031 \n",
            "\n",
            "Validation Accuracy: 0.8033\n",
            "\n",
            "Train Accuracy: 0.8117\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0041195263671875 \n",
            "\n",
            "Validation Accuracy: 0.8009\n",
            "\n",
            "Train Accuracy: 0.8104\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.004106329650878906 \n",
            "\n",
            "Validation Accuracy: 0.8001\n",
            "\n",
            "Train Accuracy: 0.8170\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0041323983764648435 \n",
            "\n",
            "Validation Accuracy: 0.8045\n",
            "\n",
            "Total time taken (in seconds): 328.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds_LuBNdAIpe",
        "outputId": "1fcd2d11-7baf-46de-ee2b-672c3dc81674"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1332\n",
            "\n",
            "Test Accuracy: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 6:**\n",
        "\n",
        "Hyper Parameter Optimization for Batch Size using Trial and Error"
      ],
      "metadata": {
        "id": "6T8qv6CK_ljB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "cmP2uC_o_ltz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(256) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PXLOTjQSAKe5",
        "outputId": "0231c635-b892-40b4-d202-1e6efb64deae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.7849\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.003197001953125 \n",
            "\n",
            "Validation Accuracy: 0.7777\n",
            "\n",
            "Train Accuracy: 0.8223\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0020624754333496095 \n",
            "\n",
            "Validation Accuracy: 0.8125\n",
            "\n",
            "Train Accuracy: 0.8145\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.00180589599609375 \n",
            "\n",
            "Validation Accuracy: 0.8051\n",
            "\n",
            "Train Accuracy: 0.8346\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0016674114990234375 \n",
            "\n",
            "Validation Accuracy: 0.8242\n",
            "\n",
            "Train Accuracy: 0.8445\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0015577162170410156 \n",
            "\n",
            "Validation Accuracy: 0.8309\n",
            "\n",
            "Train Accuracy: 0.8585\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0014909503173828125 \n",
            "\n",
            "Validation Accuracy: 0.8460\n",
            "\n",
            "Train Accuracy: 0.8576\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0014285127258300782 \n",
            "\n",
            "Validation Accuracy: 0.8460\n",
            "\n",
            "Train Accuracy: 0.8619\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0013768423461914064 \n",
            "\n",
            "Validation Accuracy: 0.8515\n",
            "\n",
            "Train Accuracy: 0.8709\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0013299807739257812 \n",
            "\n",
            "Validation Accuracy: 0.8615\n",
            "\n",
            "Train Accuracy: 0.8698\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.001289949188232422 \n",
            "\n",
            "Validation Accuracy: 0.8578\n",
            "\n",
            "Train Accuracy: 0.8787\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0012526011657714843 \n",
            "\n",
            "Validation Accuracy: 0.8621\n",
            "\n",
            "Train Accuracy: 0.8794\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.001218196029663086 \n",
            "\n",
            "Validation Accuracy: 0.8616\n",
            "\n",
            "Train Accuracy: 0.8898\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0011896858215332032 \n",
            "\n",
            "Validation Accuracy: 0.8702\n",
            "\n",
            "Train Accuracy: 0.8789\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0011568756103515624 \n",
            "\n",
            "Validation Accuracy: 0.8597\n",
            "\n",
            "Train Accuracy: 0.8895\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0011365516662597656 \n",
            "\n",
            "Validation Accuracy: 0.8671\n",
            "\n",
            "Train Accuracy: 0.8953\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.001102261962890625 \n",
            "\n",
            "Validation Accuracy: 0.8735\n",
            "\n",
            "Train Accuracy: 0.8804\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0010819190216064453 \n",
            "\n",
            "Validation Accuracy: 0.8603\n",
            "\n",
            "Train Accuracy: 0.8832\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0010651844787597656 \n",
            "\n",
            "Validation Accuracy: 0.8631\n",
            "\n",
            "Train Accuracy: 0.8822\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0010407583618164062 \n",
            "\n",
            "Validation Accuracy: 0.8602\n",
            "\n",
            "Train Accuracy: 0.8906\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.001018035659790039 \n",
            "\n",
            "Validation Accuracy: 0.8640\n",
            "\n",
            "Total time taken (in seconds): 171.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlHZahNMAaa6",
        "outputId": "b58fadbc-7f68-4def-8e13-32bc6db661bf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1002\n",
            "\n",
            "Test Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(64) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zF5X60HcJJpn",
        "outputId": "bdd7f0ef-4bb3-42c7-cfdf-a085dbcd4517"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.7529\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.009293170166015625 \n",
            "\n",
            "Validation Accuracy: 0.7435\n",
            "\n",
            "Train Accuracy: 0.7716\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.006466202392578125 \n",
            "\n",
            "Validation Accuracy: 0.7600\n",
            "\n",
            "Train Accuracy: 0.8430\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0057790887451171875 \n",
            "\n",
            "Validation Accuracy: 0.8319\n",
            "\n",
            "Train Accuracy: 0.7936\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.005331533203125 \n",
            "\n",
            "Validation Accuracy: 0.7785\n",
            "\n",
            "Train Accuracy: 0.8139\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.004995124816894531 \n",
            "\n",
            "Validation Accuracy: 0.7942\n",
            "\n",
            "Train Accuracy: 0.8724\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.004745496215820312 \n",
            "\n",
            "Validation Accuracy: 0.8546\n",
            "\n",
            "Train Accuracy: 0.8631\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.004469491882324219 \n",
            "\n",
            "Validation Accuracy: 0.8418\n",
            "\n",
            "Train Accuracy: 0.8846\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0042986767578125 \n",
            "\n",
            "Validation Accuracy: 0.8641\n",
            "\n",
            "Train Accuracy: 0.9051\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.004094411010742188 \n",
            "\n",
            "Validation Accuracy: 0.8812\n",
            "\n",
            "Train Accuracy: 0.8797\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.003944064025878906 \n",
            "\n",
            "Validation Accuracy: 0.8559\n",
            "\n",
            "Train Accuracy: 0.8879\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.003808597412109375 \n",
            "\n",
            "Validation Accuracy: 0.8575\n",
            "\n",
            "Train Accuracy: 0.9042\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.003646602478027344 \n",
            "\n",
            "Validation Accuracy: 0.8736\n",
            "\n",
            "Train Accuracy: 0.9069\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0035425286865234376 \n",
            "\n",
            "Validation Accuracy: 0.8723\n",
            "\n",
            "Train Accuracy: 0.9025\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0034033944702148437 \n",
            "\n",
            "Validation Accuracy: 0.8689\n",
            "\n",
            "Train Accuracy: 0.8234\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.003291136474609375 \n",
            "\n",
            "Validation Accuracy: 0.7935\n",
            "\n",
            "Train Accuracy: 0.9149\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.003173216247558594 \n",
            "\n",
            "Validation Accuracy: 0.8798\n",
            "\n",
            "Train Accuracy: 0.9080\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.003068558654785156 \n",
            "\n",
            "Validation Accuracy: 0.8730\n",
            "\n",
            "Train Accuracy: 0.9159\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0029681869506835937 \n",
            "\n",
            "Validation Accuracy: 0.8738\n",
            "\n",
            "Train Accuracy: 0.8866\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0028675579833984375 \n",
            "\n",
            "Validation Accuracy: 0.8486\n",
            "\n",
            "Train Accuracy: 0.8812\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0027561895751953124 \n",
            "\n",
            "Validation Accuracy: 0.8457\n",
            "\n",
            "Total time taken (in seconds): 388.83\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFsvBMiGJTHX",
        "outputId": "3fb88ba2-e4c1-4b74-9b85-af4bc0ccb620"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1704\n",
            "\n",
            "Test Accuracy: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 7:**\n",
        "\n",
        "Hyper Parameter Optimization for Learning Rate using Trial and Error"
      ],
      "metadata": {
        "id": "1i1iFf2x_nZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.0001)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "yrlhg0qF_n70"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5S81-VFwAMTx",
        "outputId": "44447ba7-f71b-49c5-cfc6-3ef8e0e0f785"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.1483\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.01903940673828125 \n",
            "\n",
            "Validation Accuracy: 0.1537\n",
            "\n",
            "Train Accuracy: 0.1703\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.01793320068359375 \n",
            "\n",
            "Validation Accuracy: 0.1821\n",
            "\n",
            "Train Accuracy: 0.2195\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0172122607421875 \n",
            "\n",
            "Validation Accuracy: 0.2302\n",
            "\n",
            "Train Accuracy: 0.2815\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0166519140625 \n",
            "\n",
            "Validation Accuracy: 0.2900\n",
            "\n",
            "Train Accuracy: 0.3355\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.01617206787109375 \n",
            "\n",
            "Validation Accuracy: 0.3409\n",
            "\n",
            "Train Accuracy: 0.3808\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.015736629638671874 \n",
            "\n",
            "Validation Accuracy: 0.3901\n",
            "\n",
            "Train Accuracy: 0.4210\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.015326689453125 \n",
            "\n",
            "Validation Accuracy: 0.4296\n",
            "\n",
            "Train Accuracy: 0.4556\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.014934058837890625 \n",
            "\n",
            "Validation Accuracy: 0.4622\n",
            "\n",
            "Train Accuracy: 0.4852\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.014555518798828125 \n",
            "\n",
            "Validation Accuracy: 0.4898\n",
            "\n",
            "Train Accuracy: 0.5083\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.014190137939453125 \n",
            "\n",
            "Validation Accuracy: 0.5149\n",
            "\n",
            "Train Accuracy: 0.5296\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.013836640625 \n",
            "\n",
            "Validation Accuracy: 0.5325\n",
            "\n",
            "Train Accuracy: 0.5481\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.013494290771484375 \n",
            "\n",
            "Validation Accuracy: 0.5498\n",
            "\n",
            "Train Accuracy: 0.5640\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.013161746826171874 \n",
            "\n",
            "Validation Accuracy: 0.5660\n",
            "\n",
            "Train Accuracy: 0.5775\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.01284099609375 \n",
            "\n",
            "Validation Accuracy: 0.5803\n",
            "\n",
            "Train Accuracy: 0.5901\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.01253080078125 \n",
            "\n",
            "Validation Accuracy: 0.5931\n",
            "\n",
            "Train Accuracy: 0.6011\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.01223193603515625 \n",
            "\n",
            "Validation Accuracy: 0.6043\n",
            "\n",
            "Train Accuracy: 0.6121\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.011944256591796876 \n",
            "\n",
            "Validation Accuracy: 0.6158\n",
            "\n",
            "Train Accuracy: 0.6214\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.01166855712890625 \n",
            "\n",
            "Validation Accuracy: 0.6264\n",
            "\n",
            "Train Accuracy: 0.6287\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.011404033203125 \n",
            "\n",
            "Validation Accuracy: 0.6355\n",
            "\n",
            "Train Accuracy: 0.6356\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.01115217041015625 \n",
            "\n",
            "Validation Accuracy: 0.6418\n",
            "\n",
            "Total time taken (in seconds): 210.74\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lhu_zALANLp",
        "outputId": "79e86047-0b8d-4c1c-b1c1-6b0f245b94ed"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.3537\n",
            "\n",
            "Test Accuracy: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "j7AsNBBJJk_T"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jp-kPIXZJvLx",
        "outputId": "75f9bf44-4595-4494-eed8-cb06c59d9e7e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.4195\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.016724569091796874 \n",
            "\n",
            "Validation Accuracy: 0.4217\n",
            "\n",
            "Train Accuracy: 0.5570\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.013323310546875 \n",
            "\n",
            "Validation Accuracy: 0.5610\n",
            "\n",
            "Train Accuracy: 0.6138\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.010878536376953124 \n",
            "\n",
            "Validation Accuracy: 0.6197\n",
            "\n",
            "Train Accuracy: 0.6518\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.009166394653320312 \n",
            "\n",
            "Validation Accuracy: 0.6561\n",
            "\n",
            "Train Accuracy: 0.6781\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.008069232177734375 \n",
            "\n",
            "Validation Accuracy: 0.6804\n",
            "\n",
            "Train Accuracy: 0.6996\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.007344639892578125 \n",
            "\n",
            "Validation Accuracy: 0.7030\n",
            "\n",
            "Train Accuracy: 0.7160\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.006832548828125 \n",
            "\n",
            "Validation Accuracy: 0.7214\n",
            "\n",
            "Train Accuracy: 0.7301\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.006449774169921875 \n",
            "\n",
            "Validation Accuracy: 0.7353\n",
            "\n",
            "Train Accuracy: 0.7414\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.006148858642578125 \n",
            "\n",
            "Validation Accuracy: 0.7443\n",
            "\n",
            "Train Accuracy: 0.7507\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.005903184814453125 \n",
            "\n",
            "Validation Accuracy: 0.7537\n",
            "\n",
            "Train Accuracy: 0.7588\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.005697406005859375 \n",
            "\n",
            "Validation Accuracy: 0.7611\n",
            "\n",
            "Train Accuracy: 0.7663\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00552200439453125 \n",
            "\n",
            "Validation Accuracy: 0.7660\n",
            "\n",
            "Train Accuracy: 0.7729\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.005369697265625 \n",
            "\n",
            "Validation Accuracy: 0.7711\n",
            "\n",
            "Train Accuracy: 0.7781\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.005237908935546875 \n",
            "\n",
            "Validation Accuracy: 0.7758\n",
            "\n",
            "Train Accuracy: 0.7833\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.005120684204101562 \n",
            "\n",
            "Validation Accuracy: 0.7792\n",
            "\n",
            "Train Accuracy: 0.7880\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0050164404296875 \n",
            "\n",
            "Validation Accuracy: 0.7837\n",
            "\n",
            "Train Accuracy: 0.7913\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.004922485656738281 \n",
            "\n",
            "Validation Accuracy: 0.7867\n",
            "\n",
            "Train Accuracy: 0.7945\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0048392733764648435 \n",
            "\n",
            "Validation Accuracy: 0.7896\n",
            "\n",
            "Train Accuracy: 0.7973\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.004762008056640625 \n",
            "\n",
            "Validation Accuracy: 0.7927\n",
            "\n",
            "Train Accuracy: 0.7997\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.004691990051269531 \n",
            "\n",
            "Validation Accuracy: 0.7951\n",
            "\n",
            "Total time taken (in seconds): 211.79\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHj7_U7OJw-M",
        "outputId": "f5d3d056-edcd-4ac4-c181-63ac26664258"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1547\n",
            "\n",
            "Test Accuracy: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 8:**\n",
        "\n",
        "Hyper Parameter Optimisation for Activation Function using Trial and Error"
      ],
      "metadata": {
        "id": "dHTYeqb7_otn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "gZasg1YS_qYC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o9rwxXujAOos",
        "outputId": "2ebe95cf-e6e7-44ae-b763-503eec68acae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8003\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.007371235961914063 \n",
            "\n",
            "Validation Accuracy: 0.7963\n",
            "\n",
            "Train Accuracy: 0.8279\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.004254449768066406 \n",
            "\n",
            "Validation Accuracy: 0.8206\n",
            "\n",
            "Train Accuracy: 0.8390\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.00376826416015625 \n",
            "\n",
            "Validation Accuracy: 0.8316\n",
            "\n",
            "Train Accuracy: 0.8480\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0035120046997070315 \n",
            "\n",
            "Validation Accuracy: 0.8400\n",
            "\n",
            "Train Accuracy: 0.8533\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0033405856323242187 \n",
            "\n",
            "Validation Accuracy: 0.8446\n",
            "\n",
            "Train Accuracy: 0.8586\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0032150225830078127 \n",
            "\n",
            "Validation Accuracy: 0.8505\n",
            "\n",
            "Train Accuracy: 0.8611\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0031121978759765627 \n",
            "\n",
            "Validation Accuracy: 0.8516\n",
            "\n",
            "Train Accuracy: 0.8657\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0030274560546875 \n",
            "\n",
            "Validation Accuracy: 0.8559\n",
            "\n",
            "Train Accuracy: 0.8684\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002955877685546875 \n",
            "\n",
            "Validation Accuracy: 0.8597\n",
            "\n",
            "Train Accuracy: 0.8703\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0028984927368164064 \n",
            "\n",
            "Validation Accuracy: 0.8609\n",
            "\n",
            "Train Accuracy: 0.8727\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0028437570190429687 \n",
            "\n",
            "Validation Accuracy: 0.8642\n",
            "\n",
            "Train Accuracy: 0.8749\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0027995660400390626 \n",
            "\n",
            "Validation Accuracy: 0.8644\n",
            "\n",
            "Train Accuracy: 0.8766\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0027501571655273436 \n",
            "\n",
            "Validation Accuracy: 0.8657\n",
            "\n",
            "Train Accuracy: 0.8781\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.002711703796386719 \n",
            "\n",
            "Validation Accuracy: 0.8642\n",
            "\n",
            "Train Accuracy: 0.8806\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.002679530029296875 \n",
            "\n",
            "Validation Accuracy: 0.8678\n",
            "\n",
            "Train Accuracy: 0.8812\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.002648117370605469 \n",
            "\n",
            "Validation Accuracy: 0.8661\n",
            "\n",
            "Train Accuracy: 0.8815\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.002615250549316406 \n",
            "\n",
            "Validation Accuracy: 0.8661\n",
            "\n",
            "Train Accuracy: 0.8840\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0025924951171875 \n",
            "\n",
            "Validation Accuracy: 0.8684\n",
            "\n",
            "Train Accuracy: 0.8843\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0025680804443359373 \n",
            "\n",
            "Validation Accuracy: 0.8678\n",
            "\n",
            "Train Accuracy: 0.8851\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0025488201904296875 \n",
            "\n",
            "Validation Accuracy: 0.8678\n",
            "\n",
            "Total time taken (in seconds): 358.44\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da48OVReAXoj",
        "outputId": "e14ba04e-8459-46e7-fe92-cb35d9386d0a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0981\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adagrad(learning_rate = 1e-4)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "oDGfqdeRKBMO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eyTtNL1yKBTA",
        "outputId": "296eb0b6-4f0f-49d4-c3db-9908fe5a26fb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.2124\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.017971292724609376 \n",
            "\n",
            "Validation Accuracy: 0.2137\n",
            "\n",
            "Train Accuracy: 0.3337\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.01652735107421875 \n",
            "\n",
            "Validation Accuracy: 0.3345\n",
            "\n",
            "Train Accuracy: 0.4501\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.015320919189453126 \n",
            "\n",
            "Validation Accuracy: 0.4500\n",
            "\n",
            "Train Accuracy: 0.5322\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.01417201171875 \n",
            "\n",
            "Validation Accuracy: 0.5377\n",
            "\n",
            "Train Accuracy: 0.5835\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.01306113037109375 \n",
            "\n",
            "Validation Accuracy: 0.5847\n",
            "\n",
            "Train Accuracy: 0.6084\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.01202735107421875 \n",
            "\n",
            "Validation Accuracy: 0.6112\n",
            "\n",
            "Train Accuracy: 0.6250\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.01110726318359375 \n",
            "\n",
            "Validation Accuracy: 0.6308\n",
            "\n",
            "Train Accuracy: 0.6388\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.01031533935546875 \n",
            "\n",
            "Validation Accuracy: 0.6416\n",
            "\n",
            "Train Accuracy: 0.6514\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.009647573852539063 \n",
            "\n",
            "Validation Accuracy: 0.6547\n",
            "\n",
            "Train Accuracy: 0.6626\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.009090535888671874 \n",
            "\n",
            "Validation Accuracy: 0.6634\n",
            "\n",
            "Train Accuracy: 0.6712\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.008627459106445313 \n",
            "\n",
            "Validation Accuracy: 0.6727\n",
            "\n",
            "Train Accuracy: 0.6778\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.008242216796875 \n",
            "\n",
            "Validation Accuracy: 0.6818\n",
            "\n",
            "Train Accuracy: 0.6850\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0079172607421875 \n",
            "\n",
            "Validation Accuracy: 0.6893\n",
            "\n",
            "Train Accuracy: 0.6912\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.007643504028320313 \n",
            "\n",
            "Validation Accuracy: 0.6943\n",
            "\n",
            "Train Accuracy: 0.6966\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.007407726440429687 \n",
            "\n",
            "Validation Accuracy: 0.6981\n",
            "\n",
            "Train Accuracy: 0.7017\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.007203829345703125 \n",
            "\n",
            "Validation Accuracy: 0.7022\n",
            "\n",
            "Train Accuracy: 0.7065\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.007024969482421875 \n",
            "\n",
            "Validation Accuracy: 0.7057\n",
            "\n",
            "Train Accuracy: 0.7114\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00686730224609375 \n",
            "\n",
            "Validation Accuracy: 0.7091\n",
            "\n",
            "Train Accuracy: 0.7154\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.006725239868164062 \n",
            "\n",
            "Validation Accuracy: 0.7122\n",
            "\n",
            "Train Accuracy: 0.7192\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.006597625122070312 \n",
            "\n",
            "Validation Accuracy: 0.7153\n",
            "\n",
            "Total time taken (in seconds): 340.53\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2SlT2mZKBZk",
        "outputId": "132c88cf-c96d-4fd4-93d0-fba742472af8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.2133\n",
            "\n",
            "Test Accuracy: 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 9:**\n",
        "\n",
        "Best Model HyperParameters: Model = cpu, LR = 0.1, Optimizer = SGD, Batch Size = 128 or 256(Same Acc), Regularization = Dropout,L2 or No Reg(Same Accuracy)"
      ],
      "metadata": {
        "id": "YxpyqfTw_rYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))/4\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "agOwXhsu_rn1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4-fT22TwARTA",
        "outputId": "59096f9f-1d5c-4677-b07e-0c63e087dae7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8352\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005342683715820313 \n",
            "\n",
            "Validation Accuracy: 0.8280\n",
            "\n",
            "Train Accuracy: 0.8536\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0035896554565429686 \n",
            "\n",
            "Validation Accuracy: 0.8453\n",
            "\n",
            "Train Accuracy: 0.8606\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0031597540283203125 \n",
            "\n",
            "Validation Accuracy: 0.8494\n",
            "\n",
            "Train Accuracy: 0.8678\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.002942143249511719 \n",
            "\n",
            "Validation Accuracy: 0.8555\n",
            "\n",
            "Train Accuracy: 0.8772\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002778352355957031 \n",
            "\n",
            "Validation Accuracy: 0.8613\n",
            "\n",
            "Train Accuracy: 0.8743\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002660474548339844 \n",
            "\n",
            "Validation Accuracy: 0.8590\n",
            "\n",
            "Train Accuracy: 0.8841\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0025424812316894533 \n",
            "\n",
            "Validation Accuracy: 0.8670\n",
            "\n",
            "Train Accuracy: 0.8762\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002453616485595703 \n",
            "\n",
            "Validation Accuracy: 0.8590\n",
            "\n",
            "Train Accuracy: 0.8795\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0023693397521972657 \n",
            "\n",
            "Validation Accuracy: 0.8594\n",
            "\n",
            "Train Accuracy: 0.8778\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.002310846405029297 \n",
            "\n",
            "Validation Accuracy: 0.8569\n",
            "\n",
            "Train Accuracy: 0.8853\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0022469384765625 \n",
            "\n",
            "Validation Accuracy: 0.8624\n",
            "\n",
            "Train Accuracy: 0.8927\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0021890541076660156 \n",
            "\n",
            "Validation Accuracy: 0.8687\n",
            "\n",
            "Train Accuracy: 0.8898\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0021303622436523438 \n",
            "\n",
            "Validation Accuracy: 0.8665\n",
            "\n",
            "Train Accuracy: 0.8901\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0020817852783203126 \n",
            "\n",
            "Validation Accuracy: 0.8633\n",
            "\n",
            "Train Accuracy: 0.8989\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0020503472900390623 \n",
            "\n",
            "Validation Accuracy: 0.8722\n",
            "\n",
            "Train Accuracy: 0.8904\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0020018585205078127 \n",
            "\n",
            "Validation Accuracy: 0.8666\n",
            "\n",
            "Train Accuracy: 0.8796\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.001957882080078125 \n",
            "\n",
            "Validation Accuracy: 0.8536\n",
            "\n",
            "Train Accuracy: 0.9020\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0019253102111816407 \n",
            "\n",
            "Validation Accuracy: 0.8699\n",
            "\n",
            "Train Accuracy: 0.8678\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0018957907104492188 \n",
            "\n",
            "Validation Accuracy: 0.8417\n",
            "\n",
            "Train Accuracy: 0.9044\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0018547335815429688 \n",
            "\n",
            "Validation Accuracy: 0.8718\n",
            "\n",
            "Total time taken (in seconds): 326.08\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDF2kH63AUyx",
        "outputId": "0416b9ba-ae7a-4456-9d21-d7cbfb770574"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0926\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST ACCURACY HAS INCREASED BY 0.01 FROM 0.86(Baseline Model), thanks to these measures.**"
      ],
      "metadata": {
        "id": "n2eLOt1jbWml"
      }
    }
  ]
}