{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST_597_Assignment2_MNIST_New.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bnfwshwE4hvt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(6447) #replaced 1234 with my emailID number\n",
        "tf.random.set_seed(6447) #replaced 1234 with my emailID number"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tzhXWC656pi",
        "outputId": "e5f652a9-f642-4e64-9d00-c4320f4be870"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqlq0mqE597P",
        "outputId": "6f41925a-d9e9-45ed-98a0-045757f13ed8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQF18YpN5-C4",
        "outputId": "9c5c9b9e-7fae-444c-80c3-09580bfa161c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255"
      ],
      "metadata": {
        "id": "jZtAM8qH6FCY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "id": "hKGQ0pvX6Kae"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 1:**\n",
        "\n",
        "Using default batch size without softmax activation in output layer, without any regularization to determine the Categorical Cross-Entropy of test dataset and determine the accuracy of default model in GPU,TPU,CPU and on default mode."
      ],
      "metadata": {
        "id": "6y3r3Mr_6LdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "VqnT8Hot68Gc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LBTr2myI7JIn",
        "outputId": "b7f355fa-1c69-4ff7-f5f0-8f891a0774cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9234\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00393859130859375 \n",
            "\n",
            "Validation Accuracy: 0.9305\n",
            "\n",
            "Train Accuracy: 0.9412\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0016870408630371093 \n",
            "\n",
            "Validation Accuracy: 0.9438\n",
            "\n",
            "Train Accuracy: 0.9578\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0012424580383300781 \n",
            "\n",
            "Validation Accuracy: 0.9548\n",
            "\n",
            "Train Accuracy: 0.9650\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0009818502807617188 \n",
            "\n",
            "Validation Accuracy: 0.9598\n",
            "\n",
            "Train Accuracy: 0.9676\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008192736053466797 \n",
            "\n",
            "Validation Accuracy: 0.9608\n",
            "\n",
            "Train Accuracy: 0.9741\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0007009522247314453 \n",
            "\n",
            "Validation Accuracy: 0.9653\n",
            "\n",
            "Train Accuracy: 0.9749\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0005963068008422852 \n",
            "\n",
            "Validation Accuracy: 0.9650\n",
            "\n",
            "Train Accuracy: 0.9803\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005219042205810547 \n",
            "\n",
            "Validation Accuracy: 0.9678\n",
            "\n",
            "Train Accuracy: 0.9833\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0004554986572265625 \n",
            "\n",
            "Validation Accuracy: 0.9698\n",
            "\n",
            "Train Accuracy: 0.9838\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.000395426139831543 \n",
            "\n",
            "Validation Accuracy: 0.9705\n",
            "\n",
            "Train Accuracy: 0.9859\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0003456953811645508 \n",
            "\n",
            "Validation Accuracy: 0.9703\n",
            "\n",
            "Train Accuracy: 0.9866\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0003018690299987793 \n",
            "\n",
            "Validation Accuracy: 0.9714\n",
            "\n",
            "Train Accuracy: 0.9880\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00025957902908325194 \n",
            "\n",
            "Validation Accuracy: 0.9716\n",
            "\n",
            "Train Accuracy: 0.9903\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00022540361404418946 \n",
            "\n",
            "Validation Accuracy: 0.9726\n",
            "\n",
            "Train Accuracy: 0.9912\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00019277523040771484 \n",
            "\n",
            "Validation Accuracy: 0.9712\n",
            "\n",
            "Train Accuracy: 0.9922\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00016632984161376953 \n",
            "\n",
            "Validation Accuracy: 0.9728\n",
            "\n",
            "Train Accuracy: 0.9930\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0001408107280731201 \n",
            "\n",
            "Validation Accuracy: 0.9739\n",
            "\n",
            "Train Accuracy: 0.9942\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0001231199550628662 \n",
            "\n",
            "Validation Accuracy: 0.9747\n",
            "\n",
            "Train Accuracy: 0.9948\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.00010350625038146973 \n",
            "\n",
            "Validation Accuracy: 0.9750\n",
            "\n",
            "Train Accuracy: 0.9960\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 8.752677917480469e-05 \n",
            "\n",
            "Validation Accuracy: 0.9748\n",
            "\n",
            "Total time taken (in seconds): 333.17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_gpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W2D4ZrOO7Vls",
        "outputId": "761ff3aa-60fa-42b8-8edd-426cac4ad632"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9188\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0039001004028320313 \n",
            "\n",
            "Validation Accuracy: 0.9245\n",
            "\n",
            "Train Accuracy: 0.9469\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0016829531860351563 \n",
            "\n",
            "Validation Accuracy: 0.9495\n",
            "\n",
            "Train Accuracy: 0.9602\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0012488506317138671 \n",
            "\n",
            "Validation Accuracy: 0.9589\n",
            "\n",
            "Train Accuracy: 0.9688\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0009958902740478516 \n",
            "\n",
            "Validation Accuracy: 0.9639\n",
            "\n",
            "Train Accuracy: 0.9742\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.000830152816772461 \n",
            "\n",
            "Validation Accuracy: 0.9670\n",
            "\n",
            "Train Accuracy: 0.9777\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.000711906509399414 \n",
            "\n",
            "Validation Accuracy: 0.9683\n",
            "\n",
            "Train Accuracy: 0.9796\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0006042739868164063 \n",
            "\n",
            "Validation Accuracy: 0.9684\n",
            "\n",
            "Train Accuracy: 0.9829\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005247521591186524 \n",
            "\n",
            "Validation Accuracy: 0.9706\n",
            "\n",
            "Train Accuracy: 0.9857\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.00045137794494628906 \n",
            "\n",
            "Validation Accuracy: 0.9722\n",
            "\n",
            "Train Accuracy: 0.9865\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.00039498775482177737 \n",
            "\n",
            "Validation Accuracy: 0.9719\n",
            "\n",
            "Train Accuracy: 0.9894\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0003401808929443359 \n",
            "\n",
            "Validation Accuracy: 0.9735\n",
            "\n",
            "Train Accuracy: 0.9903\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00030092397689819334 \n",
            "\n",
            "Validation Accuracy: 0.9731\n",
            "\n",
            "Train Accuracy: 0.9915\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00025639980316162107 \n",
            "\n",
            "Validation Accuracy: 0.9741\n",
            "\n",
            "Train Accuracy: 0.9924\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00022607471466064454 \n",
            "\n",
            "Validation Accuracy: 0.9748\n",
            "\n",
            "Train Accuracy: 0.9939\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0001917296028137207 \n",
            "\n",
            "Validation Accuracy: 0.9749\n",
            "\n",
            "Train Accuracy: 0.9948\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00016487358093261717 \n",
            "\n",
            "Validation Accuracy: 0.9744\n",
            "\n",
            "Train Accuracy: 0.9950\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00014008662223815918 \n",
            "\n",
            "Validation Accuracy: 0.9748\n",
            "\n",
            "Train Accuracy: 0.9959\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00012122116088867188 \n",
            "\n",
            "Validation Accuracy: 0.9750\n",
            "\n",
            "Train Accuracy: 0.9958\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.00010464973449707031 \n",
            "\n",
            "Validation Accuracy: 0.9738\n",
            "\n",
            "Train Accuracy: 0.9967\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 8.916372299194336e-05 \n",
            "\n",
            "Validation Accuracy: 0.9746\n",
            "\n",
            "Total time taken (in seconds): 280.34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_tpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='tpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_tpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_tpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_tpu.loss(preds, outputs)\n",
        "    mlp_on_tpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_tpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_tpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NUTgor7a8kZY",
        "outputId": "6459486f-a905-4ab2-9d5a-e063860e46c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9221\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.003832458801269531 \n",
            "\n",
            "Validation Accuracy: 0.9332\n",
            "\n",
            "Train Accuracy: 0.9446\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0016711297607421875 \n",
            "\n",
            "Validation Accuracy: 0.9486\n",
            "\n",
            "Train Accuracy: 0.9575\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.001253113784790039 \n",
            "\n",
            "Validation Accuracy: 0.9577\n",
            "\n",
            "Train Accuracy: 0.9648\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0009997321319580078 \n",
            "\n",
            "Validation Accuracy: 0.9622\n",
            "\n",
            "Train Accuracy: 0.9698\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008400882720947265 \n",
            "\n",
            "Validation Accuracy: 0.9638\n",
            "\n",
            "Train Accuracy: 0.9752\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.000712779541015625 \n",
            "\n",
            "Validation Accuracy: 0.9666\n",
            "\n",
            "Train Accuracy: 0.9744\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0006105321884155273 \n",
            "\n",
            "Validation Accuracy: 0.9662\n",
            "\n",
            "Train Accuracy: 0.9792\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005335707473754882 \n",
            "\n",
            "Validation Accuracy: 0.9677\n",
            "\n",
            "Train Accuracy: 0.9793\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0004666926193237305 \n",
            "\n",
            "Validation Accuracy: 0.9669\n",
            "\n",
            "Train Accuracy: 0.9793\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.00040667789459228516 \n",
            "\n",
            "Validation Accuracy: 0.9655\n",
            "\n",
            "Train Accuracy: 0.9838\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0003511195755004883 \n",
            "\n",
            "Validation Accuracy: 0.9694\n",
            "\n",
            "Train Accuracy: 0.9844\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00031245018005371096 \n",
            "\n",
            "Validation Accuracy: 0.9683\n",
            "\n",
            "Train Accuracy: 0.9883\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00026211719512939455 \n",
            "\n",
            "Validation Accuracy: 0.9703\n",
            "\n",
            "Train Accuracy: 0.9901\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00022718629837036134 \n",
            "\n",
            "Validation Accuracy: 0.9712\n",
            "\n",
            "Train Accuracy: 0.9920\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00019490358352661132 \n",
            "\n",
            "Validation Accuracy: 0.9712\n",
            "\n",
            "Train Accuracy: 0.9932\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00016648138046264648 \n",
            "\n",
            "Validation Accuracy: 0.9713\n",
            "\n",
            "Train Accuracy: 0.9937\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00013923166275024414 \n",
            "\n",
            "Validation Accuracy: 0.9715\n",
            "\n",
            "Train Accuracy: 0.9948\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.000122717924118042 \n",
            "\n",
            "Validation Accuracy: 0.9709\n",
            "\n",
            "Train Accuracy: 0.9953\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0001028774356842041 \n",
            "\n",
            "Validation Accuracy: 0.9715\n",
            "\n",
            "Train Accuracy: 0.9962\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 9.026141166687012e-05 \n",
            "\n",
            "Validation Accuracy: 0.9714\n",
            "\n",
            "Total time taken (in seconds): 198.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V-mdw2mH9BIQ",
        "outputId": "77381c29-41b1-4ea2-f892-18b34f70cdbc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9171\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.003797075500488281 \n",
            "\n",
            "Validation Accuracy: 0.9294\n",
            "\n",
            "Train Accuracy: 0.9369\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0016630039978027345 \n",
            "\n",
            "Validation Accuracy: 0.9417\n",
            "\n",
            "Train Accuracy: 0.9585\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0012318392944335938 \n",
            "\n",
            "Validation Accuracy: 0.9575\n",
            "\n",
            "Train Accuracy: 0.9649\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0009813417053222656 \n",
            "\n",
            "Validation Accuracy: 0.9622\n",
            "\n",
            "Train Accuracy: 0.9717\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008163775634765625 \n",
            "\n",
            "Validation Accuracy: 0.9667\n",
            "\n",
            "Train Accuracy: 0.9764\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0006954207611083984 \n",
            "\n",
            "Validation Accuracy: 0.9695\n",
            "\n",
            "Train Accuracy: 0.9787\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0005846036529541015 \n",
            "\n",
            "Validation Accuracy: 0.9694\n",
            "\n",
            "Train Accuracy: 0.9816\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005121396255493164 \n",
            "\n",
            "Validation Accuracy: 0.9701\n",
            "\n",
            "Train Accuracy: 0.9841\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0004429060363769531 \n",
            "\n",
            "Validation Accuracy: 0.9716\n",
            "\n",
            "Train Accuracy: 0.9832\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0003811008071899414 \n",
            "\n",
            "Validation Accuracy: 0.9694\n",
            "\n",
            "Train Accuracy: 0.9856\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.00033359443664550783 \n",
            "\n",
            "Validation Accuracy: 0.9707\n",
            "\n",
            "Train Accuracy: 0.9876\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.000289051456451416 \n",
            "\n",
            "Validation Accuracy: 0.9714\n",
            "\n",
            "Train Accuracy: 0.9891\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0002458114814758301 \n",
            "\n",
            "Validation Accuracy: 0.9726\n",
            "\n",
            "Train Accuracy: 0.9894\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0002147050666809082 \n",
            "\n",
            "Validation Accuracy: 0.9722\n",
            "\n",
            "Train Accuracy: 0.9914\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00018409534454345703 \n",
            "\n",
            "Validation Accuracy: 0.9735\n",
            "\n",
            "Train Accuracy: 0.9924\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0001597618865966797 \n",
            "\n",
            "Validation Accuracy: 0.9731\n",
            "\n",
            "Train Accuracy: 0.9935\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00013618974685668944 \n",
            "\n",
            "Validation Accuracy: 0.9741\n",
            "\n",
            "Train Accuracy: 0.9942\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00011880546569824219 \n",
            "\n",
            "Validation Accuracy: 0.9743\n",
            "\n",
            "Train Accuracy: 0.9953\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 9.809290885925293e-05 \n",
            "\n",
            "Validation Accuracy: 0.9748\n",
            "\n",
            "Train Accuracy: 0.9963\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 8.512205123901367e-05 \n",
            "\n",
            "Validation Accuracy: 0.9753\n",
            "\n",
            "Total time taken (in seconds): 189.98\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaB0lEQVR4nO3df4xV533n8ffHw4+WJhrbeOQl/Boa01bjWiXRLZtdZasoNAFb6+CsrBYv6rJbpGkkkGJttxsoUmtbRSrZTbB2a2c1WahZaxRgnXQzjpx1HWwpWmkNXFxsDA7riQEbROwpJuNESNiDv/vHeca+5/rOzBnur/nxeUlXc85znvPc51xf3w/nPOeHIgIzM7NRN7S7A2ZmNrU4GMzMLMfBYGZmOQ4GMzPLcTCYmVnOnHZ3oBFuueWW6O7ubnc3zMymlWPHjv1jRHRVl8+IYOju7qZcLre7G2Zm04qkc7XKfSjJzMxyHAxmZpbjYDAzsxwHg5mZ5TgYzMwsZ9YGQ/+Jfrof7uaGB2+g++Fu+k/0t7tLZmZTwow4XXWy+k/00/tkL1feuwLAueFz9D7ZC8DGOza2s2tmZm03K/cYdhza8UEojLry3hV2HNrRph6ZmU0dszIYXh9+fVLlZmazyawMhmWdyyZVbmY2m8zKYNi5ZicL5i7IlS2Yu4Cda3a2qUdmZlPHrAyGjXdspO/uPpZ3LkeI5Z3L6bu7zwPPZmaAZsIzn0ulUvgmemZmkyPpWESUqstn5R6DmZmNzcFgZmY5DgYzM8txMJiZWY6DwczMchwMZmaW42AwM7McB4OZmeUUCgZJ6ySdljQoaVuN5fMlHUjLD0vqrli2PZWflrQ2lf2KpCOSXpR0UtKDFfUfk3RG0vH0WlX/ZpqZWVETPo9BUgfwCPAF4DxwVNJARJyqqLYZuBwRt0naAOwC/lBSD7ABuB34BPAjSb8BXAU+HxG/lDQX+D+SfhgRz6f2/iwinmjURpqZWXFF9hhWA4MR8VpEvAvsB9ZX1VkP7EvTTwBrJCmV74+IqxFxBhgEVkfml6n+3PSa/vfmMDObAYoEw2LgjYr586msZp2IGAGGgYXjrSupQ9Jx4C3gmYg4XFFvp6SXJO2WNL9WpyT1SipLKg8NDRXYDDMzK6Jtg88RcS0iVgFLgNWSfjst2g78FvC7wM3A18ZYvy8iShFR6urqakmfzcxmgyLBcAFYWjG/JJXVrCNpDtAJXCqybkT8HHgOWJfmL6ZDTVeBvyU7lGVmZi1SJBiOAislrZA0j2wweaCqzgCwKU3fCzwb2f28B4AN6aylFcBK4IikLkk3Akj6VbKB7Z+k+UXpr4B7gJfr2UAzM5ucCc9KiogRSVuBp4EOYG9EnJT0EFCOiAFgD/C4pEHgbbLwINU7CJwCRoAtEXEt/fjvS2c83QAcjIgfpLfsl9QFCDgOfKWRG2xmZuPzg3rMzGYpP6jHzMwKcTCYmVmOg8HMzHIcDGZmluNgMDOzHAeDmZnlOBjMzCzHwWBmZjkOBjMzy3EwmJlZjoPBzMxyHAxmZpbjYDAzsxwHg5mZ5TgYzMwsx8FgZmY5DgYzM8txMJiZWU6hYJC0TtJpSYOSttVYPl/SgbT8sKTuimXbU/lpSWtT2a9IOiLpRUknJT1YUX9FamMwtTmv/s00M7OiJgwGSR3AI8CdQA9wn6SeqmqbgcsRcRuwG9iV1u0BNgC3A+uAR1N7V4HPR8TvAKuAdZI+k9raBexObV1ObZuZWYsU2WNYDQxGxGsR8S6wH1hfVWc9sC9NPwGskaRUvj8irkbEGWAQWB2ZX6b6c9Mr0jqfT22Q2rznOrfNzMyuQ5FgWAy8UTF/PpXVrBMRI8AwsHC8dSV1SDoOvAU8ExGH0zo/T22M9V6k9XsllSWVh4aGCmyGmZkV0bbB54i4FhGrgCXAakm/Pcn1+yKiFBGlrq6u5nTSzGwWKhIMF4ClFfNLUlnNOpLmAJ3ApSLrRsTPgefIxiAuATemNsZ6LzMza6IiwXAUWJnOFppHNpg8UFVnANiUpu8Fno2ISOUb0llLK4CVwBFJXZJuBJD0q8AXgJ+kdZ5LbZDa/P71b56ZmU3WnIkqRMSIpK3A00AHsDciTkp6CChHxACwB3hc0iDwNll4kOodBE4BI8CWiLgmaRGwL52hdANwMCJ+kN7ya8B+SX8F/ENq28zMWkTZP9Knt1KpFOVyud3dMDObViQdi4hSdbmvfDYzsxwHg5mZ5TgYzMwsx8FgZmY5DgYzM8txMJiZWY6DwczMchwMZmaW42AwM7McB4OZmeU4GMzMLMfBYGZmOQ4GMzPLcTCYmVmOg8HMzHIcDGZmluNgMDOznELBIGmdpNOSBiVtq7F8vqQDaflhSd0Vy7an8tOS1qaypZKek3RK0klJX62o/4CkC5KOp9dd9W+mmZkVNeEzn9NzmR8BvgCcB45KGoiIUxXVNgOXI+I2SRuAXcAfSuohe/7z7cAngB9J+g2y5z//aUS8IOnjwDFJz1S0uTsi/nOjNtLMzIorssewGhiMiNci4l1gP7C+qs56YF+afgJYI0mpfH9EXI2IM8AgsDoiLkbECwAR8QvgFWBx/ZtjZmb1KhIMi4E3KubP89Ef8Q/qRMQIMAwsLLJuOuz0KeBwRfFWSS9J2ivpplqdktQrqSypPDQ0VGAzzMysiLYOPkv6GPBd4P6IeCcVfwv4JLAKuAh8o9a6EdEXEaWIKHV1dbWkv2Zms0GRYLgALK2YX5LKataRNAfoBC6Nt66kuWSh0B8R3xutEBFvRsS1iHgf+DbZoSwzM2uRIsFwFFgpaYWkeWSDyQNVdQaATWn6XuDZiIhUviGdtbQCWAkcSeMPe4BXIuKblQ1JWlQx+2Xg5clulJmZXb8Jz0qKiBFJW4GngQ5gb0SclPQQUI6IAbIf+cclDQJvk4UHqd5B4BTZmUhbIuKapM8CfwSckHQ8vdWfR8RTwNclrQICOAv8SQO318zMJqDsH/bTW6lUinK53O5umJlNK5KORUSputxXPpuZWY6DwczMchwMZmaW42AwM7McB4OZmeU4GMzMLMfBYGZmOQ4GMzPLcTCYmVmOg8HMzHIcDGZmluNgMDOzHAeDmZnlOBjMzCzHwWBmZjkOBjMzy3EwmJlZjoPBzMxyCgWDpHWSTksalLStxvL5kg6k5YcldVcs257KT0tam8qWSnpO0ilJJyV9taL+zZKekfRq+ntT/ZtpZmZFTRgMkjqAR4A7gR7gPkk9VdU2A5cj4jZgN7ArrdsDbABuB9YBj6b2RoA/jYge4DPAloo2twGHImIlcCjNm5lZixTZY1gNDEbEaxHxLrAfWF9VZz2wL00/AayRpFS+PyKuRsQZYBBYHREXI+IFgIj4BfAKsLhGW/uAe65v08zM7HoUCYbFwBsV8+f58Ef8I3UiYgQYBhYWWTcddvoUcDgV3RoRF9P0z4Bba3VKUq+ksqTy0NBQgc0wM7Mi2jr4LOljwHeB+yPinerlERFA1Fo3IvoiohQRpa6urib31Mxs9igSDBeApRXzS1JZzTqS5gCdwKXx1pU0lywU+iPiexV13pS0KNVZBLxVdGPMzKx+RYLhKLBS0gpJ88gGkweq6gwAm9L0vcCz6V/7A8CGdNbSCmAlcCSNP+wBXomIb47T1ibg+5PdKDMzu35zJqoQESOStgJPAx3A3og4KekhoBwRA2Q/8o9LGgTeJgsPUr2DwCmyM5G2RMQ1SZ8F/gg4Iel4eqs/j4ingL8GDkraDJwD/qCRG2xmZuNT9g/76a1UKkW5XG53N8zMphVJxyKiVF3uK5/NzCzHwWBmZjkOBjMzy3EwmJlZjoPBzMxyHAxmZpbjYDAzsxwHg5mZ5TgYzMwsx8FwnfpP9NP9cDc3PHgD3Q9303+iv91dMjNriAnvlWQf1X+in94ne7ny3hUAzg2fo/fJXgA23rGxnV0zM6ub9xiuw45DOz4IhVFX3rvCjkM72tQjM7PGcTBch9eHX59UuZnZdOJguA7LOpdNqtzMbDpxMFyHnWt2smDuglzZgrkL2LlmZ5t6ZGbWOA6G67Dxjo303d3H8s7lCLG8czl9d/d54NnMZgQ/qMfMbJaq60E9ktZJOi1pUNK2GsvnSzqQlh+W1F2xbHsqPy1pbUX5XklvSXq5qq0HJF2QdDy97prMhpqZWX0mDAZJHcAjwJ1AD3CfpJ6qapuByxFxG7Ab2JXW7SF7/vPtwDrg0dQewGOprJbdEbEqvZ6a3CaZmVk9iuwxrAYGI+K1iHgX2A+sr6qzHtiXpp8A1khSKt8fEVcj4gwwmNojIn4MvN2AbTAzswYqEgyLgTcq5s+nspp1ImIEGAYWFly3lq2SXkqHm24qUN/MzBpkKp6V9C3gk8Aq4CLwjVqVJPVKKksqDw0NtbJ/ZmYzWpFguAAsrZhfkspq1pE0B+gELhVcNyci3oyIaxHxPvBt0qGnGvX6IqIUEaWurq4Cm2FmZkUUCYajwEpJKyTNIxtMHqiqMwBsStP3As9Gdh7sALAhnbW0AlgJHBnvzSQtqpj9MvDyWHXNzKzxJry7akSMSNoKPA10AHsj4qSkh4ByRAwAe4DHJQ2SDShvSOuelHQQOAWMAFsi4hqApO8AnwNukXQe+MuI2AN8XdIqIICzwJ80coPNzGx8vsDNzGyWqusCNzMzmz0cDGZmluNgMDOzHAeDmZnlOBjMzCzHwWBmZjkOhjbpP9FP98Pd3PDgDXQ/3E3/if52d8nMDChwgZs1Xv+Jfnqf7OXKe1cAODd8jt4newH8FDgzazvvMbTBjkM7PgiFUVfeu8KOQzva1CMzsw85GNrg9eHXJ1VuZtZKDoY2WNa5bFLlZmat5GBog51rdrJg7oJc2YK5C9i5ZmebemRm9iEHQxtsvGMjfXf3sbxzOUIs71xO3919Hng2synBd1c1M5ulfHdVMzMrxMFgZmY5DgYzM8txMJiZWU6hYJC0TtJpSYOSttVYPl/SgbT8sKTuimXbU/lpSWsryvdKekvSy1Vt3SzpGUmvpr83Xf/mmZnZZE0YDJI6gEeAO4Ee4D5JPVXVNgOXI+I2YDewK63bA2wAbgfWAY+m9gAeS2XVtgGHImIlcCjNWxXfhM/MmqXIHsNqYDAiXouId4H9wPqqOuuBfWn6CWCNJKXy/RFxNSLOAIOpPSLix8DbNd6vsq19wD2T2J5ZYfQmfOeGzxHEBzfhcziYWSMUCYbFwBsV8+dTWc06ETECDAMLC65b7daIuJimfwbcWquSpF5JZUnloaGhApsxc/gmfGbWTFN68Dmyq+9qXoEXEX0RUYqIUldXV4t71l6+CZ+ZNVORYLgALK2YX5LKataRNAfoBC4VXLfam5IWpbYWAW8V6OOs4pvwmVkzFQmGo8BKSSskzSMbTB6oqjMAbErT9wLPpn/tDwAb0llLK4CVwJEJ3q+yrU3A9wv0cVbxTfjMrJkmDIY0ZrAVeBp4BTgYESclPSTpS6naHmChpEHg35POJIqIk8BB4BTwv4EtEXENQNJ3gP8L/Kak85I2p7b+GviCpFeB30/zVsE34TOzZvJN9MzMZinfRM/MzApxMMxSvkDOzMYyp90dsNYbvUBu9FqI0QvkAI9TmJn3GGYjXyBnZuNxMMxCvkDOzMbjYJiFfIGcmY3HwTAL+QI5MxuPg2EWasQFcj6ryWzm8gVuNmnVZzVBtsfhq6/Nphdf4GYN47OazGY2B4NNms9qMpvZHAw2aT6ryWxmczDYpPmsJrOZzcFgk+azmsxmNp+VZC3ns5rMpgaflWRThs9qMpvaHAzWcj6ryWxqczBYyzXirCaPUZg1T6FgkLRO0mlJg5K21Vg+X9KBtPywpO6KZdtT+WlJaydqU9Jjks5IOp5eq+rbRJtq6j2raXSM4tzwOYL44HkSDgezxpgwGCR1AI8AdwI9wH2SeqqqbQYuR8RtwG5gV1q3B9gA3A6sAx6V1FGgzT+LiFXpdbyuLbQpp96zmjxGYdZcRZ7gthoYjIjXACTtB9YDpyrqrAceSNNPAH8jSal8f0RcBc5IGkztUaBNm8E23rHxus9A8hiFWXMVOZS0GHijYv58KqtZJyJGgGFg4TjrTtTmTkkvSdotaX6tTknqlVSWVB4aGiqwGTZTeIzCrLmm4uDzduC3gN8Fbga+VqtSRPRFRCkiSl1dXa3sn7WZxyjMmqtIMFwAllbML0llNetImgN0ApfGWXfMNiPiYmSuAn/Lh4eezACPUZg1W5FgOAqslLRC0jyyweSBqjoDwKY0fS/wbGSXVA8AG9JZSyuAlcCR8dqUtCj9FXAP8HI9G2gz08Y7NnL2/rO8/5fvc/b+s5Mar2jEGIUPRdlMNuHgc0SMSNoKPA10AHsj4qSkh4ByRAwAe4DH0+Dy22Q/9KR6B8kGlUeALRFxDaBWm+kt+yV1AQKOA19p3OaaZWMR54bP1SwvovqWHqOHogDf0sNmBN8ryWadeu/V1P1wd81gWd65nLP3n21kV82ayvdKMkvqHaPwoSib6Ypcx2A249RzHYUPRdlM5z0Gs0mq93TZRpwV5T0OayYHg9kktftQlK/DsGbz4LNZi9U7eN2Iwe/+E/3sOLSD14dfZ1nnMnau2enDWLOQB5/Npoh6D0V5j8OazcFg1mL1Hoqq915RHuOwifisJLM2qOesqJ1rdta8DqPVexw+q2rm8h6D2TTjPQ5rNgeD2TRUz72iZsIYh4OluRwMZrPMdN/jcLA0n4PBbBaaznscDpbmczCY2aS0e49jJgTLaDtTNVwcDGY2ae3c45juwQJTf6/FwWBmLVXvHsd0DxaYOnstY3EwmFnL1bPHMd2DBabGXst4fIGbmU079VwgOLre9d4rqt4LDKH+W7c3Yq9lPIX2GCStk3Ra0qCkbTWWz5d0IC0/LKm7Ytn2VH5a0tqJ2kzPgT6cyg+kZ0KbmTVMO/dYYGrstYxnwmCQ1AE8AtwJ9AD3SeqpqrYZuBwRtwG7gV1p3R6y5z/fDqwDHpXUMUGbu4Ddqa3LqW0zsymjnmAZXb+dh8MmUuRQ0mpgMCJeA5C0H1gPnKqosx54IE0/AfyNJKXy/RFxFTgjaTC1R602Jb0CfB7416nOvtTut65r68zMpqh2Hg6bSJFgWAy8UTF/HvinY9WJiBFJw8DCVP581bqL03StNhcCP4+IkRr1cyT1Ar0Ay5Y1ZvfJzGy6qCdYJjJtz0qKiL6IKEVEqaurq93dMTObMYoEwwVgacX8klRWs46kOUAncGmcdccqvwTcmNoY673MzKyJigTDUWBlOltoHtlg8kBVnQFgU5q+F3g2smeGDgAb0llLK4CVwJGx2kzrPJfaILX5/evfPDMzm6wJxxjSmMFW4GmgA9gbESclPQSUI2IA2AM8ngaX3yb7oSfVO0g2UD0CbImIawC12kxv+TVgv6S/Av4htW1mZi2i7B/p01upVIpyudzubpiZTSuSjkVE6SPlMyEYJA0BH72McGq4BfjHdndiHO5ffdy/+rh/9aunj8sj4iNn78yIYJjKJJVrJfJU4f7Vx/2rj/tXv2b0cdqermpmZs3hYDAzsxwHQ/P1tbsDE3D/6uP+1cf9q1/D++gxBjMzy/Eeg5mZ5TgYzMwsx8HQAJKWSnpO0ilJJyV9tUadz0kalnQ8vf6ixX08K+lEeu+PXA2ozH9JD0h6SdKnW9i336z4XI5LekfS/VV1Wvr5Sdor6S1JL1eU3SzpGUmvpr83jbHuplTnVUmbatVpUv/+k6SfpP9+fyfpxjHWHfe70MT+PSDpQsV/w7vGWHfcB4M1sX8HKvp2VtLxMdZtxedX8zelZd/BiPCrzhewCPh0mv448P+Anqo6nwN+0MY+ngVuGWf5XcAPAQGfAQ63qZ8dwM/ILrxp2+cH/B7waeDlirKvA9vS9DZgV431bgZeS39vStM3tah/XwTmpOldtfpX5LvQxP49APyHAv/9fwr8OjAPeLH6/6Vm9a9q+TeAv2jj51fzN6VV30HvMTRARFyMiBfS9C+AVxjjORJT2Hrgf0TmebK73C5qQz/WAD+NiLZeyR4RPya771el9WQPjyL9vafGqmuBZyLi7Yi4DDxD9vTCpvcvIv4+PnyWyfNkdyduizE+vyI+eDBYRLwLjD4YrKHG658kAX8AfKfR71vUOL8pLfkOOhgaTNnzrj8FHK6x+J9JelHSDyXd3tKOQQB/L+mYsoccVav1QKZ2hNsGxv4fsp2fH8CtEXExTf8MuLVGnanyOf4x2R5gLRN9F5ppazrUtXeMwyBT4fP7F8CbEfHqGMtb+vlV/aa05DvoYGggSR8DvgvcHxHvVC1+gezwyO8A/xX4Xy3u3mcj4tNkz9neIun3Wvz+E1J2C/YvAf+zxuJ2f345ke2zT8lzvSXtILubcf8YVdr1XfgW8ElgFXCR7HDNVHQf4+8ttOzzG+83pZnfQQdDg0iaS/YfsD8ivle9PCLeiYhfpumngLmSbmlV/yLiQvr7FvB3fPjs7VFFHsjUbHcCL0TEm9UL2v35JW+OHl5Lf9+qUaetn6Okfwv8S2Bj+uH4iALfhaaIiDcj4lpEvA98e4z3bffnNwf4V8CBseq06vMb4zelJd9BB0MDpGOSe4BXIuKbY9T5J6keklaTffaXWtS/X5P08dFpskHKl6uqDQD/Jp2d9BlguGKXtVXG/JdaOz+/CpUPpBrrIVJPA1+UdFM6VPLFVNZ0ktYB/xH4UkRcGaNOke9Cs/pXOWb15THet8iDwZrp94GfRMT5Wgtb9fmN85vSmu9gM0fWZ8sL+CzZLt1LwPH0ugv4CvCVVGcrcJLsLIvngX/ewv79enrfF1MfdqTyyv4JeITsjJATQKnFn+Gvkf3Qd1aUte3zIwuoi8B7ZMdoNwMLgUPAq8CPgJtT3RLw3yvW/WNgML3+XQv7N0h2bHn0O/jfUt1PAE+N911oUf8eT9+tl8h+4BZV9y/N30V2Fs5PW9m/VP7Y6Heuom47Pr+xflNa8h30LTHMzCzHh5LMzCzHwWBmZjkOBjMzy3EwmJlZjoPBzMxyHAxmZpbjYDAzs5z/D37NtAVAca1MAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_default = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_default.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_default.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7K0Q49Kf9I3c",
        "outputId": "7b4f7b40-6d29-48cd-8e5f-37cd0c523f3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9269\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00384845458984375 \n",
            "\n",
            "Validation Accuracy: 0.9343\n",
            "\n",
            "Train Accuracy: 0.9357\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.001679119873046875 \n",
            "\n",
            "Validation Accuracy: 0.9419\n",
            "\n",
            "Train Accuracy: 0.9562\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0012357581329345702 \n",
            "\n",
            "Validation Accuracy: 0.9551\n",
            "\n",
            "Train Accuracy: 0.9588\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0009752535247802734 \n",
            "\n",
            "Validation Accuracy: 0.9564\n",
            "\n",
            "Train Accuracy: 0.9680\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008210462188720703 \n",
            "\n",
            "Validation Accuracy: 0.9620\n",
            "\n",
            "Train Accuracy: 0.9726\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0006989602661132812 \n",
            "\n",
            "Validation Accuracy: 0.9657\n",
            "\n",
            "Train Accuracy: 0.9737\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0005962605667114258 \n",
            "\n",
            "Validation Accuracy: 0.9654\n",
            "\n",
            "Train Accuracy: 0.9792\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005209699249267578 \n",
            "\n",
            "Validation Accuracy: 0.9680\n",
            "\n",
            "Train Accuracy: 0.9810\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.00045332023620605466 \n",
            "\n",
            "Validation Accuracy: 0.9686\n",
            "\n",
            "Train Accuracy: 0.9822\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0003889390182495117 \n",
            "\n",
            "Validation Accuracy: 0.9688\n",
            "\n",
            "Train Accuracy: 0.9855\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.00034070827484130857 \n",
            "\n",
            "Validation Accuracy: 0.9714\n",
            "\n",
            "Train Accuracy: 0.9873\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0002938446617126465 \n",
            "\n",
            "Validation Accuracy: 0.9723\n",
            "\n",
            "Train Accuracy: 0.9878\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00025417106628417967 \n",
            "\n",
            "Validation Accuracy: 0.9713\n",
            "\n",
            "Train Accuracy: 0.9881\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00022240755081176758 \n",
            "\n",
            "Validation Accuracy: 0.9712\n",
            "\n",
            "Train Accuracy: 0.9912\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00018938379287719727 \n",
            "\n",
            "Validation Accuracy: 0.9722\n",
            "\n",
            "Train Accuracy: 0.9924\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00016271753311157227 \n",
            "\n",
            "Validation Accuracy: 0.9726\n",
            "\n",
            "Train Accuracy: 0.9933\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00013755295753479005 \n",
            "\n",
            "Validation Accuracy: 0.9737\n",
            "\n",
            "Train Accuracy: 0.9948\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00011874613761901855 \n",
            "\n",
            "Validation Accuracy: 0.9749\n",
            "\n",
            "Train Accuracy: 0.9952\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 9.87147331237793e-05 \n",
            "\n",
            "Validation Accuracy: 0.9739\n",
            "\n",
            "Train Accuracy: 0.9960\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 8.910606384277343e-05 \n",
            "\n",
            "Validation Accuracy: 0.9744\n",
            "\n",
            "Total time taken (in seconds): 200.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9m0tsn09dCd",
        "outputId": "89cae5ff-d6f1-4747-fc3a-01544b17693d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0249\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_gpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWvaIS1r9i9P",
        "outputId": "bd21a345-ea94-4ddc-b502-e02b003a9793"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0251\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_tpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_tpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_tpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRsnUDoK9jqD",
        "outputId": "a38a54ae-7a15-45f1-cc40-a84e1279f4c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0262\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_default.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_default.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8XaX3pA9kK2",
        "outputId": "65b5801b-6c37-4197-992e-93473237da9e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0273\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 2:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying Dropout penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on cpu function and not mlp on other modes."
      ],
      "metadata": {
        "id": "5AHqM4aU-HoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.dlayer1, self.dlayer2, self.dlayer3, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dropout(rate=0.2), tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    h1 = self.dlayer1(h1)\n",
        "\n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    h2 = self.dlayer2(h2)\n",
        "\n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "    h3 = self.dlayer3(h3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "Saiuqvvo-U_g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0_29Lu2F-vnk",
        "outputId": "c170d150-e9e2-4ec4-b84f-2e59b72d928e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9200\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0037928497314453124 \n",
            "\n",
            "Validation Accuracy: 0.9271\n",
            "\n",
            "Train Accuracy: 0.9431\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0016540782165527343 \n",
            "\n",
            "Validation Accuracy: 0.9463\n",
            "\n",
            "Train Accuracy: 0.9596\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0012394468688964844 \n",
            "\n",
            "Validation Accuracy: 0.9589\n",
            "\n",
            "Train Accuracy: 0.9645\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0009917610931396484 \n",
            "\n",
            "Validation Accuracy: 0.9608\n",
            "\n",
            "Train Accuracy: 0.9709\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008350493621826172 \n",
            "\n",
            "Validation Accuracy: 0.9656\n",
            "\n",
            "Train Accuracy: 0.9745\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0007147328186035156 \n",
            "\n",
            "Validation Accuracy: 0.9675\n",
            "\n",
            "Train Accuracy: 0.9767\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0006069582366943359 \n",
            "\n",
            "Validation Accuracy: 0.9681\n",
            "\n",
            "Train Accuracy: 0.9806\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005323690795898437 \n",
            "\n",
            "Validation Accuracy: 0.9690\n",
            "\n",
            "Train Accuracy: 0.9830\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0004668082427978516 \n",
            "\n",
            "Validation Accuracy: 0.9702\n",
            "\n",
            "Train Accuracy: 0.9804\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0004084708023071289 \n",
            "\n",
            "Validation Accuracy: 0.9680\n",
            "\n",
            "Train Accuracy: 0.9875\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0003528226089477539 \n",
            "\n",
            "Validation Accuracy: 0.9729\n",
            "\n",
            "Train Accuracy: 0.9864\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00031400077819824216 \n",
            "\n",
            "Validation Accuracy: 0.9705\n",
            "\n",
            "Train Accuracy: 0.9894\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00027029069900512696 \n",
            "\n",
            "Validation Accuracy: 0.9734\n",
            "\n",
            "Train Accuracy: 0.9900\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00023583986282348633 \n",
            "\n",
            "Validation Accuracy: 0.9735\n",
            "\n",
            "Train Accuracy: 0.9919\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00020425443649291992 \n",
            "\n",
            "Validation Accuracy: 0.9742\n",
            "\n",
            "Train Accuracy: 0.9928\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00017665191650390624 \n",
            "\n",
            "Validation Accuracy: 0.9733\n",
            "\n",
            "Train Accuracy: 0.9931\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00015317197799682617 \n",
            "\n",
            "Validation Accuracy: 0.9734\n",
            "\n",
            "Train Accuracy: 0.9940\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00013207270622253418 \n",
            "\n",
            "Validation Accuracy: 0.9731\n",
            "\n",
            "Train Accuracy: 0.9944\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.00010998998641967773 \n",
            "\n",
            "Validation Accuracy: 0.9739\n",
            "\n",
            "Train Accuracy: 0.9960\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 9.782108306884766e-05 \n",
            "\n",
            "Validation Accuracy: 0.9739\n",
            "\n",
            "Total time taken (in seconds): 199.67\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUvzKYRn_IQO",
        "outputId": "17c3b4a3-f79d-4e42-bf60-ab767c9afab3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0251\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 3:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l1 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on cpu function and not mlp on other modes."
      ],
      "metadata": {
        "id": "KVdoHy_Q-7yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L1 = (tf.reduce_sum(tf.math.abs(self.W1))+ tf.reduce_sum(tf.math.abs(self.W2))+tf.reduce_sum(tf.math.abs(self.W3)) + tf.reduce_sum(tf.math.abs(self.W4)))\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L1\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "92eCFcBK_Ftg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dFupnyIB_SK2",
        "outputId": "122d1d79-0cd3-4d6f-afad-af62c3b71a58"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9102\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004244842834472656 \n",
            "\n",
            "Validation Accuracy: 0.9199\n",
            "\n",
            "Train Accuracy: 0.8939\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0023888348388671874 \n",
            "\n",
            "Validation Accuracy: 0.9051\n",
            "\n",
            "Train Accuracy: 0.8947\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.002361602020263672 \n",
            "\n",
            "Validation Accuracy: 0.9050\n",
            "\n",
            "Train Accuracy: 0.9027\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0023913768005371094 \n",
            "\n",
            "Validation Accuracy: 0.9145\n",
            "\n",
            "Train Accuracy: 0.9280\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0023613333129882813 \n",
            "\n",
            "Validation Accuracy: 0.9360\n",
            "\n",
            "Train Accuracy: 0.9062\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002252071990966797 \n",
            "\n",
            "Validation Accuracy: 0.9146\n",
            "\n",
            "Train Accuracy: 0.9146\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0021241474914550783 \n",
            "\n",
            "Validation Accuracy: 0.9209\n",
            "\n",
            "Train Accuracy: 0.9261\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0020736181640625 \n",
            "\n",
            "Validation Accuracy: 0.9356\n",
            "\n",
            "Train Accuracy: 0.9347\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0019056878662109375 \n",
            "\n",
            "Validation Accuracy: 0.9426\n",
            "\n",
            "Train Accuracy: 0.9241\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0019340242004394531 \n",
            "\n",
            "Validation Accuracy: 0.9326\n",
            "\n",
            "Train Accuracy: 0.9365\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0018085093688964844 \n",
            "\n",
            "Validation Accuracy: 0.9431\n",
            "\n",
            "Train Accuracy: 0.9353\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.001776511688232422 \n",
            "\n",
            "Validation Accuracy: 0.9423\n",
            "\n",
            "Train Accuracy: 0.9410\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0017970281982421874 \n",
            "\n",
            "Validation Accuracy: 0.9452\n",
            "\n",
            "Train Accuracy: 0.9417\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0017715892028808593 \n",
            "\n",
            "Validation Accuracy: 0.9474\n",
            "\n",
            "Train Accuracy: 0.9399\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00176700927734375 \n",
            "\n",
            "Validation Accuracy: 0.9426\n",
            "\n",
            "Train Accuracy: 0.8760\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0016519090270996095 \n",
            "\n",
            "Validation Accuracy: 0.8829\n",
            "\n",
            "Train Accuracy: 0.9468\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0016102540588378907 \n",
            "\n",
            "Validation Accuracy: 0.9491\n",
            "\n",
            "Train Accuracy: 0.9245\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.002123645324707031 \n",
            "\n",
            "Validation Accuracy: 0.9310\n",
            "\n",
            "Train Accuracy: 0.9441\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0015538145446777343 \n",
            "\n",
            "Validation Accuracy: 0.9481\n",
            "\n",
            "Train Accuracy: 0.9409\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016955419921875 \n",
            "\n",
            "Validation Accuracy: 0.9453\n",
            "\n",
            "Total time taken (in seconds): 209.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybvbLjbJ_SSe",
        "outputId": "241243d2-4bb9-4b25-e706-94312456fb96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0489\n",
            "\n",
            "Test Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 4:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l2 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on cpu function and not mlp on other modes."
      ],
      "metadata": {
        "id": "iPZIZXbs_b96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))/4\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "wfOxgGgP_cLA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oMbFi0rM_fSE",
        "outputId": "27f6ce9a-118b-4e03-b6f5-82fa89abff84"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9220\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0038350042724609374 \n",
            "\n",
            "Validation Accuracy: 0.9326\n",
            "\n",
            "Train Accuracy: 0.9362\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.001669705352783203 \n",
            "\n",
            "Validation Accuracy: 0.9415\n",
            "\n",
            "Train Accuracy: 0.9567\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.001267484130859375 \n",
            "\n",
            "Validation Accuracy: 0.9568\n",
            "\n",
            "Train Accuracy: 0.9614\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0010321977233886718 \n",
            "\n",
            "Validation Accuracy: 0.9597\n",
            "\n",
            "Train Accuracy: 0.9672\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008803965759277344 \n",
            "\n",
            "Validation Accuracy: 0.9625\n",
            "\n",
            "Train Accuracy: 0.9720\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.00076451171875 \n",
            "\n",
            "Validation Accuracy: 0.9666\n",
            "\n",
            "Train Accuracy: 0.9729\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0006625321197509766 \n",
            "\n",
            "Validation Accuracy: 0.9673\n",
            "\n",
            "Train Accuracy: 0.9784\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005917183303833008 \n",
            "\n",
            "Validation Accuracy: 0.9709\n",
            "\n",
            "Train Accuracy: 0.9796\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0005298286437988281 \n",
            "\n",
            "Validation Accuracy: 0.9715\n",
            "\n",
            "Train Accuracy: 0.9788\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0004741574478149414 \n",
            "\n",
            "Validation Accuracy: 0.9711\n",
            "\n",
            "Train Accuracy: 0.9837\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0004238924026489258 \n",
            "\n",
            "Validation Accuracy: 0.9723\n",
            "\n",
            "Train Accuracy: 0.9832\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00038321220397949217 \n",
            "\n",
            "Validation Accuracy: 0.9727\n",
            "\n",
            "Train Accuracy: 0.9852\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0003446505355834961 \n",
            "\n",
            "Validation Accuracy: 0.9735\n",
            "\n",
            "Train Accuracy: 0.9850\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00031586982727050783 \n",
            "\n",
            "Validation Accuracy: 0.9730\n",
            "\n",
            "Train Accuracy: 0.9891\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0002833498954772949 \n",
            "\n",
            "Validation Accuracy: 0.9758\n",
            "\n",
            "Train Accuracy: 0.9892\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0002612782859802246 \n",
            "\n",
            "Validation Accuracy: 0.9749\n",
            "\n",
            "Train Accuracy: 0.9896\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.000233004150390625 \n",
            "\n",
            "Validation Accuracy: 0.9766\n",
            "\n",
            "Train Accuracy: 0.9910\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00021861707687377929 \n",
            "\n",
            "Validation Accuracy: 0.9763\n",
            "\n",
            "Train Accuracy: 0.9902\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0001985163688659668 \n",
            "\n",
            "Validation Accuracy: 0.9752\n",
            "\n",
            "Train Accuracy: 0.9933\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.00018649871826171874 \n",
            "\n",
            "Validation Accuracy: 0.9767\n",
            "\n",
            "Total time taken (in seconds): 208.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUatt060_fYp",
        "outputId": "67ce95ce-366d-44df-eb38-078bb9292eda"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0216\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 5:**\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying a combination of l1 and l2 penalty/regularization[Elastic Net Regularization].Dropout regularization from Keras. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ],
      "metadata": {
        "id": "qK51vzkg_f_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L1 = (tf.reduce_sum(tf.math.abs(self.W1))+ tf.reduce_sum(tf.math.abs(self.W2))+tf.reduce_sum(tf.math.abs(self.W3)) + tf.reduce_sum(tf.math.abs(self.W4)))\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))/4\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L1 + 0.005 * L2\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "9ZUKvVNX_gwu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xWg85jXEAHa1",
        "outputId": "90f70856-821d-4c74-dd5d-120c9bcef396"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8917\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004333890075683594 \n",
            "\n",
            "Validation Accuracy: 0.9019\n",
            "\n",
            "Train Accuracy: 0.8892\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.002708355712890625 \n",
            "\n",
            "Validation Accuracy: 0.9025\n",
            "\n",
            "Train Accuracy: 0.8918\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0026907125854492187 \n",
            "\n",
            "Validation Accuracy: 0.9040\n",
            "\n",
            "Train Accuracy: 0.9015\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0026328494262695314 \n",
            "\n",
            "Validation Accuracy: 0.9159\n",
            "\n",
            "Train Accuracy: 0.9151\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002466947326660156 \n",
            "\n",
            "Validation Accuracy: 0.9262\n",
            "\n",
            "Train Accuracy: 0.9084\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0023447299194335938 \n",
            "\n",
            "Validation Accuracy: 0.9156\n",
            "\n",
            "Train Accuracy: 0.8965\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0022092112731933594 \n",
            "\n",
            "Validation Accuracy: 0.9068\n",
            "\n",
            "Train Accuracy: 0.9276\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0021363755798339845 \n",
            "\n",
            "Validation Accuracy: 0.9327\n",
            "\n",
            "Train Accuracy: 0.9332\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0020557064819335938 \n",
            "\n",
            "Validation Accuracy: 0.9414\n",
            "\n",
            "Train Accuracy: 0.9174\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0020116900634765626 \n",
            "\n",
            "Validation Accuracy: 0.9267\n",
            "\n",
            "Train Accuracy: 0.9279\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.002029413604736328 \n",
            "\n",
            "Validation Accuracy: 0.9364\n",
            "\n",
            "Train Accuracy: 0.9308\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0019455250549316406 \n",
            "\n",
            "Validation Accuracy: 0.9402\n",
            "\n",
            "Train Accuracy: 0.9390\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0019401991271972656 \n",
            "\n",
            "Validation Accuracy: 0.9456\n",
            "\n",
            "Train Accuracy: 0.9036\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0019429605102539063 \n",
            "\n",
            "Validation Accuracy: 0.9109\n",
            "\n",
            "Train Accuracy: 0.9280\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0018882878112792968 \n",
            "\n",
            "Validation Accuracy: 0.9360\n",
            "\n",
            "Train Accuracy: 0.9372\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018868132019042969 \n",
            "\n",
            "Validation Accuracy: 0.9427\n",
            "\n",
            "Train Accuracy: 0.9379\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.001856167449951172 \n",
            "\n",
            "Validation Accuracy: 0.9455\n",
            "\n",
            "Train Accuracy: 0.8367\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0018466766357421874 \n",
            "\n",
            "Validation Accuracy: 0.8459\n",
            "\n",
            "Train Accuracy: 0.9202\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0018604251098632812 \n",
            "\n",
            "Validation Accuracy: 0.9266\n",
            "\n",
            "Train Accuracy: 0.9341\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.002006520538330078 \n",
            "\n",
            "Validation Accuracy: 0.9404\n",
            "\n",
            "Total time taken (in seconds): 295.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds_LuBNdAIpe",
        "outputId": "ac59ef3a-c0c1-40cc-d8d3-e422b98e22b1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0567\n",
            "\n",
            "Test Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 6:**\n",
        "\n",
        "Hyper Parameter Optimization for Batch Size using Trial and Error"
      ],
      "metadata": {
        "id": "6T8qv6CK_ljB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "cmP2uC_o_ltz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(256) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PXLOTjQSAKe5",
        "outputId": "3474bad0-c55a-46bc-c5d8-ae91cf16a05a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8747\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.002527323760986328 \n",
            "\n",
            "Validation Accuracy: 0.8905\n",
            "\n",
            "Train Accuracy: 0.8987\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0011094089508056641 \n",
            "\n",
            "Validation Accuracy: 0.9082\n",
            "\n",
            "Train Accuracy: 0.9402\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0008683643341064454 \n",
            "\n",
            "Validation Accuracy: 0.9435\n",
            "\n",
            "Train Accuracy: 0.9444\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0007290935516357422 \n",
            "\n",
            "Validation Accuracy: 0.9491\n",
            "\n",
            "Train Accuracy: 0.9567\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0006308089065551758 \n",
            "\n",
            "Validation Accuracy: 0.9562\n",
            "\n",
            "Train Accuracy: 0.9581\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0005490301513671875 \n",
            "\n",
            "Validation Accuracy: 0.9579\n",
            "\n",
            "Train Accuracy: 0.9647\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0004911091232299805 \n",
            "\n",
            "Validation Accuracy: 0.9610\n",
            "\n",
            "Train Accuracy: 0.9698\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.00043653514862060545 \n",
            "\n",
            "Validation Accuracy: 0.9651\n",
            "\n",
            "Train Accuracy: 0.9724\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0003967219161987305 \n",
            "\n",
            "Validation Accuracy: 0.9667\n",
            "\n",
            "Train Accuracy: 0.9712\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0003635246276855469 \n",
            "\n",
            "Validation Accuracy: 0.9645\n",
            "\n",
            "Train Accuracy: 0.9736\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0003304633712768555 \n",
            "\n",
            "Validation Accuracy: 0.9655\n",
            "\n",
            "Train Accuracy: 0.9742\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0003033825492858887 \n",
            "\n",
            "Validation Accuracy: 0.9646\n",
            "\n",
            "Train Accuracy: 0.9804\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00027690311431884763 \n",
            "\n",
            "Validation Accuracy: 0.9691\n",
            "\n",
            "Train Accuracy: 0.9815\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00025269330978393555 \n",
            "\n",
            "Validation Accuracy: 0.9707\n",
            "\n",
            "Train Accuracy: 0.9828\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00023468114852905273 \n",
            "\n",
            "Validation Accuracy: 0.9716\n",
            "\n",
            "Train Accuracy: 0.9841\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00021598318099975587 \n",
            "\n",
            "Validation Accuracy: 0.9721\n",
            "\n",
            "Train Accuracy: 0.9859\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00019765539169311523 \n",
            "\n",
            "Validation Accuracy: 0.9726\n",
            "\n",
            "Train Accuracy: 0.9872\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00018295810699462891 \n",
            "\n",
            "Validation Accuracy: 0.9724\n",
            "\n",
            "Train Accuracy: 0.9884\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0001678072166442871 \n",
            "\n",
            "Validation Accuracy: 0.9729\n",
            "\n",
            "Train Accuracy: 0.9895\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.00015879798889160157 \n",
            "\n",
            "Validation Accuracy: 0.9735\n",
            "\n",
            "Total time taken (in seconds): 148.71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlHZahNMAaa6",
        "outputId": "6deda49a-10cf-415b-a929-7d5124ddfa94"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0230\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(64) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zF5X60HcJJpn",
        "outputId": "a20c2905-d677-489c-921f-ce2b3e740d1a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9429\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005788213500976563 \n",
            "\n",
            "Validation Accuracy: 0.9468\n",
            "\n",
            "Train Accuracy: 0.9630\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.00248618408203125 \n",
            "\n",
            "Validation Accuracy: 0.9602\n",
            "\n",
            "Train Accuracy: 0.9695\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0017873818969726563 \n",
            "\n",
            "Validation Accuracy: 0.9636\n",
            "\n",
            "Train Accuracy: 0.9748\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0013688674926757812 \n",
            "\n",
            "Validation Accuracy: 0.9661\n",
            "\n",
            "Train Accuracy: 0.9802\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0011215206146240235 \n",
            "\n",
            "Validation Accuracy: 0.9693\n",
            "\n",
            "Train Accuracy: 0.9729\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0008883085632324218 \n",
            "\n",
            "Validation Accuracy: 0.9626\n",
            "\n",
            "Train Accuracy: 0.9825\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0007283372497558593 \n",
            "\n",
            "Validation Accuracy: 0.9676\n",
            "\n",
            "Train Accuracy: 0.9879\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005836821365356445 \n",
            "\n",
            "Validation Accuracy: 0.9719\n",
            "\n",
            "Train Accuracy: 0.9863\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0004581095886230469 \n",
            "\n",
            "Validation Accuracy: 0.9712\n",
            "\n",
            "Train Accuracy: 0.9914\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.00037482925415039065 \n",
            "\n",
            "Validation Accuracy: 0.9734\n",
            "\n",
            "Train Accuracy: 0.9911\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0003418566513061523 \n",
            "\n",
            "Validation Accuracy: 0.9719\n",
            "\n",
            "Train Accuracy: 0.9931\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00023318862915039062 \n",
            "\n",
            "Validation Accuracy: 0.9733\n",
            "\n",
            "Train Accuracy: 0.9955\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0001728789710998535 \n",
            "\n",
            "Validation Accuracy: 0.9754\n",
            "\n",
            "Train Accuracy: 0.9960\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00013009991645812987 \n",
            "\n",
            "Validation Accuracy: 0.9755\n",
            "\n",
            "Train Accuracy: 0.9962\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00010283166885375976 \n",
            "\n",
            "Validation Accuracy: 0.9757\n",
            "\n",
            "Train Accuracy: 0.9972\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 8.356544494628906e-05 \n",
            "\n",
            "Validation Accuracy: 0.9750\n",
            "\n",
            "Train Accuracy: 0.9979\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 7.583492279052735e-05 \n",
            "\n",
            "Validation Accuracy: 0.9761\n",
            "\n",
            "Train Accuracy: 0.9986\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 4.3998227119445803e-05 \n",
            "\n",
            "Validation Accuracy: 0.9757\n",
            "\n",
            "Train Accuracy: 0.9993\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 3.179727554321289e-05 \n",
            "\n",
            "Validation Accuracy: 0.9758\n",
            "\n",
            "Train Accuracy: 0.9995\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 2.5634765625e-05 \n",
            "\n",
            "Validation Accuracy: 0.9763\n",
            "\n",
            "Total time taken (in seconds): 366.98\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFsvBMiGJTHX",
        "outputId": "1540612d-ff77-4ad2-a097-e1c6cc828bf0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0281\n",
            "\n",
            "Test Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 7:**\n",
        "\n",
        "Hyper Parameter Optimization for Learning Rate using Trial and Error"
      ],
      "metadata": {
        "id": "1i1iFf2x_nZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.0001)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "yrlhg0qF_n70"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5S81-VFwAMTx",
        "outputId": "27735cc5-f565-4c60-96b9-aa29d5567835"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.0847\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.018395963134765624 \n",
            "\n",
            "Validation Accuracy: 0.0850\n",
            "\n",
            "Train Accuracy: 0.1125\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.01797009765625 \n",
            "\n",
            "Validation Accuracy: 0.1138\n",
            "\n",
            "Train Accuracy: 0.1448\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0176326904296875 \n",
            "\n",
            "Validation Accuracy: 0.1459\n",
            "\n",
            "Train Accuracy: 0.1804\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.01734888916015625 \n",
            "\n",
            "Validation Accuracy: 0.1782\n",
            "\n",
            "Train Accuracy: 0.2187\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.017098427734375 \n",
            "\n",
            "Validation Accuracy: 0.2193\n",
            "\n",
            "Train Accuracy: 0.2575\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.016869842529296877 \n",
            "\n",
            "Validation Accuracy: 0.2651\n",
            "\n",
            "Train Accuracy: 0.2934\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.01665509521484375 \n",
            "\n",
            "Validation Accuracy: 0.3019\n",
            "\n",
            "Train Accuracy: 0.3268\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.016449498291015625 \n",
            "\n",
            "Validation Accuracy: 0.3364\n",
            "\n",
            "Train Accuracy: 0.3568\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.016249981689453127 \n",
            "\n",
            "Validation Accuracy: 0.3656\n",
            "\n",
            "Train Accuracy: 0.3833\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.016053663330078124 \n",
            "\n",
            "Validation Accuracy: 0.3899\n",
            "\n",
            "Train Accuracy: 0.4074\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.01585956787109375 \n",
            "\n",
            "Validation Accuracy: 0.4141\n",
            "\n",
            "Train Accuracy: 0.4290\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.015666185302734375 \n",
            "\n",
            "Validation Accuracy: 0.4372\n",
            "\n",
            "Train Accuracy: 0.4480\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.015472872314453125 \n",
            "\n",
            "Validation Accuracy: 0.4581\n",
            "\n",
            "Train Accuracy: 0.4649\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.01527944091796875 \n",
            "\n",
            "Validation Accuracy: 0.4751\n",
            "\n",
            "Train Accuracy: 0.4795\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.015084793701171875 \n",
            "\n",
            "Validation Accuracy: 0.4938\n",
            "\n",
            "Train Accuracy: 0.4934\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.014889034423828124 \n",
            "\n",
            "Validation Accuracy: 0.5082\n",
            "\n",
            "Train Accuracy: 0.5060\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.01469194580078125 \n",
            "\n",
            "Validation Accuracy: 0.5225\n",
            "\n",
            "Train Accuracy: 0.5185\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.014493345947265625 \n",
            "\n",
            "Validation Accuracy: 0.5350\n",
            "\n",
            "Train Accuracy: 0.5294\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.01429309326171875 \n",
            "\n",
            "Validation Accuracy: 0.5473\n",
            "\n",
            "Train Accuracy: 0.5399\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0140913037109375 \n",
            "\n",
            "Validation Accuracy: 0.5576\n",
            "\n",
            "Total time taken (in seconds): 193.90\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lhu_zALANLp",
        "outputId": "8f2c72cc-3a0c-496d-9166-9dac68259b59"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.4430\n",
            "\n",
            "Test Accuracy: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "j7AsNBBJJk_T"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jp-kPIXZJvLx",
        "outputId": "5e52626e-687e-4384-f4b2-bf174a14d0cc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.3558\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.017294365234375 \n",
            "\n",
            "Validation Accuracy: 0.3572\n",
            "\n",
            "Train Accuracy: 0.4936\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.015393995361328124 \n",
            "\n",
            "Validation Accuracy: 0.5024\n",
            "\n",
            "Train Accuracy: 0.5843\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.01348860107421875 \n",
            "\n",
            "Validation Accuracy: 0.5983\n",
            "\n",
            "Train Accuracy: 0.6514\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.011526802978515625 \n",
            "\n",
            "Validation Accuracy: 0.6687\n",
            "\n",
            "Train Accuracy: 0.6990\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0097697412109375 \n",
            "\n",
            "Validation Accuracy: 0.7229\n",
            "\n",
            "Train Accuracy: 0.7337\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.008374005126953125 \n",
            "\n",
            "Validation Accuracy: 0.7646\n",
            "\n",
            "Train Accuracy: 0.7591\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.007326719360351563 \n",
            "\n",
            "Validation Accuracy: 0.7913\n",
            "\n",
            "Train Accuracy: 0.7803\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.00653896240234375 \n",
            "\n",
            "Validation Accuracy: 0.8078\n",
            "\n",
            "Train Accuracy: 0.7966\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.005935200805664062 \n",
            "\n",
            "Validation Accuracy: 0.8214\n",
            "\n",
            "Train Accuracy: 0.8104\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0054607373046875 \n",
            "\n",
            "Validation Accuracy: 0.8342\n",
            "\n",
            "Train Accuracy: 0.8217\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.005079801940917969 \n",
            "\n",
            "Validation Accuracy: 0.8433\n",
            "\n",
            "Train Accuracy: 0.8314\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.004770062255859375 \n",
            "\n",
            "Validation Accuracy: 0.8515\n",
            "\n",
            "Train Accuracy: 0.8391\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00451301513671875 \n",
            "\n",
            "Validation Accuracy: 0.8582\n",
            "\n",
            "Train Accuracy: 0.8456\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.004299246520996094 \n",
            "\n",
            "Validation Accuracy: 0.8634\n",
            "\n",
            "Train Accuracy: 0.8513\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.004117931823730468 \n",
            "\n",
            "Validation Accuracy: 0.8678\n",
            "\n",
            "Train Accuracy: 0.8568\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.003962533264160156 \n",
            "\n",
            "Validation Accuracy: 0.8713\n",
            "\n",
            "Train Accuracy: 0.8616\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0038277944946289062 \n",
            "\n",
            "Validation Accuracy: 0.8750\n",
            "\n",
            "Train Accuracy: 0.8656\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0037094656372070314 \n",
            "\n",
            "Validation Accuracy: 0.8783\n",
            "\n",
            "Train Accuracy: 0.8692\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.003604552917480469 \n",
            "\n",
            "Validation Accuracy: 0.8808\n",
            "\n",
            "Train Accuracy: 0.8722\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0035106243896484376 \n",
            "\n",
            "Validation Accuracy: 0.8850\n",
            "\n",
            "Total time taken (in seconds): 200.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHj7_U7OJw-M",
        "outputId": "48c537e6-a9b9-450a-84a0-e2fa09ee41d1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1032\n",
            "\n",
            "Test Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 8:**\n",
        "\n",
        "Hyper Parameter Optimisation for Activation Function using Trial and Error"
      ],
      "metadata": {
        "id": "dHTYeqb7_otn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "gZasg1YS_qYC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o9rwxXujAOos",
        "outputId": "f41576aa-8ff2-47c1-9b89-58016c6bcec8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8717\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.008604240112304687 \n",
            "\n",
            "Validation Accuracy: 0.8861\n",
            "\n",
            "Train Accuracy: 0.9062\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0029692794799804686 \n",
            "\n",
            "Validation Accuracy: 0.9179\n",
            "\n",
            "Train Accuracy: 0.9212\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0023196597290039062 \n",
            "\n",
            "Validation Accuracy: 0.9306\n",
            "\n",
            "Train Accuracy: 0.9310\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0020050076293945313 \n",
            "\n",
            "Validation Accuracy: 0.9364\n",
            "\n",
            "Train Accuracy: 0.9370\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0017959553527832032 \n",
            "\n",
            "Validation Accuracy: 0.9421\n",
            "\n",
            "Train Accuracy: 0.9426\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0016393472290039063 \n",
            "\n",
            "Validation Accuracy: 0.9463\n",
            "\n",
            "Train Accuracy: 0.9467\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0015134660339355469 \n",
            "\n",
            "Validation Accuracy: 0.9504\n",
            "\n",
            "Train Accuracy: 0.9493\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0014115592956542968 \n",
            "\n",
            "Validation Accuracy: 0.9542\n",
            "\n",
            "Train Accuracy: 0.9528\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.001326039276123047 \n",
            "\n",
            "Validation Accuracy: 0.9550\n",
            "\n",
            "Train Accuracy: 0.9555\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0012529833221435547 \n",
            "\n",
            "Validation Accuracy: 0.9570\n",
            "\n",
            "Train Accuracy: 0.9580\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.001189482879638672 \n",
            "\n",
            "Validation Accuracy: 0.9575\n",
            "\n",
            "Train Accuracy: 0.9600\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0011380887603759766 \n",
            "\n",
            "Validation Accuracy: 0.9574\n",
            "\n",
            "Train Accuracy: 0.9617\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0010884073638916015 \n",
            "\n",
            "Validation Accuracy: 0.9599\n",
            "\n",
            "Train Accuracy: 0.9635\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0010502950286865233 \n",
            "\n",
            "Validation Accuracy: 0.9606\n",
            "\n",
            "Train Accuracy: 0.9648\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0010109201049804688 \n",
            "\n",
            "Validation Accuracy: 0.9618\n",
            "\n",
            "Train Accuracy: 0.9661\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0009802879333496094 \n",
            "\n",
            "Validation Accuracy: 0.9610\n",
            "\n",
            "Train Accuracy: 0.9674\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0009470075988769532 \n",
            "\n",
            "Validation Accuracy: 0.9617\n",
            "\n",
            "Train Accuracy: 0.9685\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0009199668121337891 \n",
            "\n",
            "Validation Accuracy: 0.9625\n",
            "\n",
            "Train Accuracy: 0.9692\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0008966194152832031 \n",
            "\n",
            "Validation Accuracy: 0.9631\n",
            "\n",
            "Train Accuracy: 0.9702\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.000876181640625 \n",
            "\n",
            "Validation Accuracy: 0.9636\n",
            "\n",
            "Total time taken (in seconds): 338.94\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da48OVReAXoj",
        "outputId": "7053fce6-8819-4571-dcc3-b1576dab1812"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0342\n",
            "\n",
            "Test Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adagrad(learning_rate = 1e-4)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "oDGfqdeRKBMO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eyTtNL1yKBTA",
        "outputId": "baa9de36-8348-40c8-de5a-973a1aa22545"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.1631\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.018111458740234374 \n",
            "\n",
            "Validation Accuracy: 0.1659\n",
            "\n",
            "Train Accuracy: 0.2163\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.017578458251953127 \n",
            "\n",
            "Validation Accuracy: 0.2198\n",
            "\n",
            "Train Accuracy: 0.2725\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.017096158447265624 \n",
            "\n",
            "Validation Accuracy: 0.2784\n",
            "\n",
            "Train Accuracy: 0.3312\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.016625494384765627 \n",
            "\n",
            "Validation Accuracy: 0.3398\n",
            "\n",
            "Train Accuracy: 0.3979\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.01614898193359375 \n",
            "\n",
            "Validation Accuracy: 0.4081\n",
            "\n",
            "Train Accuracy: 0.4662\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.01565573486328125 \n",
            "\n",
            "Validation Accuracy: 0.4818\n",
            "\n",
            "Train Accuracy: 0.5199\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.01514045654296875 \n",
            "\n",
            "Validation Accuracy: 0.5369\n",
            "\n",
            "Train Accuracy: 0.5573\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.014600836181640625 \n",
            "\n",
            "Validation Accuracy: 0.5788\n",
            "\n",
            "Train Accuracy: 0.5852\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.01403655029296875 \n",
            "\n",
            "Validation Accuracy: 0.6075\n",
            "\n",
            "Train Accuracy: 0.6085\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0134490869140625 \n",
            "\n",
            "Validation Accuracy: 0.6337\n",
            "\n",
            "Train Accuracy: 0.6290\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.012842691650390625 \n",
            "\n",
            "Validation Accuracy: 0.6542\n",
            "\n",
            "Train Accuracy: 0.6457\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.012222967529296875 \n",
            "\n",
            "Validation Accuracy: 0.6722\n",
            "\n",
            "Train Accuracy: 0.6638\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.011599522705078125 \n",
            "\n",
            "Validation Accuracy: 0.6891\n",
            "\n",
            "Train Accuracy: 0.6794\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.010983280029296875 \n",
            "\n",
            "Validation Accuracy: 0.7045\n",
            "\n",
            "Train Accuracy: 0.6942\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.01038561767578125 \n",
            "\n",
            "Validation Accuracy: 0.7186\n",
            "\n",
            "Train Accuracy: 0.7081\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.009816675415039063 \n",
            "\n",
            "Validation Accuracy: 0.7343\n",
            "\n",
            "Train Accuracy: 0.7215\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.009285552368164062 \n",
            "\n",
            "Validation Accuracy: 0.7484\n",
            "\n",
            "Train Accuracy: 0.7340\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0087955859375 \n",
            "\n",
            "Validation Accuracy: 0.7606\n",
            "\n",
            "Train Accuracy: 0.7439\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.008348802490234375 \n",
            "\n",
            "Validation Accuracy: 0.7694\n",
            "\n",
            "Train Accuracy: 0.7534\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.007944606323242188 \n",
            "\n",
            "Validation Accuracy: 0.7778\n",
            "\n",
            "Total time taken (in seconds): 221.29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVYklEQVR4nO3dcbBc5Xnf8e8PCTHBCTIWGteWEJcE0o4SWpvcoU6aZtxRQoWntmhKWlG1wTGtJm2Y1HWZlowGj03CNLROwC7UHdUwwYwmYFO7uU5xsQu0GaY15UJtZDCEawWBFGLLwIgCQ0HO0z/2CF8tu3v36t7du9r9fmbu6Ox73j33PUdH59F79jzPpqqQJE2ek1Z6AJKklWEAkKQJZQCQpAllAJCkCWUAkKQJtXqlB7AYZ5xxRk1NTa30MCTphPLQQw99r6rWt7efUAFgamqK2dnZlR6GJJ1Qkuzv1O4tIEmaUAYASZpQBgBJmlAGAEmaUAYASZpQYx8A9uzdw9QNU5z08ZOYumGKPXv3rPSQJGkknFCPgS7Wnr172Pmlnbzy+isA7D+8n51f2gnAjvN2rOTQJGnF9TUDSLI1yRNJ5pJc1WH9KUnuaNY/kGSqaV+X5L4kLyW5se09lybZm+SRJP81yRnLsUPz7bpn1xsX/6Neef0Vdt2za7l/lSSdcBYMAElWATcBFwGbgUuTbG7rdjnwQlWdA1wPXNe0vwpcDVzZts3VwCeBv1FVfxl4BLhiCfvR0dOHn15UuyRNkn5mABcAc1W1r6peA24HtrX12Qbc2izfCWxJkqp6uarupxUI5kvz85YkAU4D/vR4d6KbTWs3LapdkiZJPwFgA/DMvNcHmraOfarqCHAYWNdtg1X1OvBPgL20LvybgZs79U2yM8lsktlDhw71MdwfuHbLtZx68qnHtJ168qlcu+XaRW1HksbRijwFlORkWgHg3cA7ad0C+o1Ofatqd1VNV9X0+vVvqmXU047zdrD7/bs5a+1ZhHDW2rPY/f7dfgAsSfT3FNBB4Mx5rzc2bZ36HGju768FnuuxzXcBVNW3AZJ8DnjTh8vLYcd5O7zgS1IH/cwAHgTOTXJ2kjXAdmCmrc8McFmzfAlwb/X+tvmDwOYkR/9L/wvAt/oftiRpqRacAVTVkSRXAHcDq4BbqurRJNcAs1U1Q+v+/W1J5oDnaQUJAJI8RetD3jVJLgYurKrHknwc+KMkrwP7gQ8u765JknpJ7/+oj5bp6eny+wAkaXGSPFRV0+3tY18KYqksJSFpXI11KYilspSEpHHmDKAHS0lIGmcGgB4sJSFpnBkAerCUhKRxZgDowVISksaZAaAHS0lIGmfmAUjSmDMPQJJ0DAPAgJlIJmlUmQg2QCaSSRplzgAGyEQySaPMADBAJpJJGmUGgAEykUzSKDMADJCJZJJGmQFggEwkkzTKTASTpDFnIpgk6RgGgBFnIpmkQTERbISZSCZpkJwBjDATySQNkgFghJlIJmmQDAAjzEQySYNkABhhJpJJGiQDwAgzkUzSIJkIJkljzkQwSdIxDABjzkQySd2YCDbGTCST1IszgDFmIpmkXvoKAEm2JnkiyVySqzqsPyXJHc36B5JMNe3rktyX5KUkN7a9Z02S3Un+OMnjSf7OcuyQfsBEMkm9LBgAkqwCbgIuAjYDlybZ3NbtcuCFqjoHuB64rml/FbgauLLDpncB362qH2+2+z+Oaw/UlYlkknrpZwZwATBXVfuq6jXgdmBbW59twK3N8p3AliSpqper6n5agaDdh4B/DVBVf15V3zuuPVBXJpJJ6qWfALABeGbe6wNNW8c+VXUEOAys67bBJG9tFn8zycNJPp/k7V367kwym2T20KFDfQxXR5lIJqmXlXoKaDWwEfifVfWRJB8BPgH8w/aOVbUb2A2tRLChjnIM7Dhvhxd8SR31MwM4CJw57/XGpq1jnySrgbXAcz22+RzwCvCF5vXngfP7GIskaZn0EwAeBM5NcnaSNcB2YKatzwxwWbN8CXBv9agx0az7EvDepmkL8Ngixi1JWqIFA0BzT/8K4G7gW8DnqurRJNck+UDT7WZgXZI54CPAG4+KJnkK+F3gg0kOzHuC6F8BH0vyCK1bP/9imfZJy8hMYml8WQxOXbVnEkPrKSI/SJZOLBaD06KZSSyNNwOAujKTWBpvBgB1ZSaxNN4MAOrKTGJpvBkA1JWZxNJ48ykgSRpzPgUkSTqGAUCSJpQBQANlJrE0uvxOYA2M30ksjTZnABoYM4ml0WYA0MCYSSyNNgOABsZMYmm0GQA0MGYSS6PNAKCBMZNYGm1mAkvSmDMTWJJ0DAOAJE0oA4AkTSgDgEaapSSkwbEUhEaWpSSkwXIGoJFlKQlpsAwAGlmWkpAGywCgkWUpCWmwDAAaWZaSkAbLAKCRZSkJabAsBSFJY85SEJKkYxgAJGlCGQA01swklrrrKwAk2ZrkiSRzSa7qsP6UJHc06x9IMtW0r0tyX5KXktzYZdszSb65lJ2QOjmaSbz/8H6KeiOT2CAgtSwYAJKsAm4CLgI2A5cm2dzW7XLghao6B7geuK5pfxW4Griyy7Z/EXjp+IYu9WYmsdRbPzOAC4C5qtpXVa8BtwPb2vpsA25tlu8EtiRJVb1cVffTCgTHSPLDwEeA3zru0Us9mEks9dZPANgAPDPv9YGmrWOfqjoCHAbWLbDd3wR+B3ilV6ckO5PMJpk9dOhQH8OVWswklnpbkQ+Bk7wL+LGq+uJCfatqd1VNV9X0+vXrhzA6jQsziaXe+gkAB4Ez573e2LR17JNkNbAWeK7HNn8amE7yFHA/8ONJ/nt/Q5b6Yyax1Fs/3wfwIHBukrNpXei3A3+/rc8McBnwv4BLgHurR4pxVX0a+DRA88TQH1bVexc5dmlBO87b4QVf6mLBAFBVR5JcAdwNrAJuqapHk1wDzFbVDHAzcFuSOeB5WkECgOZ/+acBa5JcDFxYVY8t/65IkhbDWkCSNOasBSRJOoYBQJImlAFA6sFaQhpn/TwFJE2ko7WEjpaTOFpLCPDJIo0FZwBSF9YS0rgzAEhdWEtI484AIHVhLSGNOwOA1IW1hDTuDABSF9YS0rgzE1iSxpyZwJKkYxgAJGlCGQAkaUIZACRpQhkApAGylpBGmbWApAGxlpBGnTMAaUCsJaRRZwCQBsRaQhp1BgBpQKwlpFFnAJAGxFpCGnUGAGlArCWkUWctIEkac9YCkiQdwwAgSRPKACBJE8oAIEkTygAgjTBrCWmQrAUkjShrCWnQnAFII8paQho0A4A0oqwlpEHrKwAk2ZrkiSRzSa7qsP6UJHc06x9IMtW0r0tyX5KXktw4r/+pSf5LkseTPJrkt5drh6RxYS0hDdqCASDJKuAm4CJgM3Bpks1t3S4HXqiqc4Drgeua9leBq4ErO2z6E1X1l4B3A38tyUXHtwvSeLKWkAatnxnABcBcVe2rqteA24FtbX22Abc2y3cCW5Kkql6uqvtpBYI3VNUrVXVfs/wa8DCwcQn7IY0dawlp0Pp5CmgD8My81weAv9qtT1UdSXIYWAd8b6GNJ3kr8H7gk13W7wR2Amza5NRXk2XHeTu84GtgVvRD4CSrgd8HPlVV+zr1qardVTVdVdPr168f7gAlaYz1EwAOAmfOe72xaevYp7morwWe62Pbu4Enq+qGPvpKkpZRPwHgQeDcJGcnWQNsB2ba+swAlzXLlwD31gJ1ppP8Fq1A8eHFDVmStBwW/Ayguad/BXA3sAq4paoeTXINMFtVM8DNwG1J5oDnaQUJAJI8BZwGrElyMXAh8CKwC3gceDgJwI1V9Znl3DlJUnd9lYKoqruAu9raPjpv+VXgl7q8d6rLZtPfECUdrz1797Drnl08ffhpNq3dxLVbrvVDZb3BWkDSmLKWkBZiKQhpTFlLSAsxAEhjylpCWogBQBpT1hLSQgwA0piylpAWYgCQxpS1hLSQLJCvNVKmp6drdnZ2pYchSSeUJA9V1XR7uzMASZpQBgBJmlAGAEmaUAYASZpQBgBJXe3Zu4epG6Y46eMnMXXDFHv27lnpIWkZWQtIUkfWEhp/zgAkdWQtofFnAJDUkbWExp8BQFJH1hIafwYASR1ZS2j8GQAkdWQtofFnLSBJGnPWApIkHcMAIEkTygAgSRPKACBpYCwlMdosBSFpICwlMfqcAUgaCEtJjD4DgKSBsJTE6DMASBoIS0mMPgOApIGwlMToMwBIGghLSYw+S0FI0phbUimIJFuTPJFkLslVHdafkuSOZv0DSaaa9nVJ7kvyUpIb297zU0n2Nu/5VJIc365Jko7HggEgySrgJuAiYDNwaZLNbd0uB16oqnOA64HrmvZXgauBKzts+tPAPwbObX62Hs8OSJKOTz8zgAuAuaraV1WvAbcD29r6bANubZbvBLYkSVW9XFX30woEb0jyDuC0qvpate5BfRa4eCk7IklanH4CwAbgmXmvDzRtHftU1RHgMLBugW0eWGCbACTZmWQ2yeyhQ4f6GK4kqR8j/xRQVe2uqumqml6/fv1KD0fSEFlLaLD6qQV0EDhz3uuNTVunPgeSrAbWAs8tsM2NC2xT0gSzltDg9TMDeBA4N8nZSdYA24GZtj4zwGXN8iXAvdXj+dKqehZ4Mcl7mqd/fhn4g0WPXtLYspbQ4C04A6iqI0muAO4GVgG3VNWjSa4BZqtqBrgZuC3JHPA8rSABQJKngNOANUkuBi6sqseAfwr8HvBDwJebH0kCrCU0DH2Vg66qu4C72to+Om/5VeCXurx3qkv7LPCT/Q5U0mTZtHYT+w/v79iu5THyHwJLmkzWEho8A4CkkWQtocGzFpAkjbkl1QKSJI0fA4AkTSgDgKSxZSZxb309BipJJxoziRfmDEDSWDKTeGEGAEljyUzihRkAJI2lbhnDZhL/gAFA0lgyk3hhBgBJY8lM4oWZCSxJY85MYEnSMQwAktTFuCeSmQgmSR1MQiKZMwBJ6mASEskMAJLUwSQkkhkAJKmDSUgkMwBIUgeTkEhmAJCkDiYhkcxEMEkacyaCSZKOYQCQpAEZ9UQyE8EkaQBOhEQyZwCSNAAnQiKZAUCSBuBESCQzAEjSAJwIiWQGAEkagBMhkcwAIEkDcCIkkvWVCJZkK/BJYBXwmar67bb1pwCfBX4KeA74e1X1VLPuN4DLge8Dv15Vdzft/xz4R0ABe4FfqapXe43DRDBJWrzjTgRLsgq4CbgI2AxcmmRzW7fLgReq6hzgeuC65r2bge3ATwBbgX+fZFWSDcCvA9NV9ZO0Asv24905SRpHg84j6OcW0AXAXFXtq6rXgNuBbW19tgG3Nst3AluSpGm/var+X1X9CTDXbA9aOQg/lGQ1cCrwp0vbFUkaH0fzCPYf3k9Rb+QRLGcQ6CcAbACemff6QNPWsU9VHQEOA+u6vbeqDgKfAJ4GngUOV9VXOv3yJDuTzCaZPXToUB/DlaQT3zDyCFbkQ+Akp9OaHZwNvBN4S5J/0KlvVe2uqumqml6/fv0whylJK2YYeQT9BICDwJnzXm9s2jr2aW7prKX1YXC39/488CdVdaiqXge+APzM8eyAJI2jYeQR9BMAHgTOTXJ2kjW0PqydaeszA1zWLF8C3Futx4tmgO1JTklyNnAu8L9p3fp5T5JTm88KtgDfWvruSNJ4GEYewYLF4KrqSJIrgLtpPa1zS1U9muQaYLaqZoCbgduSzAHP0zzR0/T7HPAYcAT4tar6PvBAkjuBh5v2/wPsXra9kqQT3NF8gV337OLpw0+zae0mrt1y7bLmEfiFMJI05vxCGEnSMQwAkjShDACSNKEMAJI0oQwAkjShTqingJIcAvav9Di6OAP43koPogfHtzSOb2kc39IsdXxnVdWbSimcUAFglCWZ7fSY1ahwfEvj+JbG8S3NoMbnLSBJmlAGAEmaUAaA5TPqpSwc39I4vqVxfEszkPH5GYAkTShnAJI0oQwAkjShDACLkOTMJPcleSzJo0n+WYc+701yOMnXm5+PDnmMTyXZ2/zuN5VOTcunkswleSTJ+UMc21+cd1y+nuTFJB9u6zPU45fkliTfTfLNeW1vS/LVJE82f57e5b2XNX2eTHJZpz4DGt+/TfJ48/f3xSRv7fLenufCAMf3sSQH5/0dvq/Le7cmeaI5F68a4vjumDe2p5J8vct7h3H8Ol5ThnYOVpU/ff4A7wDOb5Z/BPhjYHNbn/cCf7iCY3wKOKPH+vcBXwYCvAd4YIXGuQr4M1oJKit2/ICfA84Hvjmv7d8AVzXLVwHXdXjf24B9zZ+nN8unD2l8FwKrm+XrOo2vn3NhgOP7GHBlH3//3wZ+FFgDfKP939Kgxte2/neAj67g8et4TRnWOegMYBGq6tmqerhZ/r+0vsVsw8qOatG2AZ+tlq8Bb03yjhUYxxbg21W1opndVfVHtL7EaL5twK3N8q3AxR3e+jeBr1bV81X1AvBVYOswxldVX6mqI83Lr9H6qtUV0eX49eMCYK6q9lXVa8DttI77suo1vubbCP8u8PvL/Xv71eOaMpRz0ABwnJJMAe8GHuiw+qeTfCPJl5P8xFAHBgV8JclDSXZ2WL8BeGbe6wOsTBDbTvd/eCt5/ADeXlXPNst/Bry9Q59ROY4fojWj62Shc2GQrmhuUd3S5fbFKBy/vw58p6qe7LJ+qMev7ZoylHPQAHAckvww8J+AD1fVi22rH6Z1W+OvAP8O+M9DHt7PVtX5wEXAryX5uSH//gWl9d3SHwA+32H1Sh+/Y1Rrrj2Sz0on2UXrK1X3dOmyUufCp4EfA94FPEvrNssoupTe//sf2vHrdU0Z5DloAFikJCfT+ovaU1VfaF9fVS9W1UvN8l3AyUnOGNb4qupg8+d3gS/SmmrPdxA4c97rjU3bMF0EPFxV32lfsdLHr/Gdo7fFmj+/26HPih7HJB8E/hawo7lAvEkf58JAVNV3qur7VfXnwH/s8ntX+vitBn4RuKNbn2Edvy7XlKGcgwaARWjuGd4MfKuqfrdLn7/Q9CPJBbSO8XNDGt9bkvzI0WVaHxZ+s63bDPDLzdNA7wEOz5tqDkvX/3mt5PGbZwY4+kTFZcAfdOhzN3BhktObWxwXNm0Dl2Qr8C+BD1TVK1369HMuDGp88z9T+ttdfu+DwLlJzm5mhNtpHfdh+Xng8ao60GnlsI5fj2vKcM7BQX7CPW4/wM/Smoo9Any9+Xkf8KvArzZ9rgAepfVUw9eAnxni+H60+b3faMawq2mfP74AN9F6AmMvMD3kY/gWWhf0tfPaVuz40QpEzwKv07qHejmwDrgHeBL4b8Dbmr7TwGfmvfdDwFzz8ytDHN8crXu/R8/B/9D0fSdwV69zYUjju605tx6hdSF7R/v4mtfvo/XUy7eHOb6m/feOnnPz+q7E8et2TRnKOWgpCEmaUN4CkqQJZQCQpAllAJCkCWUAkKQJZQCQpAllAJCkCWUAkKQJ9f8BaZRFbnRtK+cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2SlT2mZKBZk",
        "outputId": "8ad86287-4bd6-4bea-c838-4ff68da4763b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.2370\n",
            "\n",
            "Test Accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 9:**\n",
        "\n",
        "Best Model HyperParameters: Model = cpu, LR = 0.1, Optimizer = SGD, Batch Size = 128 or 256(Same Acc), Regularization = Dropout,L2 or No Reg(Same Accuracy)"
      ],
      "metadata": {
        "id": "YxpyqfTw_rYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "\n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))/4\n",
        "      current_loss = self.loss(predicted, y_train) + 0.001 * L2\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)"
      ],
      "metadata": {
        "id": "agOwXhsu_rn1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128) #seed same as emailID number\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4-fT22TwARTA",
        "outputId": "55fa1d2d-c1a8-4181-efb1-a9a8072c325c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9130\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00398046630859375 \n",
            "\n",
            "Validation Accuracy: 0.9221\n",
            "\n",
            "Train Accuracy: 0.9298\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0017332978820800782 \n",
            "\n",
            "Validation Accuracy: 0.9344\n",
            "\n",
            "Train Accuracy: 0.9547\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0013069447326660155 \n",
            "\n",
            "Validation Accuracy: 0.9538\n",
            "\n",
            "Train Accuracy: 0.9577\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0010562254333496093 \n",
            "\n",
            "Validation Accuracy: 0.9561\n",
            "\n",
            "Train Accuracy: 0.9665\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008960999298095703 \n",
            "\n",
            "Validation Accuracy: 0.9612\n",
            "\n",
            "Train Accuracy: 0.9689\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0007759777069091797 \n",
            "\n",
            "Validation Accuracy: 0.9633\n",
            "\n",
            "Train Accuracy: 0.9736\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0006639466094970703 \n",
            "\n",
            "Validation Accuracy: 0.9650\n",
            "\n",
            "Train Accuracy: 0.9769\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005919917678833008 \n",
            "\n",
            "Validation Accuracy: 0.9688\n",
            "\n",
            "Train Accuracy: 0.9795\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0005273355484008789 \n",
            "\n",
            "Validation Accuracy: 0.9693\n",
            "\n",
            "Train Accuracy: 0.9803\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0004724435806274414 \n",
            "\n",
            "Validation Accuracy: 0.9696\n",
            "\n",
            "Train Accuracy: 0.9829\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0004242219924926758 \n",
            "\n",
            "Validation Accuracy: 0.9717\n",
            "\n",
            "Train Accuracy: 0.9841\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00038219696044921876 \n",
            "\n",
            "Validation Accuracy: 0.9711\n",
            "\n",
            "Train Accuracy: 0.9865\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0003444364929199219 \n",
            "\n",
            "Validation Accuracy: 0.9725\n",
            "\n",
            "Train Accuracy: 0.9883\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.00031058061599731443 \n",
            "\n",
            "Validation Accuracy: 0.9731\n",
            "\n",
            "Train Accuracy: 0.9892\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00028169639587402346 \n",
            "\n",
            "Validation Accuracy: 0.9739\n",
            "\n",
            "Train Accuracy: 0.9909\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0002577135467529297 \n",
            "\n",
            "Validation Accuracy: 0.9751\n",
            "\n",
            "Train Accuracy: 0.9914\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00023416717529296875 \n",
            "\n",
            "Validation Accuracy: 0.9746\n",
            "\n",
            "Train Accuracy: 0.9919\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0002180842971801758 \n",
            "\n",
            "Validation Accuracy: 0.9752\n",
            "\n",
            "Train Accuracy: 0.9927\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.00020138044357299805 \n",
            "\n",
            "Validation Accuracy: 0.9747\n",
            "\n",
            "Train Accuracy: 0.9935\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0001890790557861328 \n",
            "\n",
            "Validation Accuracy: 0.9758\n",
            "\n",
            "Total time taken (in seconds): 228.81\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDF2kH63AUyx",
        "outputId": "35c33f01-c526-4a40-b8a2-3c156ebfa5ce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0204\n",
            "\n",
            "Test Accuracy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST ACCURACY HAS INCREASED BY 0.01 FROM 0.97, thanks to these measures.**"
      ],
      "metadata": {
        "id": "n2eLOt1jbWml"
      }
    }
  ]
}