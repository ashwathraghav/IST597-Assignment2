{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVS3rMVhgV4m"
      },
      "source": [
        "# Case 1: \n",
        "\n",
        "Using default batch size without softmax activation in output layer, without any regularization to determine the Categorical Cross-Entropy of test dataset and determine the accuracy of default model in GPU,TPU,CPU and on default mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(6447)\n",
        "tf.random.set_seed(6447)\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0j9g-dk_3SO7"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B1WSN1MJ3X4-"
      },
      "outputs": [],
      "source": [
        "x, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True)\n",
        "x = (x/255).astype('float32')\n",
        "y = to_categorical(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXpKl0y_3Y3q",
        "outputId": "123e0e77-b26c-4f3e-d3fe-2e483970fad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "30ca5da8-25a4-4542-fd86-b47589b89c0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Define the input layer size, hidden layers size and output layer size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden = [128,64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000\n",
        "number_of_test_examples = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "  def accuracy(self,  y_true, y_pred):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moAeRMJ56kr6",
        "outputId": "5893ee0e-3d03-4d6b-fc8a-8169115f782a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.41649543723739496\n",
            "Number of Epoch = 1 - Accuracy:= 12.405047761292016\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.4070060727415966\n",
            "Number of Epoch = 2 - Accuracy:= 12.421839096966911\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.3726960346638655\n",
            "Number of Epoch = 3 - Accuracy:= 12.171425699185924\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.29517391018907563\n",
            "Number of Epoch = 4 - Accuracy:= 11.815150669642858\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.295773043592437\n",
            "Number of Epoch = 5 - Accuracy:= 11.337817793132878\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.3504071691176471\n",
            "Number of Epoch = 6 - Accuracy:= 10.810099625787815\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.4045714285714286\n",
            "Number of Epoch = 7 - Accuracy:= 10.052131204044118\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.4322915244222689\n",
            "Number of Epoch = 8 - Accuracy:= 9.205090434611344\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.3927326680672269\n",
            "Number of Epoch = 9 - Accuracy:= 9.894146574645484\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.37269541097689074\n",
            "Number of Epoch = 10 - Accuracy:= 9.868951877626051\n",
            "\n",
            "Total time taken (in seconds): 584.68\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMFAuH18Ve0",
        "outputId": "d83c9c37-9627-476c-f1cf-98deac11f63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.3675650932247899\n",
            "Number of Epoch = 1 - Accuracy:= 6.218575100938813\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.346741662289916\n",
            "Number of Epoch = 2 - Accuracy:= 6.551343388918067\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.3055718881302521\n",
            "Number of Epoch = 3 - Accuracy:= 6.620254644826681\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.27229651391806725\n",
            "Number of Epoch = 4 - Accuracy:= 6.265620896796219\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.270413931197479\n",
            "Number of Epoch = 5 - Accuracy:= 6.529496906184348\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.2769935661764706\n",
            "Number of Epoch = 6 - Accuracy:= 7.7328557887998945\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.2732294839810924\n",
            "Number of Epoch = 7 - Accuracy:= 8.640414710806198\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.29185110294117644\n",
            "Number of Epoch = 8 - Accuracy:= 10.163045931263131\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.28231423975840336\n",
            "Number of Epoch = 9 - Accuracy:= 11.297496635372898\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.27580862657563027\n",
            "Number of Epoch = 10 - Accuracy:= 11.81344989167542\n",
            "\n",
            "Total time taken (in seconds): 562.03\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4lsqPhB6Xi",
        "outputId": "05d59779-f152-44db-e6bc-4d463775c806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.22557735359768907\n",
            "Number of Epoch = 1 - Accuracy:= 5.968150932247899\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.16549463300945377\n",
            "Number of Epoch = 2 - Accuracy:= 4.734505789620536\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.14369055278361345\n",
            "Number of Epoch = 3 - Accuracy:= 3.9210371770778623\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.12700515362394957\n",
            "Number of Epoch = 4 - Accuracy:= 4.6118040966386555\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.12130336626838235\n",
            "Number of Epoch = 5 - Accuracy:= 4.312644125032826\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.11806086692489495\n",
            "Number of Epoch = 6 - Accuracy:= 3.699186180819984\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.11470710510766807\n",
            "Number of Epoch = 7 - Accuracy:= 3.3781663910681456\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.11327781972163865\n",
            "Number of Epoch = 8 - Accuracy:= 3.1395114930737917\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.11194233357405463\n",
            "Number of Epoch = 9 - Accuracy:= 2.784878770844275\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.11074327074579832\n",
            "Number of Epoch = 10 - Accuracy:= 2.5394907718946955\n",
            "\n",
            "Total time taken (in seconds): 677.11\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LkaUg-TY7GdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c770fc8f-f217-47c6-88eb-451f7474bc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.34022311580882353\n",
            "Number of Epoch = 1 - Accuracy:= 14.364687623096115\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.24267632287289917\n",
            "Number of Epoch = 2 - Accuracy:= 26.53423918395483\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.20824089088760503\n",
            "Number of Epoch = 3 - Accuracy:= 39.83845686712185\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.2074990644695378\n",
            "Number of Epoch = 4 - Accuracy:= 54.22502379858193\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.22552781972163866\n",
            "Number of Epoch = 5 - Accuracy:= 69.66366038602942\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.26027368369222686\n",
            "Number of Epoch = 6 - Accuracy:= 88.86981355042018\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.38820686712184876\n",
            "Number of Epoch = 7 - Accuracy:= 107.37678079044117\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.501226431197479\n",
            "Number of Epoch = 8 - Accuracy:= 124.07367712710085\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.43782070640756304\n",
            "Number of Epoch = 9 - Accuracy:= 138.85502560399158\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.34148650866596636\n",
            "Number of Epoch = 10 - Accuracy:= 151.435546875\n",
            "\n",
            "Total time taken (in seconds): 568.26\n"
          ]
        }
      ],
      "source": [
        "#TPU mode\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='tpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc_total= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EKxWn7CNDVN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715d2a4c-d8c2-493b-ad49-7f61453b90a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0002\n",
            "Test Accuracy: 2.2667\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYHCDjN2w49"
      },
      "source": [
        "# Case 2\n",
        "\n",
        "Using default batch size with softmax activation in output layer, without any regularization to determine the Categorical Cross-Entropy of test dataset and determine the accuracy of default model in GPU,TPU,CPU and on default mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yFsDyDx_3CxR"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y_U0O2T6DPk0"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P89jRHdSEa8f"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goQ_NiPWEjze",
        "outputId": "9357addd-22de-4dfe-d6aa-02ef2f40a7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 4.277167542016807\n",
            "Number of Epoch = 1 - Accuracy:= 50.2974740677521\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.7885979516806723\n",
            "Number of Epoch = 2 - Accuracy:= 63.77486541491597\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.2992858455882352\n",
            "Number of Epoch = 3 - Accuracy:= 66.29577123818278\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.0366507352941177\n",
            "Number of Epoch = 4 - Accuracy:= 67.45548434217437\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.8557383797268907\n",
            "Number of Epoch = 5 - Accuracy:= 67.85714696034664\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7166187631302521\n",
            "Number of Epoch = 6 - Accuracy:= 67.9899676667542\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.6026525735294118\n",
            "Number of Epoch = 7 - Accuracy:= 67.8806378019958\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.5088586528361344\n",
            "Number of Epoch = 8 - Accuracy:= 67.82856814600841\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.43496090467436976\n",
            "Number of Epoch = 9 - Accuracy:= 67.86557904411765\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.38000430015756304\n",
            "Number of Epoch = 10 - Accuracy:= 67.62186515231092\n",
            "\n",
            "Total time taken (in seconds): 742.62\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32) \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QKhRiMOEvVK",
        "outputId": "3ffa1033-01b4-4b41-a8f1-25905972c568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0007\n",
            "Test Accuracy: 67.3524\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyml7OwiguJX"
      },
      "source": [
        "# Case 3\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying Dropout penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NH6VSpWshNaR"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "E4scKXSMHIAU"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ujRr5_7EHuQb"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjuTtAtKJgmv",
        "outputId": "1e99dfa5-6690-48ac-c773-0b36e07ccd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 4.665143382352941\n",
            "Number of Epoch = 1 - Accuracy:= 50.10257599133403\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.7891188287815125\n",
            "Number of Epoch = 2 - Accuracy:= 64.06885586265756\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.3091373424369748\n",
            "Number of Epoch = 3 - Accuracy:= 67.86209132090336\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.054913668592437\n",
            "Number of Epoch = 4 - Accuracy:= 69.90421481092437\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.8879038209033614\n",
            "Number of Epoch = 5 - Accuracy:= 71.03527113970588\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7662441570378151\n",
            "Number of Epoch = 6 - Accuracy:= 71.7950121454832\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.6711848082983193\n",
            "Number of Epoch = 7 - Accuracy:= 72.34461987920167\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.5949285714285715\n",
            "Number of Epoch = 8 - Accuracy:= 72.69422104779412\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5326043855042016\n",
            "Number of Epoch = 9 - Accuracy:= 72.97810530462185\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.47977136948529414\n",
            "Number of Epoch = 10 - Accuracy:= 73.22021894695379\n",
            "\n",
            "Total time taken (in seconds): 924.50\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8SFiKEjJncf",
        "outputId": "211bc313-e91d-4300-e995-0ab7a388808f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 72.6857\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggLjxVFZhPgY"
      },
      "source": [
        "# Case 4\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l1 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-NL-s875hXtH"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "O6ySQ-DmZyk_"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) # L1 = absolute sum of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) + 0.03 * L1 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1cWNHuQqbnKw"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_azRmh6NbqQf",
        "outputId": "e6bea002-4ae2-4144-bd68-ca9d010df169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 4.035077468487395\n",
            "Number of Epoch = 1 - Accuracy:= 48.757984834558826\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.552138524159664\n",
            "Number of Epoch = 2 - Accuracy:= 61.884010635504204\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.0551963629201682\n",
            "Number of Epoch = 3 - Accuracy:= 63.61843487394958\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.7850534401260504\n",
            "Number of Epoch = 4 - Accuracy:= 63.56296776523109\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.6144686843487395\n",
            "Number of Epoch = 5 - Accuracy:= 62.43858324579832\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.5011823135504202\n",
            "Number of Epoch = 6 - Accuracy:= 61.13448250393908\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.42525256039915965\n",
            "Number of Epoch = 7 - Accuracy:= 59.53443818933823\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.3697816767331933\n",
            "Number of Epoch = 8 - Accuracy:= 57.68739741990546\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.32476578912815124\n",
            "Number of Epoch = 9 - Accuracy:= 55.971458114495796\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.2835397518382353\n",
            "Number of Epoch = 10 - Accuracy:= 54.48572085084034\n",
            "\n",
            "Total time taken (in seconds): 896.03\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IOqv_DNb_-0",
        "outputId": "b668ca98-cca7-4564-9fe1-2abec8dc7d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0005\n",
            "Test Accuracy: 53.3810\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTF1L7WXhZBr"
      },
      "source": [
        "# Case 5\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l2 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oHsf3UV8h3A4"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9R3aPWRJgIEk"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) + 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RjadkB_ggMpu"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3E8ZVhsgQaQ",
        "outputId": "d901fe37-6afc-4cd8-e7bb-eb04344cffde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.9969457720588235\n",
            "Number of Epoch = 1 - Accuracy:= 3.333206817883403\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7721226365546219\n",
            "Number of Epoch = 2 - Accuracy:= 6.780364733784139\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.5773415178571428\n",
            "Number of Epoch = 3 - Accuracy:= 8.01174901112789\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.4808226431197479\n",
            "Number of Epoch = 4 - Accuracy:= 8.618988806460084\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.419793362657563\n",
            "Number of Epoch = 5 - Accuracy:= 9.01242039784664\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.37608570772058825\n",
            "Number of Epoch = 6 - Accuracy:= 9.299447708771009\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.34265867909663866\n",
            "Number of Epoch = 7 - Accuracy:= 9.507119058560924\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.31567246586134456\n",
            "Number of Epoch = 8 - Accuracy:= 9.668767848936449\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.29338826155462183\n",
            "Number of Epoch = 9 - Accuracy:= 9.82394793855042\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.27456089154411767\n",
            "Number of Epoch = 10 - Accuracy:= 9.938968946953782\n",
            "\n",
            "Total time taken (in seconds): 183.92\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQxAqK1pgav_",
        "outputId": "825f0150-fa5e-475a-99ac-9bdf82d060dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0034\n",
            "Test Accuracy: 63.2381\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWYl-DUAh3fX"
      },
      "source": [
        "# Case 6\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying a combination of l1 and l2 penalty/regularization[Elastic Net Regularization].Dropout regularization from Keras. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PytDFCuhiDh0"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5C6OTwajprZe"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) # L1 = absolute sum of weights (Also known as Lasso)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) + 0.02*L1 + 0.03 * L2 # Lambda/Regularization Parameter for L1 = 0.02, Lambda/Regularization Parameter for L2 = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "  def accuracy(self,  y_true, y_pred):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = tf.math.argmax(y_true, axis=-1)\n",
        "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5F2URu9Qprmg"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbFz4lSuprv-",
        "outputId": "512a1125-edde-4e8c-81dd-1d64eb99f0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.7671460084033614\n",
            "Number of Epoch = 1 - Accuracy:= 13.88388035878414\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.8393765099789916\n",
            "Number of Epoch = 2 - Accuracy:= 20.341571198792018\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.618664325105042\n",
            "Number of Epoch = 3 - Accuracy:= 27.892059069721636\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.5018616071428571\n",
            "Number of Epoch = 4 - Accuracy:= 36.111578420430675\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.4271548713235294\n",
            "Number of Epoch = 5 - Accuracy:= 44.75950301995798\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.3745648962710084\n",
            "Number of Epoch = 6 - Accuracy:= 53.72291475183823\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.3351660976890756\n",
            "Number of Epoch = 7 - Accuracy:= 62.90696395745798\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.3042320443802521\n",
            "Number of Epoch = 8 - Accuracy:= 72.27388064600841\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.2790219603466387\n",
            "Number of Epoch = 9 - Accuracy:= 81.77597327993698\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.2577517561712185\n",
            "Number of Epoch = 10 - Accuracy:= 91.39392397584034\n",
            "\n",
            "Total time taken (in seconds): 192.73\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc_total= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU9L3M5hpr4e",
        "outputId": "ffe873c5-6a2f-4e43-991b-24e793649a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0030\n",
            "Test Accuracy: 61.2667\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT-CU3DB0Tpf"
      },
      "source": [
        "# Case 7\n",
        "\n",
        "Hyper Parameter Optimization for Batch Size using Trial and Error and softmax activation on output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CVofaVi80hML"
      },
      "outputs": [],
      "source": [
        "#Let Training Batch Size be 128\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "NZ7EajRSdJyU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5"
      ],
      "metadata": {
        "id": "-OfVA5mcdK8r"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc_total= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ATYdpJjdXLI",
        "outputId": "e07845f4-457d-43c2-c61f-d09327ec2bf0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.7262291228991598\n",
            "Number of Epoch = 1 - Accuracy:= 96.64676339285714\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.7170543592436974\n",
            "Number of Epoch = 2 - Accuracy:= 104.17673319327731\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.5700988707983193\n",
            "Number of Epoch = 3 - Accuracy:= 112.49294248949579\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.4868792345063025\n",
            "Number of Epoch = 4 - Accuracy:= 121.30177422531511\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.43056417410714287\n",
            "Number of Epoch = 5 - Accuracy:= 130.45561974789916\n",
            "\n",
            "Total time taken (in seconds): 65.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ4ZU7o5dXQd",
        "outputId": "5a2e4b2c-b22a-4380-dc20-8dae647cb762"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0051\n",
            "Test Accuracy: 58.9714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let Training Batch Size be 15\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "uoWa3LRuzoDh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "dkJYrpNPdMwF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "ST4uum5IdU1g"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(15)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 15 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK8cXgIsdVp3",
        "outputId": "276932c9-bfde-43fe-a69d-caf64312aec5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 4.179804096638655\n",
            "Number of Epoch = 1 - Accuracy:= 50.607901949842436\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.443171875\n",
            "Number of Epoch = 2 - Accuracy:= 63.311242368040965\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.8925803571428571\n",
            "Number of Epoch = 3 - Accuracy:= 65.29044938287815\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.5969323135504202\n",
            "Number of Epoch = 4 - Accuracy:= 65.58296267725841\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.433682937237395\n",
            "Number of Epoch = 5 - Accuracy:= 65.0743972393645\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.3494404543067227\n",
            "Number of Epoch = 6 - Accuracy:= 64.46430212710084\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.29636853335084035\n",
            "Number of Epoch = 7 - Accuracy:= 64.38113223805148\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.2585167574842437\n",
            "Number of Epoch = 8 - Accuracy:= 64.10217182576156\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.22990936843487395\n",
            "Number of Epoch = 9 - Accuracy:= 63.78948923319327\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.20711552980567227\n",
            "Number of Epoch = 10 - Accuracy:= 63.92655060070903\n",
            "\n",
            "Total time taken (in seconds): 785.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bLHbbCXdV3f",
        "outputId": "1490f0f5-9b54-447e-b9b2-e4ea995d6f22"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0003\n",
            "Test Accuracy: 64.7810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From these experiments, less training batch size yields good accuracy**"
      ],
      "metadata": {
        "id": "SawRLfUQbApz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rImDXTU00j1"
      },
      "source": [
        "# Case 8\n",
        "\n",
        "Hyper Parameter Optimization for Learning Rate using Trial and Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "m86w6B-809Xh"
      },
      "outputs": [],
      "source": [
        "#Let Learning Rate be 1e-3\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "zBI1HhAurWyh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "_5L41FIJrW8Y"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCOB0AuWrXEj",
        "outputId": "d9dd4231-a9d4-41ca-b6f2-3923fd8861f7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.1311907825630252\n",
            "Number of Epoch = 1 - Accuracy:= 119.4422268907563\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.1577020417542017\n",
            "Number of Epoch = 2 - Accuracy:= 118.06744025735294\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.12599798943014706\n",
            "Number of Epoch = 3 - Accuracy:= 122.39025571165966\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.11327458639705883\n",
            "Number of Epoch = 4 - Accuracy:= 125.81877790178571\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.10626658514968487\n",
            "Number of Epoch = 5 - Accuracy:= 128.36970686712186\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.10124446888130252\n",
            "Number of Epoch = 6 - Accuracy:= 131.05895483193277\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.09759325761554621\n",
            "Number of Epoch = 7 - Accuracy:= 132.9885684742647\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.09430331702993698\n",
            "Number of Epoch = 8 - Accuracy:= 135.1630449054622\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.09195959164915966\n",
            "Number of Epoch = 9 - Accuracy:= 136.65226989233193\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.08970894334296219\n",
            "Number of Epoch = 10 - Accuracy:= 137.89602481617646\n",
            "\n",
            "Total time taken (in seconds): 1302.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "Uq24dcNTrXLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f7fc8d4-f91b-430b-b77a-b1e5c164cc07"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0001\n",
            "Test Accuracy: 67.4190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "JHaH3hmR0957"
      },
      "outputs": [],
      "source": [
        "#Let Learning Rate be 1e-5\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "wN_EwI6crYeo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "FTg2jJsdrYl2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "id": "dE08Utw6rYsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384cad8a-8ae2-4dca-c292-c9cf4b62380a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 18.672939075630254\n",
            "Number of Epoch = 1 - Accuracy:= 55.13989873293067\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 8.7080131302521\n",
            "Number of Epoch = 2 - Accuracy:= 93.71758469012606\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 6.404541491596639\n",
            "Number of Epoch = 3 - Accuracy:= 108.30942259716387\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 5.239406512605042\n",
            "Number of Epoch = 4 - Accuracy:= 115.72118730304621\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 4.51012237394958\n",
            "Number of Epoch = 5 - Accuracy:= 120.58829273897058\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 3.9901670168067227\n",
            "Number of Epoch = 6 - Accuracy:= 124.15173647584035\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 3.5982914915966386\n",
            "Number of Epoch = 7 - Accuracy:= 126.71284959296219\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 3.2857865021008403\n",
            "Number of Epoch = 8 - Accuracy:= 128.73983226102942\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 3.028159401260504\n",
            "Number of Epoch = 9 - Accuracy:= 130.23545003939077\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 2.814277836134454\n",
            "Number of Epoch = 10 - Accuracy:= 131.603646927521\n",
            "\n",
            "Total time taken (in seconds): 1214.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "IT668FvdrYyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4df5613-1951-4971-ea22-894bc1c71071"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0027\n",
            "Test Accuracy: 65.5524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From these experiments, higher learning rate yields good accuracy**"
      ],
      "metadata": {
        "id": "EDTJIuHIbWQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 9\n",
        "\n",
        "Hyper Parameter Optimisation for Activation Function using Trial and Error"
      ],
      "metadata": {
        "id": "3QMkDj7Xp4uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use sigmoid\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "dnYOl-jeqITx"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.keras.activations.sigmoid(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.keras.activations.sigmoid(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "1yU0REmpst3t"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "Uz7_el4GsuLN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "id": "ZfXCjLB2suei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb912b2c-2aae-443f-ef66-3330d88998eb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.3642765231092437\n",
            "Number of Epoch = 1 - Accuracy:= 6.142939206932773\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.34774258140756303\n",
            "Number of Epoch = 2 - Accuracy:= 5.905961749934349\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.33342134978991594\n",
            "Number of Epoch = 3 - Accuracy:= 5.6672927471769965\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.3210939797794118\n",
            "Number of Epoch = 4 - Accuracy:= 5.492502420890231\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.3104695706407563\n",
            "Number of Epoch = 5 - Accuracy:= 5.460569750361082\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.30123988970588234\n",
            "Number of Epoch = 6 - Accuracy:= 5.388297457654937\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.29312496717436975\n",
            "Number of Epoch = 7 - Accuracy:= 5.243758001247374\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.28588881959033613\n",
            "Number of Epoch = 8 - Accuracy:= 5.272328609178046\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.2793443408613445\n",
            "Number of Epoch = 9 - Accuracy:= 5.24711596064207\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.27334837841386556\n",
            "Number of Epoch = 10 - Accuracy:= 5.292495342863708\n",
            "\n",
            "Total time taken (in seconds): 750.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "yp5w5KUjsulm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e35efe-009b-4505-eda5-d4f087fdd05d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0005\n",
            "Test Accuracy: 5.1810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use tanh\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "3k41AVdlsviJ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.tanh(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.tanh(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "UiWg_-g2s8in"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "lrUgekBms8ps"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "id": "HRoq4G_Bs8xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24fc89b-53a9-451d-e597-7bff724621b4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.0490091911764705\n",
            "Number of Epoch = 1 - Accuracy:= 25.354533219537817\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.9826112788865546\n",
            "Number of Epoch = 2 - Accuracy:= 26.436868106617645\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.9291058954831932\n",
            "Number of Epoch = 3 - Accuracy:= 27.68050608915441\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.8851118040966387\n",
            "Number of Epoch = 4 - Accuracy:= 28.927553834033613\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.8477333902310924\n",
            "Number of Epoch = 5 - Accuracy:= 30.063616071428573\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.8152284663865547\n",
            "Number of Epoch = 6 - Accuracy:= 31.404762588629204\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7864837841386555\n",
            "Number of Epoch = 7 - Accuracy:= 32.665274996717436\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7608396796218487\n",
            "Number of Epoch = 8 - Accuracy:= 33.92915203190651\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.7376493566176471\n",
            "Number of Epoch = 9 - Accuracy:= 34.99463300945378\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.7163786764705883\n",
            "Number of Epoch = 10 - Accuracy:= 35.95258748030462\n",
            "\n",
            "Total time taken (in seconds): 1327.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "3jeUoEMIs82u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ce5c33-a242-483b-d9c4-6a7693db18e3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0007\n",
            "Test Accuracy: 18.1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 10\n",
        "\n",
        "Hyper Parameter Optimisation for Optimizer using Trial and Error"
      ],
      "metadata": {
        "id": "UfwRLGtsJPmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use Adagrad\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "i8ow1kO6JekX"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.tanh(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.tanh(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "Ot_8b933JewH"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "HhNvqCd0Je6U"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JztZXQrzJfEb",
        "outputId": "cf742fbe-d817-4464-82e7-1373df2d270e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.52705078125\n",
            "Number of Epoch = 1 - Accuracy:= 9.482398281578257\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.4901023831407563\n",
            "Number of Epoch = 2 - Accuracy:= 10.077332055869222\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.4578363642331933\n",
            "Number of Epoch = 3 - Accuracy:= 10.890766560530462\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.429066668855042\n",
            "Number of Epoch = 4 - Accuracy:= 11.853794642857144\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.4029908088235294\n",
            "Number of Epoch = 5 - Accuracy:= 12.878996520483193\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.37967233455882354\n",
            "Number of Epoch = 6 - Accuracy:= 14.042015780921743\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.35908866202731093\n",
            "Number of Epoch = 7 - Accuracy:= 15.443678399094013\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.34096083902310925\n",
            "Number of Epoch = 8 - Accuracy:= 16.726062934939602\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.32509768907563025\n",
            "Number of Epoch = 9 - Accuracy:= 17.964724757090337\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.3111820181197479\n",
            "Number of Epoch = 10 - Accuracy:= 19.32440749737395\n",
            "\n",
            "Total time taken (in seconds): 789.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm04OAjxJfO4",
        "outputId": "e5126697-1557-4904-b859-b3b61e37ced4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0006\n",
            "Test Accuracy: 19.6095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use Adam\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "ALI0z3RFJf02"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.tanh(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.tanh(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "Xp9d0vrzJf9s"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "grHEt-pRJgGD"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUWePme9JgOv",
        "outputId": "4ee55705-3ebc-4ecb-b352-90b16ce28246"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.6735883665966387\n",
            "Number of Epoch = 1 - Accuracy:= 10.509268111541491\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.4948926930147059\n",
            "Number of Epoch = 2 - Accuracy:= 11.872277524290967\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.3893385963760504\n",
            "Number of Epoch = 3 - Accuracy:= 16.961362181591387\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.32207221638655464\n",
            "Number of Epoch = 4 - Accuracy:= 23.284062746192227\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.27694898897058823\n",
            "Number of Epoch = 5 - Accuracy:= 29.00837874212185\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.2433827632615546\n",
            "Number of Epoch = 6 - Accuracy:= 33.48571879923845\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.2169197413340336\n",
            "Number of Epoch = 7 - Accuracy:= 37.42015165441177\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.1962352448792017\n",
            "Number of Epoch = 8 - Accuracy:= 40.55129004726891\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.1798439305409664\n",
            "Number of Epoch = 9 - Accuracy:= 43.28740070246849\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.16644646139705882\n",
            "Number of Epoch = 10 - Accuracy:= 45.66384503019958\n",
            "\n",
            "Total time taken (in seconds): 1197.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C22bk1gJga7",
        "outputId": "b7d1cfaa-0d45-419b-8a90-d3cbfe8dd5d0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0003\n",
            "Test Accuracy: 46.3810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 11\n",
        "\n",
        "Calling the most optimal function 10 times and plotting the accuracy for each time: Optimal model has Dropout layer Regularization, Batch size = 20, Activation Fn = Relu, Optimizer = SGD, Learning Rate = 1e-4."
      ],
      "metadata": {
        "id": "iQ1IbnNPuI41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "n_oRfbtLNHEk"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "kx2wm_4xNQzQ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 4"
      ],
      "metadata": {
        "id": "TEqM-DEyNanu"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "def train():\n",
        "  mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "  time_start = time.time()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    acc= tf.zeros([1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_default.forward(inputs) \n",
        "      outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "      preds = tf.cast(preds, dtype=tf.float32)\n",
        "      accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "      acc = acc + accuracy\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "      mlp_on_default.backward(inputs, outputs)\n",
        "    print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "    print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "    time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "id": "X_TTWTRuNfc0"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list =[]\n",
        "for i in range(0,9):\n",
        "  train()\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "  print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "  maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "  yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "  val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  accuracy_list.append(val_acc)\n",
        "  print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "bi01DOiHuX0q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "2f2baf64-97b1-4bac-95c2-ea3b32f221b8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 4.261551995798319\n",
            "Number of Epoch = 1 - Accuracy:= 53.72768267463235\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.8469971113445378\n",
            "Number of Epoch = 2 - Accuracy:= 65.82179375656513\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.3773840598739495\n",
            "Number of Epoch = 3 - Accuracy:= 68.53106125262605\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.1159445903361345\n",
            "Number of Epoch = 4 - Accuracy:= 69.94281775210084\n",
            "\n",
            "Total time taken (in seconds): 311.44\n",
            "Test Categorical entropy loss: 0.0003\n",
            "Test Accuracy: 46.3810\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 4.074890756302521\n",
            "Number of Epoch = 1 - Accuracy:= 49.96299730829832\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.6774948792016806\n",
            "Number of Epoch = 2 - Accuracy:= 64.3848312762605\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.2579891018907563\n",
            "Number of Epoch = 3 - Accuracy:= 67.49403394170169\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.031264574579832\n",
            "Number of Epoch = 4 - Accuracy:= 69.02022879464286\n",
            "\n",
            "Total time taken (in seconds): 372.60\n",
            "Test Categorical entropy loss: 0.0003\n",
            "Test Accuracy: 46.3810\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 3.9140136554621847\n",
            "Number of Epoch = 1 - Accuracy:= 52.27900554753151\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.758486607142857\n",
            "Number of Epoch = 2 - Accuracy:= 63.670598903623954\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.2780416228991596\n",
            "Number of Epoch = 3 - Accuracy:= 66.62521746980042\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-2a770da029c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtest_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-dff0e7899d73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6447\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_default\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    245\u001b[0m   return _from_tensor_list_helper(\n\u001b[1;32m    246\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m       element_spec, tensor_list)\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence)\u001b[0m\n\u001b[1;32m    177\u001b[0m         f\"{structure}, flat_sequence: {flat_sequence}.\")\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_packed_nest_with_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sequence_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[0;34m(structure, flat, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \"\"\"\n\u001b[1;32m    131\u001b[0m   \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_packed_nest_with_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36m_yield_value\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \"\"\"\n\u001b[1;32m     64\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_collections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Iterate through dictionaries in a deterministic order by sorting the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# keys. Notice this means that we ignore the original order of `OrderedDict`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(accuracy_list)\n"
      ],
      "metadata": {
        "id": "hjiD9-bKvEW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0ce4f5df-5585-444d-e938-95001b27ebb9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f55e73d74d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_base = np.array([6.21, 6.55, 6.62, 6.26, 6.52, 7.73, 8.64, 10.16, 11.29, 11.81])/100.0\n",
        "mean_base = np.sum(accuracy_base)/accuracy_base.shape[0]\n",
        "standard_dev_base = np.sqrt(np.sum((accuracy_base-mean_base)**2)/(accuracy_base.shape[0]-1.0))\n",
        "standard_error_base = standard_dev_base/np.sqrt(accuracy_base.shape[0])\n",
        "variance_base = standard_dev_base**2\n",
        "\n",
        "accuracy_L1 = np.array([ 48.75, 61.88, 63.61, 63.56, 62.43, 61.13, 59.53, 57.68, 55.97, 54.48])/100.0\n",
        "mean_L1 = np.sum(accuracy_L1)/accuracy_L1.shape[0]\n",
        "standard_dev_L1 = np.sqrt(np.sum((accuracy_L1-mean_L1)**2)/(accuracy_L1.shape[0]-1.0))\n",
        "standard_error_L1 = standard_dev_L1/np.sqrt(accuracy_L1.shape[0])\n",
        "variance_L1 = standard_dev_L1**2\n",
        "\n",
        "\n",
        "accuracy_optimised = np.array([50.10, 64.06, 67.86, 69.90, 71.03, 71.79, 72.34, 72.69, 72.97, 73.22])/100.0\n",
        "mean_optimised = np.sum(accuracy_optimised)/accuracy_optimised.shape[0]\n",
        "standard_dev_optimised = np.sqrt(np.sum((accuracy_optimised-mean_optimised)**2)/(accuracy_optimised.shape[0]-1.0))\n",
        "standard_error_optimised = standard_dev_optimised/np.sqrt(accuracy_optimised.shape[0])\n",
        "variance_optimised = standard_dev_optimised**2\n",
        "\n",
        "x = np.array([0,1,2])\n",
        "y_mean = np.array([mean_base, mean_L1, mean_optimised])\n",
        "y_standard_error = np.array([standard_error_base, standard_error_L1, standard_error_optimised])\n",
        "y_variance = np.array([variance_base, variance_L1, variance_optimised])\n",
        "\n",
        "\n",
        "plt.figure(0)\n",
        "my_xticks = ['Without Regularization', 'L1 Regularization', 'optimised']\n",
        "plt.plot(x, y_mean, 'go', label='Mean Accuracy')\n",
        "plt.plot(x, y_standard_error, 'bo', label='Mean Standard Error')\n",
        "plt.plot(x, y_variance, 'ro', label='Mean Variance')\n",
        "plt.xticks(x, my_xticks)\n",
        "plt.xlabel('Model')\n",
        "plt.title('Fashion-MNIST dataset')\n",
        "plt.legend()\n",
        "plt.savefig('MNIST_plots.jpg',dpi=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KP5qTFokO3LG",
        "outputId": "10357c06-4446-4685-abb4-bf2d9aced4c0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "mse, bias, var = bias_variance_decomp(mlp_on_default, x_train, y_train, x_test, y_test, loss='mse', num_rounds=200, random_seed=1)"
      ],
      "metadata": {
        "id": "OGP8U6IvRS_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IST597_MLP_Assignment2_Fashion_MNIST.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}