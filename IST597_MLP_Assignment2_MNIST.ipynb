{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/IST597_MLP_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVS3rMVhgV4m"
      },
      "source": [
        "# Case 1: \n",
        "\n",
        "Using default batch size without softmax activation in output layer, without any regularization to determine the Categorical Cross-Entropy of test dataset and determine the accuracy of default model in GPU,TPU,CPU and on default mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "np.random.seed(6447)\n",
        "tf.random.set_seed(6447)\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0j9g-dk_3SO7"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B1WSN1MJ3X4-"
      },
      "outputs": [],
      "source": [
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "x = (x/255).astype('float32')\n",
        "y = to_categorical(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXpKl0y_3Y3q",
        "outputId": "4a1027a1-5d02-4a03-9205-b36a8ec7be4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "2a24d19a-9e77-40c9-9fd6-1aa8ad7f13c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Define the input layer size, hidden layers size and output layer size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden = [128,64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000\n",
        "number_of_test_examples = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "  def accuracy(self,  y_true, y_pred):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = np.argmax(y_true, axis=-1)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moAeRMJ56kr6",
        "outputId": "024c0755-3674-4fa6-d8b6-424d543871ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.33996129858193275\n",
            "Number of Epoch = 1 - Accuracy:= 15.497486787683822\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.33593753282563027\n",
            "Number of Epoch = 2 - Accuracy:= 15.769741539193802\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.3352567949054622\n",
            "Number of Epoch = 3 - Accuracy:= 15.682368943671218\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.3434445575105042\n",
            "Number of Epoch = 4 - Accuracy:= 14.512609145220587\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.34259338891806723\n",
            "Number of Epoch = 5 - Accuracy:= 14.497484736081933\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.34176956407563025\n",
            "Number of Epoch = 6 - Accuracy:= 14.537820255055149\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.3393435202205882\n",
            "Number of Epoch = 7 - Accuracy:= 14.875618557970064\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.3283411567752101\n",
            "Number of Epoch = 8 - Accuracy:= 15.203350676207982\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.3269456079306723\n",
            "Number of Epoch = 9 - Accuracy:= 16.836985868566178\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.3258871783088235\n",
            "Number of Epoch = 10 - Accuracy:= 18.56307034532563\n",
            "\n",
            "Total time taken (in seconds): 1219.47\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMFAuH18Ve0",
        "outputId": "97a9b3e9-ef53-418a-f6b9-284313591beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.3343270745798319\n",
            "Number of Epoch = 1 - Accuracy:= 8.326127252658875\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.29925649947478994\n",
            "Number of Epoch = 2 - Accuracy:= 8.6068115234375\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.27727553834033614\n",
            "Number of Epoch = 3 - Accuracy:= 8.657221433495273\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.25083647912289914\n",
            "Number of Epoch = 4 - Accuracy:= 8.91771127396271\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.23271229976365546\n",
            "Number of Epoch = 5 - Accuracy:= 8.971487862723214\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.22044564075630252\n",
            "Number of Epoch = 6 - Accuracy:= 9.312655921743698\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.2106342568277311\n",
            "Number of Epoch = 7 - Accuracy:= 9.716005982471113\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.20129098279936974\n",
            "Number of Epoch = 8 - Accuracy:= 10.233630268513656\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.18934150144432774\n",
            "Number of Epoch = 9 - Accuracy:= 10.645396000196953\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.17415697216386555\n",
            "Number of Epoch = 10 - Accuracy:= 10.87227239528624\n",
            "\n",
            "Total time taken (in seconds): 800.51\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4lsqPhB6Xi",
        "outputId": "64531a48-1184-4e9f-ea74-9bd77fe3cb46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.3374291951155462\n",
            "Number of Epoch = 1 - Accuracy:= 9.653837316176471\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.2570235031512605\n",
            "Number of Epoch = 2 - Accuracy:= 9.547950039390757\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.22171481092436976\n",
            "Number of Epoch = 3 - Accuracy:= 8.791678292410714\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.19523834690126052\n",
            "Number of Epoch = 4 - Accuracy:= 8.275720420003939\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.17483439469537815\n",
            "Number of Epoch = 5 - Accuracy:= 7.722778833212973\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.161937631302521\n",
            "Number of Epoch = 6 - Accuracy:= 7.279084534204307\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.15080337447478992\n",
            "Number of Epoch = 7 - Accuracy:= 6.911004587381828\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.14032444852941175\n",
            "Number of Epoch = 8 - Accuracy:= 6.558059820607931\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.13428128282563026\n",
            "Number of Epoch = 9 - Accuracy:= 6.455544351529674\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.13041017266281513\n",
            "Number of Epoch = 10 - Accuracy:= 6.178225733652836\n",
            "\n",
            "Total time taken (in seconds): 698.37\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LkaUg-TY7GdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d1e391-40b5-47ec-b7ce-6e8e70ee4feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.30038163077731095\n",
            "Number of Epoch = 1 - Accuracy:= 21.62870929621849\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.2847385110294118\n",
            "Number of Epoch = 2 - Accuracy:= 38.947971376050425\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.27493421743697477\n",
            "Number of Epoch = 3 - Accuracy:= 56.26710625656513\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.2707809709821429\n",
            "Number of Epoch = 4 - Accuracy:= 74.20650603991596\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.2566316964285714\n",
            "Number of Epoch = 5 - Accuracy:= 92.34892003676471\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.2370674730829832\n",
            "Number of Epoch = 6 - Accuracy:= 109.42402507878151\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.21712194721638656\n",
            "Number of Epoch = 7 - Accuracy:= 126.31597131039915\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.2038630842962185\n",
            "Number of Epoch = 8 - Accuracy:= 143.25379136029412\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.19274478072478993\n",
            "Number of Epoch = 9 - Accuracy:= 160.26917016806723\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.18405189732142857\n",
            "Number of Epoch = 10 - Accuracy:= 177.35395877100842\n",
            "\n",
            "Total time taken (in seconds): 888.75\n"
          ]
        }
      ],
      "source": [
        "#TPU mode\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='tpu')\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc_total= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EKxWn7CNDVN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30b7667-9cc9-45e5-a8df-ad57135783e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0002\n",
            "Test Accuracy: 5.6000\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYHCDjN2w49"
      },
      "source": [
        "# Case 2\n",
        "\n",
        "Using default batch size with softmax activation in output layer, without any regularization to determine the Categorical Cross-Entropy of test dataset and determine the accuracy of default model in GPU,TPU,CPU and on default mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yFsDyDx_3CxR"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y_U0O2T6DPk0"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P89jRHdSEa8f"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goQ_NiPWEjze",
        "outputId": "1f0c1ef0-138b-4dc3-c231-27a59e0f787b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.011259978991597\n",
            "Number of Epoch = 1 - Accuracy:= 39.78483209690126\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.06938799894958\n",
            "Number of Epoch = 2 - Accuracy:= 62.03524652048319\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4493610819327731\n",
            "Number of Epoch = 3 - Accuracy:= 67.85378643644958\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.1450513392857142\n",
            "Number of Epoch = 4 - Accuracy:= 70.99331177783613\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.9532591255252101\n",
            "Number of Epoch = 5 - Accuracy:= 72.81512605042016\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.8170021008403361\n",
            "Number of Epoch = 6 - Accuracy:= 73.99676667542016\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7140345982142857\n",
            "Number of Epoch = 7 - Accuracy:= 74.7663799894958\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.6312430409663865\n",
            "Number of Epoch = 8 - Accuracy:= 75.2554572610294\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5625561318277311\n",
            "Number of Epoch = 9 - Accuracy:= 75.46907004989495\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.5042408744747899\n",
            "Number of Epoch = 10 - Accuracy:= 75.64554063813024\n",
            "\n",
            "Total time taken (in seconds): 828.99\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32) \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QKhRiMOEvVK",
        "outputId": "af7bcac6-ac10-43c2-9682-d343b1a08ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0012\n",
            "Test Accuracy: 80.5905\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyml7OwiguJX"
      },
      "source": [
        "# Case 3\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying Dropout penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NH6VSpWshNaR"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "E4scKXSMHIAU"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ujRr5_7EHuQb"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjuTtAtKJgmv",
        "outputId": "5fcb403d-06e3-4ecc-816d-90d5f8cf0227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.3444506302521\n",
            "Number of Epoch = 1 - Accuracy:= 40.82018940388655\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.133702993697479\n",
            "Number of Epoch = 2 - Accuracy:= 62.68072150735294\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4853623949579833\n",
            "Number of Epoch = 3 - Accuracy:= 69.37145483193278\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.1803868172268908\n",
            "Number of Epoch = 4 - Accuracy:= 72.76139870010503\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.9939482668067227\n",
            "Number of Epoch = 5 - Accuracy:= 74.95306755514706\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.8652255777310924\n",
            "Number of Epoch = 6 - Accuracy:= 76.41517857142858\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.769320181197479\n",
            "Number of Epoch = 7 - Accuracy:= 77.43875558035714\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.6934044117647059\n",
            "Number of Epoch = 8 - Accuracy:= 78.20178735556723\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.631827205882353\n",
            "Number of Epoch = 9 - Accuracy:= 78.7547925420168\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.5803373818277311\n",
            "Number of Epoch = 10 - Accuracy:= 79.30097820378151\n",
            "\n",
            "Total time taken (in seconds): 866.62\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8SFiKEjJncf",
        "outputId": "a97cf095-712b-4bfe-bde0-44b7590d6065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0011\n",
            "Test Accuracy: 79.0286\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggLjxVFZhPgY"
      },
      "source": [
        "# Case 4\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l1 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-NL-s875hXtH"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "O6ySQ-DmZyk_"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) # L1 = absolute sum of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) + 0.03 * L1 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1cWNHuQqbnKw"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_azRmh6NbqQf",
        "outputId": "bcd6421c-c7e0-40f7-f9fd-5cb5cdffa3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.065636029411765\n",
            "Number of Epoch = 1 - Accuracy:= 40.7479360884979\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.074591911764706\n",
            "Number of Epoch = 2 - Accuracy:= 61.2638277967437\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.3941135766806723\n",
            "Number of Epoch = 3 - Accuracy:= 66.25050059086135\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.0706287421218488\n",
            "Number of Epoch = 4 - Accuracy:= 68.18159138655462\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.8755820640756302\n",
            "Number of Epoch = 5 - Accuracy:= 68.76307691045169\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7420100446428571\n",
            "Number of Epoch = 6 - Accuracy:= 68.7714638589811\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.6439654017857143\n",
            "Number of Epoch = 7 - Accuracy:= 68.32946674763656\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.5679954700630252\n",
            "Number of Epoch = 8 - Accuracy:= 67.92934283088236\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5086356355042017\n",
            "Number of Epoch = 9 - Accuracy:= 66.96807707457984\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.4595338432247899\n",
            "Number of Epoch = 10 - Accuracy:= 65.88566422662815\n",
            "\n",
            "Total time taken (in seconds): 893.81\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IOqv_DNb_-0",
        "outputId": "31027082-fcff-476c-e9ae-9789fabe81ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0008\n",
            "Test Accuracy: 65.3143\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTF1L7WXhZBr"
      },
      "source": [
        "# Case 5\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying l2 penalty/regularization. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oHsf3UV8h3A4"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9R3aPWRJgIEk"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) + 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RjadkB_ggMpu"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3E8ZVhsgQaQ",
        "outputId": "ff1425f8-7536-4295-c152-8d59eee1de22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.942091780462185\n",
            "Number of Epoch = 1 - Accuracy:= 3.0166413122866333\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.0974676339285714\n",
            "Number of Epoch = 2 - Accuracy:= 5.0409079159007355\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.8253432904411765\n",
            "Number of Epoch = 3 - Accuracy:= 6.423338817949055\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6739732799369748\n",
            "Number of Epoch = 4 - Accuracy:= 7.290558117778362\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5758572741596638\n",
            "Number of Epoch = 5 - Accuracy:= 7.937004025242909\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.5062754398634454\n",
            "Number of Epoch = 6 - Accuracy:= 8.415830467929359\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.4541504398634454\n",
            "Number of Epoch = 7 - Accuracy:= 8.788086963300945\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.4131860556722689\n",
            "Number of Epoch = 8 - Accuracy:= 9.097728056066176\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.37995857405462186\n",
            "Number of Epoch = 9 - Accuracy:= 9.352338005514707\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.35240231092436974\n",
            "Number of Epoch = 10 - Accuracy:= 9.558289087119224\n",
            "\n",
            "Total time taken (in seconds): 146.74\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQxAqK1pgav_",
        "outputId": "212ae9ac-50b9-4274-b308-ff7f35cb7fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0042\n",
            "Test Accuracy: 61.5429\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWYl-DUAh3fX"
      },
      "source": [
        "# Case 6\n",
        "\n",
        "Change Batch Size(HyperParameter Optimization) and try to regularize the objective function by applying a combination of l1 and l2 penalty/regularization[Elastic Net Regularization].Dropout regularization from Keras. Since the codebase is huge because of the model running in different config, I am only going to implement the mlp on default function and not mlp on other modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PytDFCuhiDh0"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5C6OTwajprZe"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      L1= (tf.reduce_sum(self.W1)+ tf.reduce_sum(self.W2)+tf.reduce_sum(self.W3)) # L1 = absolute sum of weights (Also known as Lasso)\n",
        "      L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) + 0.02*L1 + 0.03 * L2 # Lambda/Regularization Parameter for L1 = 0.02, Lambda/Regularization Parameter for L2 = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "  def accuracy(self,  y_true, y_pred):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true = tf.math.argmax(y_true, axis=-1)\n",
        "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5F2URu9Qprmg"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbFz4lSuprv-",
        "outputId": "b79a74fe-e46b-4889-dc60-c9480dbb1fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 2.172958508403361\n",
            "Number of Epoch = 1 - Accuracy:= 11.871858997505253\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.1293400735294117\n",
            "Number of Epoch = 2 - Accuracy:= 16.56373609013918\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.8094517463235295\n",
            "Number of Epoch = 3 - Accuracy:= 22.767963826155462\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6401135766806723\n",
            "Number of Epoch = 4 - Accuracy:= 29.955843372505253\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5339504661239496\n",
            "Number of Epoch = 5 - Accuracy:= 37.82232306985294\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.46077169774159665\n",
            "Number of Epoch = 6 - Accuracy:= 46.19825860031513\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.40714961922268905\n",
            "Number of Epoch = 7 - Accuracy:= 54.96629218093487\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.3660302324054622\n",
            "Number of Epoch = 8 - Accuracy:= 64.05296005120799\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.33328463103991596\n",
            "Number of Epoch = 9 - Accuracy:= 73.39918428308823\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.30660038077731094\n",
            "Number of Epoch = 10 - Accuracy:= 82.97654608718487\n",
            "\n",
            "Total time taken (in seconds): 158.95\n"
          ]
        }
      ],
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc_total= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU9L3M5hpr4e",
        "outputId": "1ed44131-48fc-4285-e592-f1158716d8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0035\n",
            "Test Accuracy: 63.4095\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT-CU3DB0Tpf"
      },
      "source": [
        "# Case 7\n",
        "\n",
        "Hyper Parameter Optimization for Batch Size using Trial and Error and softmax activation on output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "CVofaVi80hML"
      },
      "outputs": [],
      "source": [
        "#Let Training Batch Size be 128\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "NZ7EajRSdJyU"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5"
      ],
      "metadata": {
        "id": "-OfVA5mcdK8r"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc_total= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(128)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ATYdpJjdXLI",
        "outputId": "983cedf7-e70d-4d24-9ebf-24068f6ac546"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 2.205459033613445\n",
            "Number of Epoch = 1 - Accuracy:= 154.4726726628151\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.117172794117647\n",
            "Number of Epoch = 2 - Accuracy:= 159.0654050682773\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.7937662815126051\n",
            "Number of Epoch = 3 - Accuracy:= 165.19216123949582\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.6274538471638655\n",
            "Number of Epoch = 4 - Accuracy:= 172.3455061712185\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.5263214613970588\n",
            "Number of Epoch = 5 - Accuracy:= 180.2446494222689\n",
            "\n",
            "Total time taken (in seconds): 92.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ4ZU7o5dXQd",
        "outputId": "1bdedeea-4ecd-42a6-97df-747a33dea4a2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0060\n",
            "Test Accuracy: 53.1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let Training Batch Size be 15\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "uoWa3LRuzoDh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "dkJYrpNPdMwF"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "ST4uum5IdU1g"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(15)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 15 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK8cXgIsdVp3",
        "outputId": "03517fc6-55a6-4cf1-9169-790f0b2bde7f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 7.7892431722689075\n",
            "Number of Epoch = 1 - Accuracy:= 45.3852969898897\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.490510504201681\n",
            "Number of Epoch = 2 - Accuracy:= 65.46275316767331\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.7295740546218488\n",
            "Number of Epoch = 3 - Accuracy:= 70.4496844636292\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.340109768907563\n",
            "Number of Epoch = 4 - Accuracy:= 73.21394925157563\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 1.0921659663865546\n",
            "Number of Epoch = 5 - Accuracy:= 74.72759445575105\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.9167146139705883\n",
            "Number of Epoch = 6 - Accuracy:= 75.59061515231093\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7831000525210084\n",
            "Number of Epoch = 7 - Accuracy:= 76.19811293658088\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.6782192095588235\n",
            "Number of Epoch = 8 - Accuracy:= 76.69400562959558\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.5942608324579832\n",
            "Number of Epoch = 9 - Accuracy:= 77.03940716911765\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.5243016675420168\n",
            "Number of Epoch = 10 - Accuracy:= 77.22434061515231\n",
            "\n",
            "Total time taken (in seconds): 1114.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bLHbbCXdV3f",
        "outputId": "bdec2b29-0fcd-4c4c-a6d3-42abcdf37b7c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0007\n",
            "Test Accuracy: 77.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From these experiments, less training batch size yields good accuracy**"
      ],
      "metadata": {
        "id": "SawRLfUQbApz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rImDXTU00j1"
      },
      "source": [
        "# Case 8\n",
        "\n",
        "Hyper Parameter Optimization for Learning Rate using Trial and Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "m86w6B-809Xh"
      },
      "outputs": [],
      "source": [
        "#Let Learning Rate be 1e-3\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "zBI1HhAurWyh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "_5L41FIJrW8Y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCOB0AuWrXEj",
        "outputId": "a8516773-b3ac-4ef1-efb7-8f14a6a8c0a9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.9369612657563025\n",
            "Number of Epoch = 1 - Accuracy:= 141.13924632352942\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.25762824973739495\n",
            "Number of Epoch = 2 - Accuracy:= 143.2238543855042\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.13246240644695378\n",
            "Number of Epoch = 3 - Accuracy:= 139.70772058823528\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.11077118073792017\n",
            "Number of Epoch = 4 - Accuracy:= 141.7884388130252\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.1012904822085084\n",
            "Number of Epoch = 5 - Accuracy:= 143.89589351365547\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.09477166491596639\n",
            "Number of Epoch = 6 - Accuracy:= 146.476825105042\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.08980893021271008\n",
            "Number of Epoch = 7 - Accuracy:= 149.1692981880252\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.08597659532563025\n",
            "Number of Epoch = 8 - Accuracy:= 150.67519038865547\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.0822092962184874\n",
            "Number of Epoch = 9 - Accuracy:= 152.7084755777311\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.07913099888392858\n",
            "Number of Epoch = 10 - Accuracy:= 154.38563550420167\n",
            "\n",
            "Total time taken (in seconds): 1620.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "Uq24dcNTrXLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3e5ef4-73f0-460f-b124-936032af686d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0001\n",
            "Test Accuracy: 78.4286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JHaH3hmR0957"
      },
      "outputs": [],
      "source": [
        "#Let Learning Rate be 1e-5\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "wN_EwI6crYeo"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "FTg2jJsdrYl2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "id": "dE08Utw6rYsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca6c077-b33b-4521-9a2e-c7cd2de288bf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 28.626443277310923\n",
            "Number of Epoch = 1 - Accuracy:= 31.44546431853992\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 13.09171743697479\n",
            "Number of Epoch = 2 - Accuracy:= 66.44432362788865\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 8.95253781512605\n",
            "Number of Epoch = 3 - Accuracy:= 89.45563616071428\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 7.062311974789916\n",
            "Number of Epoch = 4 - Accuracy:= 103.49245010504202\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 5.9579338235294115\n",
            "Number of Epoch = 5 - Accuracy:= 112.47080980829831\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 5.212201680672269\n",
            "Number of Epoch = 6 - Accuracy:= 118.73659893644958\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 4.668250525210084\n",
            "Number of Epoch = 7 - Accuracy:= 123.77844340861344\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 4.250621060924369\n",
            "Number of Epoch = 8 - Accuracy:= 127.49289325105042\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 3.9149275210084036\n",
            "Number of Epoch = 9 - Accuracy:= 130.52126280199582\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 3.6362534138655462\n",
            "Number of Epoch = 10 - Accuracy:= 133.02218191964286\n",
            "\n",
            "Total time taken (in seconds): 1608.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "IT668FvdrYyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0cfe60-1d27-4a6c-e2fb-8362ebbbfa8d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0035\n",
            "Test Accuracy: 66.4381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From these experiments, higher learning rate yields good accuracy**"
      ],
      "metadata": {
        "id": "EDTJIuHIbWQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 9\n",
        "\n",
        "Hyper Parameter Optimisation for Activation Function using Trial and Error"
      ],
      "metadata": {
        "id": "3QMkDj7Xp4uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use sigmoid\n",
        "\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "dnYOl-jeqITx"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.keras.activations.sigmoid(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.keras.activations.sigmoid(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "1yU0REmpst3t"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "Uz7_el4GsuLN"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "id": "ZfXCjLB2suei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb96128-71cd-4813-b4f3-d3d92879dd8c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.8955722163865546\n",
            "Number of Epoch = 1 - Accuracy:= 18.114620946034666\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.8675361081932773\n",
            "Number of Epoch = 2 - Accuracy:= 18.33645245207458\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.8410790441176471\n",
            "Number of Epoch = 3 - Accuracy:= 18.538131072741596\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.8158172268907563\n",
            "Number of Epoch = 4 - Accuracy:= 18.770037995667018\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.7914929096638655\n",
            "Number of Epoch = 5 - Accuracy:= 18.93807239692752\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.7679376969537816\n",
            "Number of Epoch = 6 - Accuracy:= 18.995191045168067\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.7450797006302521\n",
            "Number of Epoch = 7 - Accuracy:= 19.173350101759453\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.7228803177521008\n",
            "Number of Epoch = 8 - Accuracy:= 19.217061531643907\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.7013386292016807\n",
            "Number of Epoch = 9 - Accuracy:= 19.23049747242647\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.6804661239495798\n",
            "Number of Epoch = 10 - Accuracy:= 19.169977268251053\n",
            "\n",
            "Total time taken (in seconds): 1612.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "yp5w5KUjsulm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1f045e-8afd-4c47-fd25-6e33bd4db965"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0006\n",
            "Test Accuracy: 9.1810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use tanh\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "3k41AVdlsviJ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.tanh(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.tanh(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "UiWg_-g2s8in"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "lrUgekBms8ps"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(10)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "id": "HRoq4G_Bs8xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7ca735-2079-403c-a2ba-d786fc68b6ee"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.1139016544117648\n",
            "Number of Epoch = 1 - Accuracy:= 19.408633961397058\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.084982668067227\n",
            "Number of Epoch = 2 - Accuracy:= 19.842213350183822\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.057905724789916\n",
            "Number of Epoch = 3 - Accuracy:= 20.373229467568276\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.0325221901260504\n",
            "Number of Epoch = 4 - Accuracy:= 20.877420890231093\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 1.0087566964285715\n",
            "Number of Epoch = 5 - Accuracy:= 21.512703518907564\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.9865191701680672\n",
            "Number of Epoch = 6 - Accuracy:= 22.171505711659663\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.9657086397058824\n",
            "Number of Epoch = 7 - Accuracy:= 22.90763072807248\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.9462326024159664\n",
            "Number of Epoch = 8 - Accuracy:= 23.63018644957983\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.9279724921218487\n",
            "Number of Epoch = 9 - Accuracy:= 24.480565175288866\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.9107887342436974\n",
            "Number of Epoch = 10 - Accuracy:= 25.159518201811977\n",
            "\n",
            "Total time taken (in seconds): 1702.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "3jeUoEMIs82u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8622e116-e7ca-4754-ac48-d29c6f4cd761"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0009\n",
            "Test Accuracy: 12.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 10\n",
        "\n",
        "Hyper Parameter Optimisation for Optimizer using Trial and Error"
      ],
      "metadata": {
        "id": "UfwRLGtsJPmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use Adagrad\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "i8ow1kO6JekX"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.tanh(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.tanh(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "Ot_8b933JewH"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "HhNvqCd0Je6U"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JztZXQrzJfEb",
        "outputId": "db7d0da1-3b20-4a3b-f62a-f6a27da8970f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 1.2349779411764705\n",
            "Number of Epoch = 1 - Accuracy:= 16.047468938747375\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.1713983718487395\n",
            "Number of Epoch = 2 - Accuracy:= 17.18701171875\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.1149443277310924\n",
            "Number of Epoch = 3 - Accuracy:= 18.34651761095063\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.064200892857143\n",
            "Number of Epoch = 4 - Accuracy:= 19.697688254989494\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 1.0179168198529411\n",
            "Number of Epoch = 5 - Accuracy:= 21.240414915966387\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.9754972426470588\n",
            "Number of Epoch = 6 - Accuracy:= 22.813439633665965\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.936459887079832\n",
            "Number of Epoch = 7 - Accuracy:= 24.591552734375\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.900398568802521\n",
            "Number of Epoch = 8 - Accuracy:= 26.21500582654937\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.8669862788865547\n",
            "Number of Epoch = 9 - Accuracy:= 27.794745437237395\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.8359084164915966\n",
            "Number of Epoch = 10 - Accuracy:= 29.744290391938026\n",
            "\n",
            "Total time taken (in seconds): 1870.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm04OAjxJfO4",
        "outputId": "dc4f8f3e-fc8e-4752-b742-46edc722437d"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0008\n",
            "Test Accuracy: 15.3810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use Adam\n",
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "ALI0z3RFJf02"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      #L2= (tf.reduce_sum(tf.square(self.W1))+ tf.reduce_sum(tf.square(self.W2))+tf.reduce_sum(tf.square(self.W3)))/3 # L2 = (absolute sum of squared weights)/no.of weights (Also known as Lasso)\n",
        "      current_loss = self.loss(predicted, y_train) #+ 0.03 * L2 # Lambda/Regularization Parameter = 0.03\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.tanh(what1)\n",
        "    #hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.tanh(what2)\n",
        "    #hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "Xp9d0vrzJf9s"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "grHEt-pRJgGD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc= tf.zeros([1], dtype=tf.float32)\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_default.forward(inputs) \n",
        "    outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "    preds = tf.cast(preds, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "    acc = acc + accuracy\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "  print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "  time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUWePme9JgOv",
        "outputId": "2929b574-a084-4a69-b7d2-fc14538fd862"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 0.5797515756302521\n",
            "Number of Epoch = 1 - Accuracy:= 10.479032628676471\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 0.5213774947478992\n",
            "Number of Epoch = 2 - Accuracy:= 12.003362575498949\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 0.47157648371848737\n",
            "Number of Epoch = 3 - Accuracy:= 13.731080127363446\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 0.42868973214285716\n",
            "Number of Epoch = 4 - Accuracy:= 15.601668157497372\n",
            "Number of Epoch = 5 - Categorical Cross-Entropy:= 0.391703518907563\n",
            "Number of Epoch = 5 - Accuracy:= 17.73784631039916\n",
            "Number of Epoch = 6 - Categorical Cross-Entropy:= 0.35937414653361344\n",
            "Number of Epoch = 6 - Accuracy:= 20.327753660057773\n",
            "Number of Epoch = 7 - Categorical Cross-Entropy:= 0.3313203125\n",
            "Number of Epoch = 7 - Accuracy:= 23.048762473739494\n",
            "Number of Epoch = 8 - Categorical Cross-Entropy:= 0.30696494222689075\n",
            "Number of Epoch = 8 - Accuracy:= 25.798329585740547\n",
            "Number of Epoch = 9 - Categorical Cross-Entropy:= 0.28562539390756303\n",
            "Number of Epoch = 9 - Accuracy:= 28.41007213432248\n",
            "Number of Epoch = 10 - Categorical Cross-Entropy:= 0.26680301339285717\n",
            "Number of Epoch = 10 - Accuracy:= 30.883988067883404\n",
            "\n",
            "Total time taken (in seconds): 1206.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C22bk1gJga7",
        "outputId": "b4167cb1-0b1a-4206-a130-0fea61ff235f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Categorical entropy loss: 0.0005\n",
            "Test Accuracy: 31.5048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 11\n",
        "\n",
        "Calling the most optimal function 10 times and plotting the accuracy for each time: Optimal model has Dropout layer Regularization, Batch size = 20, Activation Fn = Relu, Optimizer = SGD, Learning Rate = 1e-4."
      ],
      "metadata": {
        "id": "iQ1IbnNPuI41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "metadata": {
        "id": "n_oRfbtLNHEk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.dropout_layer, self.device =\\\n",
        "    size_input, size_hidden, size_output, tf.keras.layers.Dropout(rate=0.2), device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer-1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer-1 and hidden layer-2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer-1\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    hhat1 = self.dropout_layer(hhat1)\n",
        "    # Compute values in hidden layer2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    hhat2 = self.dropout_layer(hhat2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.keras.activations.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "kx2wm_4xNQzQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 4"
      ],
      "metadata": {
        "id": "TEqM-DEyNanu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "def train():\n",
        "  mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "  time_start = time.time()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    acc= tf.zeros([1], dtype=tf.float32)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(25, seed=epoch*(6447)).batch(20)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_default.forward(inputs) \n",
        "      outputs = tf.cast(tf.reshape(outputs, (-1,10)), dtype=tf.float32)\n",
        "      preds = tf.cast(preds, dtype=tf.float32)\n",
        "      accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(outputs, 1), tf.argmax(preds, 1)), \"float\"))\n",
        "      acc = acc + accuracy\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_default.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_default.loss(preds, outputs)\n",
        "      mlp_on_default.backward(inputs, outputs)\n",
        "    print('Number of Epoch = {} - Categorical Cross-Entropy:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / x_train.shape[0]))\n",
        "    print('Number of Epoch = {} - Accuracy:= {}'.format(epoch + 1, (np.sum(acc) * 20 / x_train.shape[0])*100))\n",
        "    time_taken = time.time() - time_start\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_TTWTRuNfc0",
        "outputId": "75b1f5b0-c264-4175-8c02-0f56fb2a9d37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 5.446060399159664\n",
            "Number of Epoch = 1 - Accuracy:= 44.3277269892332\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.962219800420168\n",
            "Number of Epoch = 2 - Accuracy:= 64.87734292935924\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4054157037815127\n",
            "Number of Epoch = 3 - Accuracy:= 70.32942161239497\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.1238029149159663\n",
            "Number of Epoch = 4 - Accuracy:= 73.17142364758404\n",
            "\n",
            "Total time taken (in seconds): 316.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list =[]\n",
        "for i in range(0,9):\n",
        "  train()\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  preds = mlp_on_default.forward(x_test)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default.loss(preds, y_test)\n",
        "\n",
        "  print('Test Categorical entropy loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / x_test.shape[0]))\n",
        "\n",
        "\n",
        "  maxposition = lambda x : np.argmax(x)\n",
        "  # List comprehension to map the lambda function across all records of yTrue and yPred\n",
        "  yTrueMax = np.array([maxposition(rec) for rec in y_test])\n",
        "  yPredMax = np.array([maxposition(rec) for rec in preds])\n",
        "  val_acc = sum(yPredMax == yTrueMax)/len(yPredMax)\n",
        "  accuracy_list.append(val_acc)\n",
        "  print('Test Accuracy: {:.4f}'.format(val_acc*100,\"%\"))"
      ],
      "metadata": {
        "id": "bi01DOiHuX0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535cf315-46fd-4630-f49a-3a0de44afc51"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.017623949579832\n",
            "Number of Epoch = 1 - Accuracy:= 38.47900800945378\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.0433325892857144\n",
            "Number of Epoch = 2 - Accuracy:= 59.853831571691174\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4000345325630252\n",
            "Number of Epoch = 3 - Accuracy:= 65.87394957983193\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.0891465992647058\n",
            "Number of Epoch = 4 - Accuracy:= 68.92772616859244\n",
            "\n",
            "Total time taken (in seconds): 403.43\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.6460703781512604\n",
            "Number of Epoch = 1 - Accuracy:= 40.15635668329832\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.010510898109244\n",
            "Number of Epoch = 2 - Accuracy:= 61.9461331407563\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.3464889705882352\n",
            "Number of Epoch = 3 - Accuracy:= 68.49747242647058\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.0419036896008402\n",
            "Number of Epoch = 4 - Accuracy:= 71.97809709821429\n",
            "\n",
            "Total time taken (in seconds): 326.92\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 5.749351365546218\n",
            "Number of Epoch = 1 - Accuracy:= 41.76637588629202\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.1294150472689077\n",
            "Number of Epoch = 2 - Accuracy:= 62.03704782694328\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4964294905462185\n",
            "Number of Epoch = 3 - Accuracy:= 67.92109128807773\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.179704569327731\n",
            "Number of Epoch = 4 - Accuracy:= 71.0503791360294\n",
            "\n",
            "Total time taken (in seconds): 326.82\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 5.419673319327731\n",
            "Number of Epoch = 1 - Accuracy:= 41.10251034007353\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.9727276785714285\n",
            "Number of Epoch = 2 - Accuracy:= 62.346269367121856\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4020238970588235\n",
            "Number of Epoch = 3 - Accuracy:= 68.50925272452731\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.1195267857142857\n",
            "Number of Epoch = 4 - Accuracy:= 71.66562171743698\n",
            "\n",
            "Total time taken (in seconds): 321.73\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 5.938950105042017\n",
            "Number of Epoch = 1 - Accuracy:= 42.68574382878151\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.056927783613445\n",
            "Number of Epoch = 2 - Accuracy:= 63.54454027704832\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.4165820640756301\n",
            "Number of Epoch = 3 - Accuracy:= 69.26891576943277\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.1043320640756302\n",
            "Number of Epoch = 4 - Accuracy:= 72.33109571953781\n",
            "\n",
            "Total time taken (in seconds): 319.18\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 5.159883403361344\n",
            "Number of Epoch = 1 - Accuracy:= 40.18658498555672\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.8525266544117647\n",
            "Number of Epoch = 2 - Accuracy:= 60.79164341517858\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.329484243697479\n",
            "Number of Epoch = 3 - Accuracy:= 66.73609424238445\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.0662234768907564\n",
            "Number of Epoch = 4 - Accuracy:= 69.89417016806723\n",
            "\n",
            "Total time taken (in seconds): 319.16\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 5.717048844537815\n",
            "Number of Epoch = 1 - Accuracy:= 45.83521533613445\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.1726578256302522\n",
            "Number of Epoch = 2 - Accuracy:= 66.79996881565125\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.5795697216386555\n",
            "Number of Epoch = 3 - Accuracy:= 72.12442555147058\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.279828256302521\n",
            "Number of Epoch = 4 - Accuracy:= 74.89252068014706\n",
            "\n",
            "Total time taken (in seconds): 321.79\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.302632352941177\n",
            "Number of Epoch = 1 - Accuracy:= 42.13104401917017\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 1.985467830882353\n",
            "Number of Epoch = 2 - Accuracy:= 65.12776145614497\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.3990568539915966\n",
            "Number of Epoch = 3 - Accuracy:= 70.69739692752101\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.116032825630252\n",
            "Number of Epoch = 4 - Accuracy:= 73.52265789128151\n",
            "\n",
            "Total time taken (in seconds): 320.15\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n",
            "Number of Epoch = 1 - Categorical Cross-Entropy:= 6.259030987394958\n",
            "Number of Epoch = 1 - Accuracy:= 42.85882927389706\n",
            "Number of Epoch = 2 - Categorical Cross-Entropy:= 2.21153125\n",
            "Number of Epoch = 2 - Accuracy:= 64.93608029149159\n",
            "Number of Epoch = 3 - Categorical Cross-Entropy:= 1.5603067226890757\n",
            "Number of Epoch = 3 - Accuracy:= 70.50918297006302\n",
            "Number of Epoch = 4 - Categorical Cross-Entropy:= 1.2428711922268907\n",
            "Number of Epoch = 4 - Accuracy:= 73.61513589810924\n",
            "\n",
            "Total time taken (in seconds): 318.25\n",
            "Test Categorical entropy loss: 0.0010\n",
            "Test Accuracy: 77.5714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(accuracy_list)\n"
      ],
      "metadata": {
        "id": "hjiD9-bKvEW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "24d2b460-e661-4ad9-cfbb-801d5c65383d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc10ead0a90>]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_base = np.array([11.50, 17.44, 17.81, 12.41, 5.20, 4.70, 6.86, 19.25, 5.44, 15.56])/100.0\n",
        "mean_base = np.sum(accuracy_base)/accuracy_base.shape[0]\n",
        "standard_dev_base = np.sqrt(np.sum((accuracy_base-mean_base)**2)/(accuracy_base.shape[0]-1.0))\n",
        "standard_error_base = standard_dev_base/np.sqrt(accuracy_base.shape[0])\n",
        "variance_base = standard_dev_base**2\n",
        "\n",
        "\n",
        "accuracy_optimised = np.array([44.52, 67.95, 78.26, 54.46, 62.47, 75.63, 67.30, 69.49, 75.33, 78.54])/100.0\n",
        "mean_optimised = np.sum(accuracy_optimised)/accuracy_optimised.shape[0]\n",
        "standard_dev_optimised = np.sqrt(np.sum((accuracy_optimised-mean_optimised)**2)/(accuracy_optimised.shape[0]-1.0))\n",
        "standard_error_optimised = standard_dev_optimised/np.sqrt(accuracy_optimised.shape[0])\n",
        "variance_optimised = standard_dev_optimised**2\n",
        "\n",
        "x = np.array([0,1])\n",
        "y_mean = np.array([mean_base, mean_optimised])\n",
        "y_standard_error = np.array([standard_error_base, standard_error_optimised])\n",
        "y_variance = np.array([variance_base, variance_optimised])\n",
        "\n",
        "\n",
        "plt.figure(0)\n",
        "my_xticks = ['Without Regularization','optimised']\n",
        "plt.plot(x, y_mean, 'go', label='Mean Accuracy')\n",
        "plt.plot(x, y_standard_error, 'bo', label='Mean Standard Error')\n",
        "plt.plot(x, y_variance, 'ro', label='Mean Variance')\n",
        "plt.xticks(x, my_xticks)\n",
        "plt.xlabel('Model')\n",
        "plt.title('MNIST-784 dataset')\n",
        "plt.legend()\n",
        "plt.savefig('MNIST_plots.jpg',dpi=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "DUS0qLH_prue",
        "outputId": "a7ad928e-4acf-479b-c868-80e174a0bf7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install mlxtend --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzeCti_IhvKl",
        "outputId": "7dfa7f09-35e2-4ae8-88f4-d55b54a2b347"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->mlxtend) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->mlxtend) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "mse, bias, var = bias_variance_decomp(mlp_on_default, x_train, y_train, x_test, y_test, loss='mse', num_rounds=200, random_seed=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "OGP8U6IvRS_i",
        "outputId": "dfd5c1c8-767d-44c8-e5a8-cb223f3dcead"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-87ee7f2de09d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbias_variance_decomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variance_decomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_on_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IST597_MLP_Assignment2_MNIST.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}